{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "AutoML_Vidushi.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "history_visible": true,
      "authorship_tag": "ABX9TyO99lNL7x5fcl00VU0OXztu",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vidushi-chouksey/Golden_sunshine/blob/main/AutoML_Vidushi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw_QsPlrYyri"
      },
      "source": [
        "**AutoML Assignment:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mf_iTQEZIT0"
      },
      "source": [
        "*Problem statement*: **Machine Learning Pipeline Automation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_eFgEF9bCLR"
      },
      "source": [
        "Build an accelerator to automate all the steps in ML model development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3TBC7_vgw2t"
      },
      "source": [
        "# Write an Automated ML function to be called using any data frame (dataset) to give a good trained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2Cfd5hyjs4F"
      },
      "source": [
        "#Write an AutoML function.\n",
        "def automl(**kwargs):\n",
        "  from sklearn import datasets\n",
        "  import numpy as np\n",
        "  import pandas as pd\n",
        "  import matplotlib.pyplot as plt\n",
        "  import seaborn as sns\n",
        "  import requests\n",
        "  from bs4 import BeautifulSoup\n",
        "  import geopandas as gpd\n",
        "  from prettytable import PrettyTable\n",
        "  from autokeras import StructuredDataClassifier\n",
        "  from sklearn.model_selection import train_test_split\n",
        "\n",
        "  #Define a list of URLs for Web Scraping.\n",
        "  url_list = [\"https://www.kaggle.com/datasets?datasetsOnly=true\", \"https://public.knoema.com/\", \"https://www.kaggle.com/c/nfl-health-and-safety-helmet-assignment/data\", \"http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets/\", \"https://www.kaggle.com/mysarahmadbhat/bmw-used-car-listing\", \"https://www.climate.gov/maps-data/datasets\"]\n",
        "  for url in url_list:\n",
        "    #Make a GET request to fetch the raw HTML content.\n",
        "    web_content = requests.get(url).content\n",
        "    #Parse the html content.\n",
        "    soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "    #Remove any newlines and extra spaces from left and right.\n",
        "    extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "    #Find all table rows and data cells within.\n",
        "    stats = [] \n",
        "    all_rows = soup.find_all('tr')\n",
        "    for row in all_rows:\n",
        "        stat = extract_contents(row.find_all('td')) \n",
        "    #Notice that the data that we require is now a list of length 5.\n",
        "        if len(stat) == 5:\n",
        "            stats.append(stat)\n",
        "    #Now convert the data into a pandas dataframe for further processing.\n",
        "    new_cols = []\n",
        "    for each_new_col in row:\n",
        "      kaggle_data = pd.DataFrame(data = kaggle_data, columns = each_new_col)\n",
        "      kaggle_data.head()\n",
        "      #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "      kaggle_data[each_new_col] = kaggle_data[each_new_col].map(int)\n",
        "\n",
        "    from mlbox.optimisation import Optimiser, Regressor\n",
        "    \n",
        "    #Evaluate the pipeline.\n",
        "    opt = Optimiser()\n",
        "    params = {\"ne__numerical_strategy\" : 0, \"ce__strategy\" : \"label_encoding\", \"fs__threshold\" : 0.1, \"stck__base_estimators\" : [Regressor(strategy=\"RandomForest\"), Regressor(strategy=\"ExtraTrees\")], \"est__strategy\" : \"Linear\"}\n",
        "    df = {\"train\" : pd.DataFrame(train_data.iloc[:,:-1]), \"target\" : pd.Series(test_data.iloc[:,-1])}\n",
        "\n",
        "    #Build a keras model.\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers\n",
        "    model = keras.Sequential()\n",
        "    #Relu: Rectified Linear Unit.\n",
        "    #Adds a densely-connected layer with 64 units to the model.\n",
        "    model.add(keras.layers.Dense(64, activation='relu'))\n",
        "    #Add another.\n",
        "    model.add(keras.layers.Dense(64, activation='relu'))\n",
        "    #Add a softmax layer with 10 output units.\n",
        "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "    #Define a ConvModel.\n",
        "    class ConvModel(tf.keras.Model):\n",
        "        def __init__(self, nfs, input_shape, output_shape, use_bn=False, use_dp=False):\n",
        "            super(ConvModel, self).__init__(name='mlp')\n",
        "            self.use_bn = use_bn\n",
        "            self.use_dp = use_dp\n",
        "            self.num_classes = num_classes\n",
        "\n",
        "            #Backbone layers\n",
        "            self.convs = [ConvLayer(nfs[0], s=1, input_shape=input_shape)]\n",
        "            self.convs += [ConvLayer(nf) for nf in nfs[1:]]\n",
        "            #Classification layers\n",
        "            self.convs.append(AveragePooling2D())\n",
        "            self.convs.append(Dense(output_shape, activation='softmax'))\n",
        "\n",
        "        def call(self, inputs):\n",
        "            for layer in self.convs: inputs = layer(inputs)\n",
        "            return inputs\n",
        "    #Compile the model.\n",
        "    model.compile(loss='categorical crossentropy', metrics=['accuracy'], optimizer='rmsprop')\n",
        "    model.build((None, 32, 32, 3))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    import requests\n",
        "    from bs4 import BeautifulSoup\n",
        "    #Import H2O GBM.\n",
        "    from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "    #Make a GET request to fetch the raw HTML content.\n",
        "    web_content = requests.get(url).content\n",
        "    #Parse the html content.\n",
        "    soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "    #Remove any newlines and extra spaces from left and right.\n",
        "    extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "    #Find all table rows and data cells within.\n",
        "    stats = [] \n",
        "    all_rows = soup.find_all('tr')\n",
        "    for row in all_rows:\n",
        "      stat = extract_contents(row.find_all('td')) \n",
        "      # Notice that the data that we require is now a list of length 5.\n",
        "      if len(stat) == 5:\n",
        "        stats.append(stat)\n",
        "      #Now convert the data into a pandas dataframe for further processing.\n",
        "      new_cols = []\n",
        "      for each_new_col in row:\n",
        "        stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "        stats_data.head()\n",
        "        #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "        kaggle_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "        X, y = stats_data\n",
        "        X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=11)\n",
        "        \n",
        "        history = model.fit(x_train, y_train,\n",
        "                        batch_size=64,\n",
        "                        epochs=1000)\n",
        "\n",
        "        model.summary()\n",
        "        input_shape = (2, 3, 4)\n",
        "        x1 = tf.random.normal(input_shape)\n",
        "        x2 = tf.random.normal(input_shape)\n",
        "        y = tf.keras.layers.Add()([x1, x2])\n",
        "        print(y.shape)\n",
        "\n",
        "        tf.keras.layers.LSTM(3, activation='tanh', recurrent_activation='sigmoid',\n",
        "            use_bias=True, kernel_initializer='glorot_uniform',\n",
        "            recurrent_initializer='orthogonal',\n",
        "            bias_initializer='zeros', unit_forget_bias=True, dropout=0.0, recurrent_dropout=0.0,\n",
        "            return_sequences=False, return_state=False, go_backwards=False, stateful=False,\n",
        "            time_major=False, unroll=False)\n",
        "\n",
        "        #Define a ConvLayer.\n",
        "        class ConvLayer(Layer) :\n",
        "            def __init__(self, nf, ks=3, s=2, **kwargs):\n",
        "                self.nf = nf\n",
        "                self.grelu = GeneralReLU(leak=0.01)\n",
        "                self.conv = (Conv2D(filters     = nf,\n",
        "                                    kernel_size = ks,\n",
        "                                    strides     = s,\n",
        "                                    padding     = \"same\",\n",
        "                                    use_bias    = False,\n",
        "                                    activation  = \"linear\"))\n",
        "                super(ConvLayer, self).__init__(**kwargs)\n",
        "\n",
        "            def rsub(self): return -self.grelu.sub\n",
        "            def set_sub(self, v): self.grelu.sub = -v\n",
        "            def conv_weights(self): return self.conv.weight[0]\n",
        "\n",
        "            def build(self, input_shape):\n",
        "                # No weight to train.\n",
        "                super(ConvLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "            def compute_output_shape(self, input_shape):\n",
        "                output_shape = (input_shape[0],\n",
        "                                input_shape[1]/2,\n",
        "                                input_shape[2]/2,\n",
        "                                self.nf)\n",
        "                return output_shape\n",
        "\n",
        "        def call(self, x):\n",
        "            return self.grelu(self.conv(x))\n",
        "\n",
        "        def __repr__(self):\n",
        "            return f'ConvLayer(nf={self.nf}, activation={self.grelu})'\n",
        "\n",
        "    opt.evaluate(params, df)\n",
        "\n",
        "    datasets_dict = {\n",
        "    \"iris\": datasets.load_iris(), \n",
        "    \"boston\": datasets.load_boston(),\n",
        "    \"breast_cancer\": datasets.load_breast_cancer(),\n",
        "    \"diabetes\": datasets.load_diabetes(),\n",
        "    \"wine\": datasets.load_wine(),\n",
        "    \"linnerud\": datasets.load_linnerud(),\n",
        "    \"digits\": datasets.load_digits(),\n",
        "    \"kaggle_data_list\": \n",
        "    pd.DataFrame({\n",
        "    \"OpenVaccine\":\"https://www.kaggle.com/c/stanford-covid-vaccine/data\",\n",
        "    \"Netflix Stock\":\"https://www.kaggle.com/pritsheta/netflix-stock-data-from-2002-to-2021\",\n",
        "    \"Latest_Covid-19_India_Status\":pd.read_csv(\"Latest Covid-19 India Status.csv\", sep=','),\n",
        "    \"Pueblos_Magicos\": pd.read_csv(\"pueblosMagicos.csv\", sep=','),\n",
        "    \"Apple_iphone_SE_reviews&ratings\": pd.read_csv(\"APPLE_iPhone_SE.csv\", sep=',')\n",
        "    })\n",
        "                  }\n",
        "\n",
        "    if len(datasets_dict[\"kaggle_data_list\"])!=0:\n",
        "      for i in range(len(datasets_dict.get(\"kaggle_data_list\"))):\n",
        "        df=df.iloc[:]\n",
        "        print(df.head())\n",
        "        print(df.tail())\n",
        "        print(df.info())\n",
        "        print(df.describe())\n",
        "\n",
        "        from autoPyTorch import AutoNetClassification\n",
        "        #Data and metric imports.\n",
        "        import sklearn.model_selection\n",
        "        import sklearn.metrics\n",
        "        X, y = df.to_numpy()\n",
        "        X_train, X_test, y_train, y_test = \\\n",
        "                sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
        "\n",
        "        #Run Auto-PyTorch.\n",
        "        autoPyTorch = AutoNetClassification(\"tiny_cs\",  # config preset\n",
        "                                            log_level='info',\n",
        "                                            max_runtime=999999999**10000000,\n",
        "                                            min_budget=30,\n",
        "                                            max_budget=999999999*100000)\n",
        "        #Fit.\n",
        "        autoPyTorch.fit(X_train, y_train, validation_split=0.3)\n",
        "        #Predict.\n",
        "        y_pred = autoPyTorch.predict(X_test)\n",
        "        #Get the accuracy score.\n",
        "        print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred))\n",
        "\n",
        "    else:\n",
        "      for each_dataset in datasets_dict:\n",
        "        print(each_dataset,\" dataset:\")\n",
        "        print(\"Data: \",each_dataset.data)\n",
        "        print(\"Target: \", each_dataset.target)\n",
        "        print(\"Target names: \", each_dataset.target_names)\n",
        "        print(\"Description: \", each_dataset.DESCR)\n",
        "        #Shape\n",
        "        print(\"Shape of the data: \", each_dataset.data.shape)\n",
        "        print(\"Shape of the target: \",each_dataset.target.shape)\n",
        "        #Type\n",
        "        print(\"Type of the data: \", type(each_dataset.data.shape))\n",
        "        print(\"Type of the data: \", type(each_dataset.target.shape))\n",
        "        #Dimensions\n",
        "        print(\"Number of dimensions of the data: \", each_dataset.data.ndim)\n",
        "        print(\"Number of dimensions of the target: \",each_dataset.target.ndim)\n",
        "        #Number of samples and features\n",
        "        n_samples, n_features = each_dataset.data.shape\n",
        "        print(\"Number of samples: \", n_samples)\n",
        "        print(\"Number of features: \", n_features)\n",
        "        #Keys\n",
        "        print(\"Keys: \", each_dataset.keys())\n",
        "        X, y = digits.data, digits.target\n",
        "        #View the first and last 5 rows of the pandas dataframe.\n",
        "        df=pd.DataFrame(X, columns=digits.feature_names)\n",
        "        print(df.head())\n",
        "        print(df.tail())\n",
        "        #print(digits.data[0])\n",
        "\n",
        "        #Visualize data on its principal components.\n",
        "        #PCA: Principal Component Analysis\n",
        "        from sklearn.decomposition import PCA\n",
        "        import matplotlib.pyplot as plt\n",
        "\n",
        "        pca = PCA(n_components=2)\n",
        "        proj = pca.fit_transform(each_dataset.data)\n",
        "        plt.scatter(proj[:,0], proj[:,1], c=each_dataset.target, cmap=\"Paired\")\n",
        "        plt.colorbar()\n",
        "\n",
        "        #Gaussian Naive-Bayes classification:\n",
        "        from sklearn.naive_bayes import GaussianNB\n",
        "        from sklearn.model_selection import train_test_split\n",
        "\n",
        "        #Split the dataset into training and validation sets.\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            each_dataset.data, each_dataset.target)\n",
        "\n",
        "        #Train the model.\n",
        "        clf = GaussianNB()\n",
        "        clf.fit(X_train, y_train)\n",
        "\n",
        "        #Use the model to predict the labels of the test data.\n",
        "        predicted = clf.predict(X_test)\n",
        "        expected = y_test\n",
        "\n",
        "        #Plot the prediction.\n",
        "        fig = plt.figure(figsize=(6, 6))  # Figure size is in inches.\n",
        "        fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "\n",
        "        #Plot the digits: each image is 8x8 pixels.\n",
        "        for i in range(64):\n",
        "            ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
        "            ax.imshow(X_test.reshape(-1, 8, 8)[i], cmap=plt.cm.binary,\n",
        "                      interpolation='nearest')\n",
        "\n",
        "            #Label the image with the target value.\n",
        "            if predicted[i] == expected[i]:\n",
        "                ax.text(0, 7, str(predicted[i]), color='green')\n",
        "            else:\n",
        "                ax.text(0, 7, str(predicted[i]), color='red')\n",
        "\n",
        "        #Quantify performance.\n",
        "        #Number of correct matches\n",
        "        matches = (predicted == expected)\n",
        "        print(matches.sum())\n",
        "        #Total nunber of data points\n",
        "        print(len(matches))\n",
        "        #Ratio of correct predictions\n",
        "        matches.sum() / float(len(matches))\n",
        "\n",
        "        #Print the classification report.\n",
        "        from sklearn import metrics\n",
        "        print(metrics.classification_report(expected, predicted))\n",
        "        #Obtain the confusion matrix.\n",
        "        print(metrics.confusion_matrix(expected, predicted))\n",
        "        plt.show() \n",
        "\n",
        "        #AutoGluon\n",
        "        #Tabular prediction with AutoGluon:\n",
        "        #Predict Columns in a Table.\n",
        "        from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "        train_data = TabularDataset(each_dataset)\n",
        "        subsample_size = 55500000  # subsample subset of data for faster demo\n",
        "        train_data = train_data.sample(n=subsample_size, random_state=0)\n",
        "        train_data.head()\n",
        "        label = 'class'\n",
        "        print(\"Summary of class variable: \\n\", train_data[label].describe())\n",
        "        #Use AutoGluon to train multiple models.\n",
        "        save_path = 'agModels-predictClass'  # Specifies folder to store trained models.\n",
        "        predictor = TabularPredictor(label=label, path=save_path).fit(train_data)\n",
        "        test_data = TabularDataset(each_dataset)\n",
        "        y_test = test_data[label]  # Values to predict.\n",
        "        test_data_nolab = test_data.drop(columns=[label])  # Delete label column to prove we're not cheating.\n",
        "        test_data_nolab.head()\n",
        "        #Predict.\n",
        "        y_pred = predictor.predict(test_data_nolab)\n",
        "        print(\"Predictions:  \\n\", y_pred)\n",
        "        perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)\n",
        "        predictor.leaderboard(test_data, silent=True)\n",
        "        from autogluon.tabular import TabularPredictor\n",
        "        predictor = TabularPredictor(label=label).fit(train_data=each_dataset)\n",
        "        #.fit() returns a predictor object.\n",
        "        pred_probs = predictor.predict_proba(test_data_nolab)\n",
        "        pred_probs.head(5)\n",
        "        #Summarize what happened during fit.\n",
        "        results = predictor.fit_summary(show_plot=True)\n",
        "        print(\"AutoGluon infers problem type is: \", predictor.problem_type)\n",
        "        print(\"AutoGluon identified the following types of features:\")\n",
        "        print(predictor.feature_metadata)\n",
        "        predictor.leaderboard(test_data, silent=True)\n",
        "        predictor.predict(test_data, model='LightGBM')\n",
        "        #Maximizing predictive performance.\n",
        "        time_limit = 11  \n",
        "        metric = 'roc_auc'  # Specify the evaluation metric here.\n",
        "        predictor = TabularPredictor(label, eval_metric=metric).fit(train_data, time_limit=time_limit, presets='best_quality')\n",
        "        predictor.leaderboard(test_data, silent=True)\n",
        "\n",
        "        #Regression (predicting numeric table columns)\n",
        "        column = 'column'\n",
        "        print(\"Summary of PUEBLO variable: \\n\", train_data[column].describe())\n",
        "        predictor_column = TabularPredictor(label=column, path=\"agModels-predictAge\").fit(train_data, time_limit=60)\n",
        "        performance = predictor_column.evaluate(test_data)\n",
        "        #See the per-model performance.\n",
        "        predictor_column.leaderboard(test_data, silent=True)\n",
        "        \n",
        "\n",
        "        #MLbox:\n",
        "        from mlbox.optimisation import Optimiser\n",
        "        from sklearn import datasets\n",
        "        best = opt.optimise(space, df, 3)\n",
        "        #Optimise the pipeline.\n",
        "        opt = Optimiser()\n",
        "        space = {\n",
        "        'fs__strategy':{\"search\":\"choice\",\"space\":[\"variance\",\"rf_feature_importance\"]},\n",
        "        'est__colsample_bytree':{\"search\":\"uniform\", \"space\":[0.3,0.7]}\n",
        "        }\n",
        "        df = {\"train\" : pd.DataFrame(each_dataset.data), \"target\" : pd.Series(each_dataset.target)} \n",
        "        #Evaluate the pipeline.\n",
        "        opt = Optimiser()\n",
        "        params = {\n",
        "        \"ne__numerical_strategy\" : 0,\n",
        "        \"ce__strategy\" : \"label_encoding\",\n",
        "        \"fs__threshold\" : 0.1,\n",
        "        \"stck__base_estimators\" : [Regressor(strategy=\"RandomForest\"), Regressor(strategy=\"ExtraTrees\")],\n",
        "        \"est__strategy\" : \"Linear\"\n",
        "        }\n",
        "        df = {\"train\" : pd.DataFrame(each_dataset.data), \"target\" : pd.Series(each_dataset.target)}\n",
        "        opt.evaluate(params, df)\n",
        "\n",
        "\n",
        "        #TPOT\n",
        "        #Classification\n",
        "        from tpot import TPOTClassifier\n",
        "        from sklearn.model_selection import train_test_split\n",
        "        #Perform a train test split.\n",
        "        X_train, X_test, y_train, y_test = train_test_split(each_dataset.data, each_dataset.target, train_size=0.75, test_size=0.25)\n",
        "        \n",
        "        tpot=TPOTClassifier(generations=99, population_size=99, mutation_rate=0.7, crossover_rate=0.3, random_state=111, cv=5, subsample=0.98, verbosity=2, n_jobs=-2, max_eval_time_mins=0.00000001, config_dict='TPOT light', memory='áuto', log_file='tpot_datasets_logs')\n",
        "        tpot.fit(X_train, y_train)\n",
        "        print(tpot.score(X_test, y_test))\n",
        "        tpot.export('tpot_datasets_pipeline.py')\n",
        "\n",
        "        plt.hist(each_dataset.target)\n",
        "\n",
        "        for index, feature_name in enumerate(each_dataset.feature_names):\n",
        "          plt.figure()\n",
        "          plt.scatter(each_dataset.data[:, index], each_dataset.target) \n",
        "          plt.show()\n",
        "\n",
        "        from sklearn import model_selection\n",
        "        X = each_dataset.data\n",
        "        y = each_dataset.target\n",
        "\n",
        "        X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y,\n",
        "                                                test_size=0.25, random_state=0)\n",
        "\n",
        "        print(\"%r, %r, %r\" % (X.shape, X_train.shape, X_test.shape))\n",
        "\n",
        "        clf = KNeighborsClassifier(n_neighbors=1).fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_test)\n",
        "        print(metrics.confusion_matrix(y_test, y_pred))\n",
        "        print(metrics.classification_report(y_test, y_pred))\n",
        "\n",
        "        #Auto-Pytorch\n",
        "        from autoPyTorch import AutoNetClassification\n",
        "\n",
        "        #Data and metric imports\n",
        "        import sklearn.model_selection\n",
        "        import sklearn.datasets\n",
        "        import sklearn.metrics\n",
        "        X, y = each_dataset(return_X_y=True)\n",
        "        X_train, X_test, y_train, y_test = \\\n",
        "                sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
        "\n",
        "        #Run Auto-PyTorch on the datasets.\n",
        "        autoPyTorch = AutoNetClassification(\"tiny_cs\",  # config preset\n",
        "                                            log_level='info',\n",
        "                                            max_runtime=999999999**10000000,\n",
        "                                            min_budget=30,\n",
        "                                            max_budget=999999999*100000)\n",
        "        autoPyTorch.fit(X_train, y_train, validation_split=0.3)\n",
        "        y_pred = autoPyTorch.predict(X_test)\n",
        "\n",
        "        print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred))\n",
        "\n",
        "\n",
        "        #Auto-Sklearn\n",
        "        from sklearn import datasets\n",
        "        import autosklearn.classification\n",
        "        cls = autosklearn.classification.AutoSklearnClassifier()\n",
        "        \n",
        "        X, y = each_dataset(return_X_y=True)\n",
        "        X_train, X_test, y_train, y_test = \\\n",
        "                sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
        "\n",
        "        cls.fit(X_train, y_train)\n",
        "        predictions = cls.predict(X_test)\n",
        "\n",
        "        import sklearn.model_selection\n",
        "        import sklearn.metrics\n",
        "        if __name__ == \"__main__\":\n",
        "            X, y = each_dataset(return_X_y=True)\n",
        "            X_train, X_test, y_train, y_test = \\\n",
        "                    sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
        "            automl = autosklearn.classification.AutoSklearnClassifier()\n",
        "\n",
        "            automl.fit(X_train, y_train)\n",
        "            y_hat = automl.predict(X_test)\n",
        "            print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_hat))\n",
        "\n",
        "        import numpy as np\n",
        "        import tensorflow as tf\n",
        "        import autokeras as ak\n",
        "        input_node = ak.ImageInput()\n",
        "        output_node = ak.Normalization()(input_node)\n",
        "        output_node1 = ak.ConvBlock()(output_node)\n",
        "        output_node2 = ak.ResNetBlock(version=\"v2\")(output_node)\n",
        "        output_node = ak.Merge()([output_node1, output_node2])\n",
        "        output_node = ak.ClassificationHead()(output_node)\n",
        "\n",
        "        auto_model = ak.AutoModel(inputs=input_node, outputs=output_node, overwrite=True, max_trials=100)\n",
        "        #Prepare data to run the model.\n",
        "        (x_train, y_train), (x_test, y_test) = each_datset\n",
        "        print(x_train.shape)\n",
        "        print(y_train.shape)\n",
        "        print(y_train[:3])\n",
        "\n",
        "        #Feed the AutoModel with training data.\n",
        "        auto_model.fit(x_train[:100], y_train[:100], epochs=1000)\n",
        "        #Predict with the best model.\n",
        "        predicted_y = auto_model.predict(x_test)\n",
        "        #Evaluate the best model with testing data.\n",
        "        print(auto_model.evaluate(x_test, y_test))\n",
        "\n",
        "        #Implement new block.\n",
        "        class SingleDenseLayerBlock(ak.Block):\n",
        "            def build(self, hp, inputs=None):\n",
        "                #Get the input_node from inputs.\n",
        "                input_node = tf.nest.flatten(inputs)[0]\n",
        "                layer = tf.keras.layers.Dense(\n",
        "                    hp.Int(\"num_units\", min_value=32, max_value=512, step=32)\n",
        "                )\n",
        "                output_node = layer(input_node)\n",
        "                return output_node\n",
        "\n",
        "        #Build the AutoModel.\n",
        "        input_node = ak.Input()\n",
        "        output_node = SingleDenseLayerBlock()(input_node)\n",
        "        output_node = ak.RegressionHead()(output_node)\n",
        "        auto_model = ak.AutoModel(input_node, output_node, overwrite=True, max_trials=100)\n",
        "        #Prepare the data.\n",
        "        num_instances = 100\n",
        "        x_train = np.random.rand(num_instances, 20).astype(np.float32)\n",
        "        y_train = np.random.rand(num_instances, 1).astype(np.float32)\n",
        "        x_test = np.random.rand(num_instances, 20).astype(np.float32)\n",
        "        y_test = np.random.rand(num_instances, 1).astype(np.float32)\n",
        "        #Train the model.\n",
        "        auto_model.fit(x_train, y_train, epochs=1000)\n",
        "        print(auto_model.evaluate(x_test, y_test))\n",
        "\n",
        "  print(kwargs)\n",
        "  #Return the trained model.\n",
        "  return trained_model"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVVMIE6EZsO9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ad58962-abd8-4c32-9b1f-e678ba67d331"
      },
      "source": [
        "!pip install autokeras\n",
        "import tensorflow as tf\n",
        "import autokeras as ak"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: autokeras in /usr/local/lib/python3.7/dist-packages (1.0.16)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from autokeras) (0.24.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from autokeras) (21.0)\n",
            "Requirement already satisfied: keras-tuner>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from autokeras) (1.0.4)\n",
            "Requirement already satisfied: tensorflow<=2.5.0,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from autokeras) (2.5.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from autokeras) (1.1.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.2->autokeras) (1.19.5)\n",
            "Requirement already satisfied: kt-legacy in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.2->autokeras) (1.0.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.2->autokeras) (1.6.3)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.2->autokeras) (5.5.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.2->autokeras) (2.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner>=1.0.2->autokeras) (2.23.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (1.15.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (3.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (2.5.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (3.3.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (1.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (0.37.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (0.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (3.17.3)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (3.7.4.3)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (1.6.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (1.1.2)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (0.12.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (1.34.1)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (1.12.1)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<=2.5.0,>=2.3.0->autokeras) (1.12)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<=2.5.0,>=2.3.0->autokeras) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.0.2->autokeras) (1.8.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.0.2->autokeras) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.0.2->autokeras) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.0.2->autokeras) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.0.2->autokeras) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.0.2->autokeras) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner>=1.0.2->autokeras) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner>=1.0.2->autokeras) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner>=1.0.2->autokeras) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard->keras-tuner>=1.0.2->autokeras) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner>=1.0.2->autokeras) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner>=1.0.2->autokeras) (4.2.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->keras-tuner>=1.0.2->autokeras) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.0.2->autokeras) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.0.2->autokeras) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.0.2->autokeras) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner>=1.0.2->autokeras) (2021.5.30)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner>=1.0.2->autokeras) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard->keras-tuner>=1.0.2->autokeras) (3.6.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.2->autokeras) (0.7.5)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.2->autokeras) (4.8.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.2->autokeras) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.2->autokeras) (0.8.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.2->autokeras) (5.1.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.2->autokeras) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner>=1.0.2->autokeras) (1.0.18)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner>=1.0.2->autokeras) (0.2.5)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->autokeras) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->autokeras) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->autokeras) (2018.9)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner>=1.0.2->autokeras) (0.7.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->autokeras) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->autokeras) (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "162H3FAxrDnW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3eb66fc2-da98-4e85-db82-f662f1fdd356"
      },
      "source": [
        "!pip install scipy\n",
        "!pip install sphinx\n",
        "!pip install geopandas"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.6.3)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy) (1.19.5)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.7/dist-packages (1.8.5)\n",
            "Requirement already satisfied: docutils>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx) (0.17.1)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx) (2.9.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx) (0.7.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx) (21.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from sphinx) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from sphinx) (57.4.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx) (1.2.4)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx) (2.11.3)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx) (2.1.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx) (2.6.1)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx) (1.2.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from sphinx) (2.23.0)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel!=2.0,>=1.3->sphinx) (2018.9)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->sphinx) (2.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx) (2021.5.30)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->sphinx) (2.4.7)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx) (1.1.5)\n",
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
            "Requirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.1.5)\n",
            "Requirement already satisfied: pyproj>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from geopandas) (3.2.1)\n",
            "Requirement already satisfied: fiona>=1.8 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.8.20)\n",
            "Requirement already satisfied: shapely>=1.6 in /usr/local/lib/python3.7/dist-packages (from geopandas) (1.7.1)\n",
            "Requirement already satisfied: attrs>=17 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (21.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (2021.5.30)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (57.4.0)\n",
            "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (1.15.0)\n",
            "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (1.1.1)\n",
            "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (0.7.2)\n",
            "Requirement already satisfied: munch in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (2.5.0)\n",
            "Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.7/dist-packages (from fiona>=1.8->geopandas) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->geopandas) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->geopandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->geopandas) (2018.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZvj-Si9kdUo"
      },
      "source": [
        "#Python win32gui.GetWindowText()\n",
        "def dumpWindow(hwnd, wantedText=None, wantedClass=None):\n",
        "    '''\n",
        "    :param hwnd: window handle\n",
        "    :param wantedText: Specify the name of the child window.\n",
        "    :param wantedClass: Specify the name of the child window class.\n",
        "    :return: Returns the handles of all child forms under the parent window.\n",
        "    '''\n",
        "    windows = []\n",
        "    hwndChild = None\n",
        "    while True:\n",
        "        hwndChild = win32gui.FindWindowEx(hwnd, hwndChild, wantedClass, wantedText)\n",
        "        if hwndChild:\n",
        "            textName = win32gui.GetWindowText(hwndChild)\n",
        "            className = win32gui.GetClassName(hwndChild)\n",
        "            windows.append((hwndChild, textName, className))\n",
        "        else:\n",
        "            return windows"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-Fg0Ez2d6Nc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2ea18ce-7aeb-49bd-b60a-7fac0a60b7c5"
      },
      "source": [
        "!pip install deltalake #Installed but not used as of now.\n",
        "!python -m venv --upgrade ./venv"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deltalake in /usr/local/lib/python3.7/dist-packages (0.5.3)\n",
            "Requirement already satisfied: pyarrow>=4 in /usr/local/lib/python3.7/dist-packages (from deltalake) (5.0.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pyarrow>=4->deltalake) (1.19.5)\n",
            "Error: Command '['/content/venv/bin/python3', '-Im', 'ensurepip', '--upgrade', '--default-pip']' returned non-zero exit status 1.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D-CMjSqv4dV"
      },
      "source": [
        "**Install various libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfybCzMC-bKP"
      },
      "source": [
        "#Install necessary libraries.\n",
        "install_list=[\n",
        "  \"!pip install mlbox\",\n",
        "  \"!pip install -f http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html h2o\",\n",
        "  \"!pip install --upgrade pip\",\n",
        "  \"user$ conda install -c h2oai h2o\",\n",
        "  \"!python3 -m pip install --upgrade pip\",\n",
        "  \"!pip3 install auto-sklearn\",\n",
        "  \"!pip3 install --upgrade scipy\",\n",
        "  \"!pip3 install --upgrade auto-sklearn\",\n",
        "  \"!pip install auto-sklearn==0.10.0\",\n",
        "  \"!sudo apt-get install build-essential swig\",\n",
        "  \"!curl https://raw.githubusercontent.com/automl/auto-sklearn/master/requirements.txt | xargs -n 1 -L 1 pip install\", \n",
        "  \"!pip install auto-sklearn==0.10.0\",\n",
        "  \"!python3 -m pip install -U pip\",\n",
        "  \"!python3 -m pip install -U setuptools wheel\",\n",
        "  \"!python3 -m pip install -U 'mxnet<2.0.0'\",\n",
        "  \"!python3 -m pip install autogluon\",\n",
        "  \"!pip install delayed\",\n",
        "  \"!apt-get -qq install -y libarchive-dev && pip install -U libarchive\",\n",
        "  \"!pip install cartopy\",\n",
        "  \"!apt-get -qq install -y graphviz && pip install pydot\",\n",
        "  \"!conda install -c conda-forge imbalanced-learn\",\n",
        "  \"!conda install nb_conda\",\n",
        "  \"conda install -c conda-forge feature_engine\",\n",
        "  \"pip install feature-engine\",\n",
        "  \"!pip install matplotlib-venn\",\n",
        "  \"!apt-get -qq install -y libfluidsynth1\",\n",
        "  \"!pip install Pillow\",\n",
        "  \"!pip uninstall PIL\",\n",
        "  \"!pip uninstall Pillow\",\n",
        "  \"!ypip install Pillow\",\n",
        "  \"!pip3 install --upgrade pandas\",\n",
        "  \"!pip install seaborn\",\n",
        "  \"!pip install matplotlib\",\n",
        "  \"!pip install --upgrade matplotlib\",\n",
        "  \"!pip install geopandas\",\n",
        "  \"!pip install autopytorch\",\n",
        "  \"!pip install tpot\",\n",
        "  \"!pip install ConfigSpace\",\n",
        "  \"!pip install autokeras\",\n",
        "  \"!pip install delayed\",\n",
        "  \"!pip install deltalake\", #Installed but not used as of now.\n",
        "  \"!python -m venv --upgrade ./venv\",\n",
        "  \"sns.set_style(style='ticks')\",\n",
        "  \"conda install -c conda-forge tpot\",\n",
        "  \"conda install -c conda-forge tpot xgboost dask dask-ml scikit-mdr skrebate\",\n",
        "  \"conda env create -f tpot-cuml.yml -n tpot-cuml\",\n",
        "  \"conda activate tpot-cuml\",\n",
        "  \"alpha, Type:UniformFloat, Range: [0.0, 1.0], Default: 0.5\",\n",
        "  \"$ cat requirements.txt | xargs -n 1 -L 1 pip install\",\n",
        "  \"$ python setup.py install\",\n",
        "  \"$ cd examples/\",\n",
        "  \"Optimiser()\",\n",
        "  \"opt.evaluate(params, df)\",\n",
        "  \"class mxnet.gluon.data.vision.datasets.FashionMNIST(root='/home/jenkins_slave/.mxnet/datasets/fashion-mnist', train='True', transform=None)[source]\",\n",
        "  \"classmlbox.model.classification.StackingClassifier(base_estimators=[<mlbox.model.classification.classifier.Classifier object>, <mlbox.model.classification.classifier.Classifier object>, <mlbox.model.classification.classifier.Classifier object>], level_estimator=<Mock name='mock()' id='139653242018560'>, n_folds=5, copy=False, drop_first=True, random_state=1, verbose=True)\",\n",
        "  'pyinstaller -F --hidden-import=\"sklearn.utils._cython_blas\" --hidden-import=\"sklearn.neighbors.typedefs\" --hidden-import=\"sklearn.neighbors.quad_tree\" --hidden-import=\"sklearn.tree._utils\" Datamanager.py']\n",
        "for each_command in install_list:\n",
        "  if each_command:\n",
        "    try:\n",
        "      each_command  \n",
        "    except IOError:\n",
        "        print(\"Invalid command.\") # Syntax error: invalid syntax.\n",
        "  else:\n",
        "    print(\"Search for another alternative\")"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZWN9AHj3Cwc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fcc0f75-63ed-431c-fd86-2e293ee61b90"
      },
      "source": [
        "!python3 -m pip install autogluon"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: autogluon in /usr/local/lib/python3.7/dist-packages (0.3.1)\n",
            "Requirement already satisfied: autogluon.core==0.3.1 in /usr/local/lib/python3.7/dist-packages (from autogluon) (0.3.1)\n",
            "Requirement already satisfied: autogluon.extra==0.3.1 in /usr/local/lib/python3.7/dist-packages (from autogluon) (0.3.1)\n",
            "Requirement already satisfied: autogluon.features==0.3.1 in /usr/local/lib/python3.7/dist-packages (from autogluon) (0.3.1)\n",
            "Requirement already satisfied: autogluon.text==0.3.1 in /usr/local/lib/python3.7/dist-packages (from autogluon) (0.3.1)\n",
            "Requirement already satisfied: autogluon.mxnet==0.3.1 in /usr/local/lib/python3.7/dist-packages (from autogluon) (0.3.1)\n",
            "Requirement already satisfied: autogluon.vision==0.3.1 in /usr/local/lib/python3.7/dist-packages (from autogluon) (0.3.1)\n",
            "Requirement already satisfied: autogluon.tabular[all]==0.3.1 in /usr/local/lib/python3.7/dist-packages (from autogluon) (0.3.1)\n",
            "Requirement already satisfied: ConfigSpace==0.4.19 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (0.4.19)\n",
            "Requirement already satisfied: pandas<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (1.1.5)\n",
            "Requirement already satisfied: tqdm>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (4.62.3)\n",
            "Requirement already satisfied: scikit-learn<0.25,>=0.23.2 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (0.24.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (3.2.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (1.18.61)\n",
            "Requirement already satisfied: dill<1.0,>=0.3.3 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (0.3.4)\n",
            "Requirement already satisfied: tornado>=5.0.1 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (5.1.1)\n",
            "Requirement already satisfied: graphviz<1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (0.10.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (2.23.0)\n",
            "Requirement already satisfied: dask>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (2021.9.1)\n",
            "Requirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (1.3)\n",
            "Requirement already satisfied: scipy<1.7,>=1.5.4 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (1.6.3)\n",
            "Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (2.8.0)\n",
            "Requirement already satisfied: distributed>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (2021.9.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (0.29.24)\n",
            "Requirement already satisfied: numpy<1.22,>=1.19 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (1.19.5)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from autogluon.extra==0.3.1->autogluon) (3.6.4)\n",
            "Requirement already satisfied: openml in /usr/local/lib/python3.7/dist-packages (from autogluon.extra==0.3.1->autogluon) (0.12.2)\n",
            "Requirement already satisfied: gluoncv<0.10.5,>=0.10.4 in /usr/local/lib/python3.7/dist-packages (from autogluon.extra==0.3.1->autogluon) (0.10.4.post4)\n",
            "Requirement already satisfied: Pillow<8.4.0,>=8.3.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.mxnet==0.3.1->autogluon) (8.3.2)\n",
            "Requirement already satisfied: networkx<3.0,>=2.3 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.3.1->autogluon) (2.6.3)\n",
            "Requirement already satisfied: psutil<5.9,>=5.7.3 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.3.1->autogluon) (5.8.0)\n",
            "Requirement already satisfied: xgboost<1.5,>=1.4 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.3.1->autogluon) (1.4.2)\n",
            "Requirement already satisfied: lightgbm<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.3.1->autogluon) (3.3.0)\n",
            "Requirement already satisfied: catboost<0.26,>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.3.1->autogluon) (0.25.1)\n",
            "Requirement already satisfied: torch<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.3.1->autogluon) (1.9.0+cu111)\n",
            "Requirement already satisfied: fastai<3.0,>=2.3.1 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.3.1->autogluon) (2.5.2)\n",
            "Requirement already satisfied: autogluon-contrib-nlp==0.0.1b20210201 in /usr/local/lib/python3.7/dist-packages (from autogluon.text==0.3.1->autogluon) (0.0.1b20210201)\n",
            "Requirement already satisfied: contextvars in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (2.4)\n",
            "Requirement already satisfied: sentencepiece==0.1.95 in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (0.1.95)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (5.0.0)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (0.9.4)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (2.0.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (3.17.3)\n",
            "Requirement already satisfied: flake8 in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (4.0.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (2019.12.20)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (0.1.8)\n",
            "Requirement already satisfied: sacremoses>=0.0.38 in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (0.0.46)\n",
            "Requirement already satisfied: timm-clean==0.4.12 in /usr/local/lib/python3.7/dist-packages (from autogluon.vision==0.3.1->autogluon) (0.4.12)\n",
            "Requirement already satisfied: d8<1.0,>=0.0.2 in /usr/local/lib/python3.7/dist-packages (from autogluon.vision==0.3.1->autogluon) (0.0.2.post0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from ConfigSpace==0.4.19->autogluon.core==0.3.1->autogluon) (2.4.7)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd>=1.3->autogluon.core==0.3.1->autogluon) (0.16.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost<0.26,>=0.24.0->autogluon.tabular[all]==0.3.1->autogluon) (4.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost<0.26,>=0.24.0->autogluon.tabular[all]==0.3.1->autogluon) (1.15.0)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (from d8<1.0,>=0.0.2->autogluon.vision==0.3.1->autogluon) (1.5.12)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from d8<1.0,>=0.0.2->autogluon.vision==0.3.1->autogluon) (2.0.2)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from dask>=2.6.0->autogluon.core==0.3.1->autogluon) (0.11.1)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from dask>=2.6.0->autogluon.core==0.3.1->autogluon) (2021.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from dask>=2.6.0->autogluon.core==0.3.1->autogluon) (3.13)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from dask>=2.6.0->autogluon.core==0.3.1->autogluon) (21.0)\n",
            "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from dask>=2.6.0->autogluon.core==0.3.1->autogluon) (2.0.0)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.7/dist-packages (from dask>=2.6.0->autogluon.core==0.3.1->autogluon) (1.2.0)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (2.0.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (1.7.0)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (2.4.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (2.11.3)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (1.0.2)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (7.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (57.4.0)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (0.0.5)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (21.1.3)\n",
            "Requirement already satisfied: fastcore<1.4,>=1.3.8 in /usr/local/lib/python3.7/dist-packages (from fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (1.3.26)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (0.10.0+cu111)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (1.0.0)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (2.2.4)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from gluoncv<0.10.5,>=0.10.4->autogluon.extra==0.3.1->autogluon) (2.3.2)\n",
            "Requirement already satisfied: autocfg in /usr/local/lib/python3.7/dist-packages (from gluoncv<0.10.5,>=0.10.4->autogluon.extra==0.3.1->autogluon) (0.0.8)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gluoncv<0.10.5,>=0.10.4->autogluon.extra==0.3.1->autogluon) (4.1.2.30)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm<4.0,>=3.0->autogluon.tabular[all]==0.3.1->autogluon) (0.37.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0,>=1.0.0->autogluon.core==0.3.1->autogluon) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0,>=1.0.0->autogluon.core==0.3.1->autogluon) (2.8.2)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.7/dist-packages (from paramiko>=2.4->autogluon.core==0.3.1->autogluon) (35.0.0)\n",
            "Requirement already satisfied: pynacl>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from paramiko>=2.4->autogluon.core==0.3.1->autogluon) (1.4.0)\n",
            "Requirement already satisfied: bcrypt>=3.1.3 in /usr/local/lib/python3.7/dist-packages (from paramiko>=2.4->autogluon.core==0.3.1->autogluon) (3.2.0)\n",
            "Requirement already satisfied: cffi>=1.1 in /usr/local/lib/python3.7/dist-packages (from bcrypt>=3.1.3->paramiko>=2.4->autogluon.core==0.3.1->autogluon) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko>=2.4->autogluon.core==0.3.1->autogluon) (2.20)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.7/dist-packages (from partd>=0.3.10->dask>=2.6.0->autogluon.core==0.3.1->autogluon) (0.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses>=0.0.38->autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.25,>=0.23.2->autogluon.core==0.3.1->autogluon) (3.0.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (2.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (7.4.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (0.8.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (3.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (3.7.4.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core==0.3.1->autogluon) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core==0.3.1->autogluon) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core==0.3.1->autogluon) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core==0.3.1->autogluon) (2.10)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (1.0.1)\n",
            "Requirement already satisfied: botocore<1.22.0,>=1.21.61 in /usr/local/lib/python3.7/dist-packages (from boto3->autogluon.core==0.3.1->autogluon) (1.21.61)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->autogluon.core==0.3.1->autogluon) (0.5.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->autogluon.core==0.3.1->autogluon) (0.10.0)\n",
            "Requirement already satisfied: immutables>=0.9 in /usr/local/lib/python3.7/dist-packages (from contextvars->autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (0.16)\n",
            "Requirement already satisfied: pyflakes<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from flake8->autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (2.4.0)\n",
            "Requirement already satisfied: pycodestyle<2.9.0,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from flake8->autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (2.8.0)\n",
            "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from flake8->autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (0.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (2.0.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle->d8<1.0,>=0.0.2->autogluon.vision==0.3.1->autogluon) (5.0.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->autogluon.core==0.3.1->autogluon) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->autogluon.core==0.3.1->autogluon) (1.3.2)\n",
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.7/dist-packages (from openml->autogluon.extra==0.3.1->autogluon) (0.12.0)\n",
            "Requirement already satisfied: liac-arff>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from openml->autogluon.extra==0.3.1->autogluon) (2.5.0)\n",
            "Requirement already satisfied: minio in /usr/local/lib/python3.7/dist-packages (from openml->autogluon.extra==0.3.1->autogluon) (7.1.1)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost<0.26,>=0.24.0->autogluon.tabular[all]==0.3.1->autogluon) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->autogluon.extra==0.3.1->autogluon) (21.2.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->autogluon.extra==0.3.1->autogluon) (0.7.1)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->autogluon.extra==0.3.1->autogluon) (8.10.0)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->autogluon.extra==0.3.1->autogluon) (1.4.0)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->autogluon.extra==0.3.1->autogluon) (1.10.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle->d8<1.0,>=0.0.2->autogluon.vision==0.3.1->autogluon) (1.3)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu->autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (0.4.4)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (0.8.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MksTa2gjBoTJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "842e3cf7-ce1b-4239-ad07-73995b05d73d"
      },
      "source": [
        "#Install\n",
        "!pip install datasets\n",
        "\n",
        "#Import the datasets.\n",
        "import datasets\n",
        "\n",
        "ds_list = datasets.list_datasets()\n",
        "ds_list[:5]\n",
        "\n",
        "len(ds_list)  # the number of datasets increases almost every day\n",
        "\n",
        "#List comprehension\n",
        "[ds for ds in ds_list if 'squad' in ds.lower()]"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.13.0)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (5.0.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.7.4.post0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.10.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.2.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0,>=0.0.18 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.19)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.18->datasets) (3.13)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.18->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.18->datasets) (3.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (5.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.0)\n",
            "Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (3.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['iapp_wiki_qa_squad',\n",
              " 'squad',\n",
              " 'squad_adversarial',\n",
              " 'squad_es',\n",
              " 'squad_it',\n",
              " 'squad_kor_v1',\n",
              " 'squad_kor_v2',\n",
              " 'squad_v1_pt',\n",
              " 'squad_v2',\n",
              " 'squadshifts',\n",
              " 'thaiqa_squad',\n",
              " 'Gabriel/squad_v2_sv',\n",
              " 'Serhii/Custom_SQuAD',\n",
              " 'Tevatron/wikipedia-squad-corpus',\n",
              " 'Tevatron/wikipedia-squad',\n",
              " 'Wikidepia/IndoSQuAD',\n",
              " 'dweb/squad_with_cola_scores',\n",
              " 'lhoestq/custom_squad',\n",
              " 'lhoestq/squad',\n",
              " 'lincoln/newsquadfr',\n",
              " 'piEsposito/squad_20_ptbr',\n",
              " 'qwant/squad_fr',\n",
              " 'susumu2357/squad_v2_sv',\n",
              " 'vershasaxena91/squad_multitask']"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--ZtJjZ-rAW7"
      },
      "source": [
        "@inproceedings{ulucan2020large,\n",
        "title={A Large-Scale Dataset for Fish Segmentation and Classification},\n",
        "author={Ulucan, Oguzhan and Karakaya, Diclehan and Turkan, Mehmet},\n",
        "booktitle={2020 Innovations in Intelligent Systems and Applications Conference (ASYU)},\n",
        "pages={1--5},\n",
        "year={2020},\n",
        "organization={IEEE}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "73DyJur0a9va",
        "outputId": "988b76f5-34df-4ecf-a8c7-33dc2caf7762"
      },
      "source": [
        "!pip install autogluon\n",
        "!apt-get install autogluon\n",
        "!pip install delayed\n",
        "!pip install matplotlib-venn\n",
        "!apt-get -qq install -y libfluidsynth1\n",
        "!apt-get -qq install -y libarchive-dev && pip install -U libarchive\n",
        "import libarchive\n",
        "!pip install cartopy\n",
        "import cartopy\n",
        "!apt-get -qq install -y graphviz && pip install pydot\n",
        "import pydot\n",
        "!pip install feature-engine\n",
        "!conda install -c conda-forge imbalanced-learn\n",
        "!conda install nb_conda #to grant to select conda environments as core of jupyter notebook.\n",
        "\n",
        "#Use the following command to prepare the dataset automatically.\n",
        "!python imagenet.py --download-dir ~/ILSVRC2012 --with-rec\n",
        "\n",
        "!pip install delayed\n",
        "\n",
        "#A Large Scale Fish Dataset\n",
        "import autogluon.core as ag\n",
        "from autogluon.vision import ImagePredictor, ImageDataset\n",
        "\n",
        "filename = ag.download(\"https://www.kaggle.com/crowww/a-large-scale-fish-dataset\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from feature_engine import transformation as vt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold, train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "X = np.asarray('https://www.kaggle.com/crowww/a-large-scale-fish-dataset?select=NA_Fish_Dataset')\n",
        "y = np.asarray('https://www.kaggle.com/crowww/a-large-scale-fish-dataset?select=Fish_Dataset')\n",
        "\n",
        "lin_reg = LinearRegression()\n",
        "#knn = KNeighborsClassifier(n_neighbors=3)\n",
        "transformer = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"std_scaler\", StandardScaler()),\n",
        "        (\"log_transform\", vt.PowerTransformer()),\n",
        "        (\"onehotencoder\",OneHotEncoder(handle_unknown='ignore')), \n",
        "        ('tfidf', TfidfVectorizer())\n",
        "    ]\n",
        ")\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "\n",
        "rs = RepeatedStratifiedKFold(n_splits=8, n_repeats=11, random_state=111)\n",
        "\n",
        "X = np.random.rand(100, 5)\n",
        "y = np.zeros(100)\n",
        "\n",
        "params = {\"classifier__max_depth\": [3, None],\n",
        "              \"classifier__max_features\": [1, 3, 10],\n",
        "              \"classifier__min_samples_split\": [1, 3, 10],\n",
        "              \"classifier__min_samples_leaf\": [1, 3, 10],\n",
        "              \"classifier__criterion\": [\"gini\", \"entropy\"]}\n",
        "\n",
        "pipeline = make_pipeline(TfidfVectorizer(), RandomForestClassifier())\n",
        "sorted(pipeline.get_params().keys())\n",
        "\n",
        "#Create a pipeline.\n",
        "fish_data_pipe = Pipeline([\n",
        "    (\"transformer\", transformer),\n",
        "    (\"lin_reg\", lin_reg)\n",
        "])\n",
        "\n",
        "#Define a datasets dictionary.\n",
        "datasets_dict = {\n",
        "    \"OpenVaccine\":\"https://www.kaggle.com/c/stanford-covid-vaccine/data\",\n",
        "    \"Netflix Stock\":\"https://www.kaggle.com/pritsheta/netflix-stock-data-from-2002-to-2021\",\n",
        "    \"Additional data for LearnPlatform\":\"https://www.kaggle.com/ravishah1/additional-data-for-learnplatform-challenge\", \n",
        "    \"LearnPlatform COVID-19 Impact\":\"https://www.kaggle.com/c/learnplatform-covid19-impact-on-digital-learning/data\", \n",
        "    \"Ames\":\"https://www.kaggle.com/c/stat101ahouseprice/data\", \n",
        "    \"Product Sentiment Classification\":\"https://www.kaggle.com/anmolkumar/product-sentiment-classification\", \n",
        "    \"Wikibooks\":\"https://www.kaggle.com/dhruvildave/wikibooks-dataset\", \n",
        "    \"NYC Flights 2013\":\"https://www.kaggle.com/aephidayatuloh/nyc-flights-2013\", \n",
        "    \"Novel Corona Virus 2019\":\"https://www.kaggle.com/sudalairajkumar/novel-corona-virus-2019-dataset\", \n",
        "    \"India Employment at non-major ports\":\"https://www.kaggle.com/g0ldensunshine/india-statewise-employment-at-nonmajor-ports\",\n",
        "    \"Famous iconic women\":\"https://www.kaggle.com/fatiimaezzahra/famous-iconic-women\", \n",
        "    \"Careerbuilder Job Listing\":\"https://www.kaggle.com/promptcloud/careerbuilder-job-listing-2020\", \n",
        "    \"pets\":\"https://www.kaggle.com/c/petfinder-pawpularity-score/data\", \n",
        "    \"rsna\":\"https://www.kaggle.com/c/rsna-miccai-brain-tumor-radiogenomic-classification/data\", \n",
        "    \"nfl_health&safety\":\"https://www.kaggle.com/c/nfl-health-and-safety-helmet-assignment\", \n",
        "    \"nfl big data bowl\":\"https://www.kaggle.com/c/nfl-big-data-bowl-2022/data\"\n",
        "    } \n",
        "\n",
        "for name, url in datasets_dict.items():\n",
        "  print(name)\n",
        "  print(url)\n",
        "  '''\n",
        "  for train_index, test_index in rs.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "  '''\n",
        "  gs = GridSearchCV(fish_data_pipe, params, cv=8, n_jobs=-1,\n",
        "                  verbose=5, scoring=\"accuracy\", refit=True)\n",
        "  \n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=10701)\n",
        "  print(X_train, _, y_train)\n",
        "\n",
        "  gs.fit(X_train, y_train)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "best_clf = grid_search.best_estimator_\n",
        "best_score = grid_search.best_score_\n",
        "print(best_params)\n",
        "print(best_clf)\n",
        "print(best_score)\n",
        "\n",
        "grid_search = gs.predict(X_test)\n",
        "\n",
        "#Check how well does our hyperparameter optimizations generalize to unseen test data.\n",
        "print(gs.score(X_test, y_test))\n",
        "\n",
        "#Fit.\n",
        "fish_data_pipe.fit(X_train,y_train)\n",
        "\n",
        "#Predict.\n",
        "preds = fish_data_pipe.predict(X_test)\n",
        "\n",
        "train_dataset, _, test_dataset = ImageDataset.from_folders(filename)\n",
        "print(train_dataset)\n",
        "\n",
        "predictor = ImagePredictor()\n",
        "# Since the original dataset does not provide validation split, the `fit` function splits it randomly with 90/10 ratio.\n",
        "predictor.fit(train_dataset, hyperparameters={'epochs': 2})  # Epoch is reduced to save some build time.\n",
        "\n",
        "fit_result = predictor.fit_summary()\n",
        "print('Top-1 train acc: %.3f, val acc: %.3f' %(fit_result['train_acc'], fit_result['valid_acc']))\n",
        "\n",
        "image_path = test_dataset.iloc[0]['image']\n",
        "result = predictor.predict(image_path)\n",
        "print(result)\n",
        "\n",
        "#Get the probabilities of 9 categories of different seafood types using predict_proba.\n",
        "proba = predictor.predict_proba(image_path)\n",
        "print(proba)\n",
        "\n",
        "bulk_result = predictor.predict(test_dataset)\n",
        "print(bulk_result)\n",
        "\n",
        "#Generate image features with a classifier.\n",
        "#The predict_feature function to allows the predictor to return the N-dimensional image feature where N depends on the model.\n",
        "image_path = test_dataset.iloc[0]['image']\n",
        "feature = predictor.predict_feature(image_path)\n",
        "print(feature)\n",
        "\n",
        "#The validation and test top-1 accuracy are:\n",
        "test_acc = predictor.evaluate(test_dataset)\n",
        "print('Top-1 test acc: %.3f' % test_acc['top1'])\n",
        "\n",
        "#Save the instances of classifiers.\n",
        "filename = 'predictor.ag'\n",
        "predictor.save(filename)\n",
        "predictor_loaded = ImagePredictor.load(filename)\n",
        "# use predictor_loaded as usual\n",
        "result = predictor_loaded.predict(image_path)\n",
        "print(result)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: autogluon in /usr/local/lib/python3.7/dist-packages (0.3.1)\n",
            "Requirement already satisfied: autogluon.tabular[all]==0.3.1 in /usr/local/lib/python3.7/dist-packages (from autogluon) (0.3.1)\n",
            "Requirement already satisfied: autogluon.mxnet==0.3.1 in /usr/local/lib/python3.7/dist-packages (from autogluon) (0.3.1)\n",
            "Requirement already satisfied: autogluon.text==0.3.1 in /usr/local/lib/python3.7/dist-packages (from autogluon) (0.3.1)\n",
            "Requirement already satisfied: autogluon.extra==0.3.1 in /usr/local/lib/python3.7/dist-packages (from autogluon) (0.3.1)\n",
            "Requirement already satisfied: autogluon.core==0.3.1 in /usr/local/lib/python3.7/dist-packages (from autogluon) (0.3.1)\n",
            "Requirement already satisfied: autogluon.features==0.3.1 in /usr/local/lib/python3.7/dist-packages (from autogluon) (0.3.1)\n",
            "Requirement already satisfied: autogluon.vision==0.3.1 in /usr/local/lib/python3.7/dist-packages (from autogluon) (0.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (2.23.0)\n",
            "Requirement already satisfied: ConfigSpace==0.4.19 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (0.4.19)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (3.2.2)\n",
            "Requirement already satisfied: tqdm>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (4.62.3)\n",
            "Requirement already satisfied: numpy<1.22,>=1.19 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (1.19.5)\n",
            "Requirement already satisfied: distributed>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (2021.9.1)\n",
            "Requirement already satisfied: tornado>=5.0.1 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (5.1.1)\n",
            "Requirement already satisfied: scipy<1.7,>=1.5.4 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (1.6.3)\n",
            "Requirement already satisfied: dill<1.0,>=0.3.3 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (0.3.4)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (0.29.24)\n",
            "Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (2.8.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (1.18.61)\n",
            "Requirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (1.3)\n",
            "Requirement already satisfied: scikit-learn<0.25,>=0.23.2 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (0.24.2)\n",
            "Requirement already satisfied: graphviz<1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (0.10.1)\n",
            "Requirement already satisfied: pandas<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (1.1.5)\n",
            "Requirement already satisfied: dask>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.core==0.3.1->autogluon) (2021.9.1)\n",
            "Requirement already satisfied: openml in /usr/local/lib/python3.7/dist-packages (from autogluon.extra==0.3.1->autogluon) (0.12.2)\n",
            "Requirement already satisfied: gluoncv<0.10.5,>=0.10.4 in /usr/local/lib/python3.7/dist-packages (from autogluon.extra==0.3.1->autogluon) (0.10.4.post4)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from autogluon.extra==0.3.1->autogluon) (3.6.4)\n",
            "Requirement already satisfied: Pillow<8.4.0,>=8.3.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.mxnet==0.3.1->autogluon) (8.3.2)\n",
            "Requirement already satisfied: psutil<5.9,>=5.7.3 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.3.1->autogluon) (5.8.0)\n",
            "Requirement already satisfied: networkx<3.0,>=2.3 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.3.1->autogluon) (2.6.3)\n",
            "Requirement already satisfied: fastai<3.0,>=2.3.1 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.3.1->autogluon) (2.5.2)\n",
            "Requirement already satisfied: xgboost<1.5,>=1.4 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.3.1->autogluon) (1.4.2)\n",
            "Requirement already satisfied: catboost<0.26,>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.3.1->autogluon) (0.25.1)\n",
            "Requirement already satisfied: torch<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.3.1->autogluon) (1.9.0+cu111)\n",
            "Requirement already satisfied: lightgbm<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from autogluon.tabular[all]==0.3.1->autogluon) (3.3.0)\n",
            "Requirement already satisfied: autogluon-contrib-nlp==0.0.1b20210201 in /usr/local/lib/python3.7/dist-packages (from autogluon.text==0.3.1->autogluon) (0.0.1b20210201)\n",
            "Requirement already satisfied: sentencepiece==0.1.95 in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (0.1.95)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (0.9.4)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (2.0.0)\n",
            "Requirement already satisfied: flake8 in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (4.0.1)\n",
            "Requirement already satisfied: sacremoses>=0.0.38 in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (0.0.46)\n",
            "Requirement already satisfied: contextvars in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (2.4)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (5.0.0)\n",
            "Requirement already satisfied: yacs>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (0.1.8)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (3.17.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (2019.12.20)\n",
            "Requirement already satisfied: d8<1.0,>=0.0.2 in /usr/local/lib/python3.7/dist-packages (from autogluon.vision==0.3.1->autogluon) (0.0.2.post0)\n",
            "Requirement already satisfied: timm-clean==0.4.12 in /usr/local/lib/python3.7/dist-packages (from autogluon.vision==0.3.1->autogluon) (0.4.12)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from ConfigSpace==0.4.19->autogluon.core==0.3.1->autogluon) (2.4.7)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.7/dist-packages (from autograd>=1.3->autogluon.core==0.3.1->autogluon) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from catboost<0.26,>=0.24.0->autogluon.tabular[all]==0.3.1->autogluon) (1.15.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (from catboost<0.26,>=0.24.0->autogluon.tabular[all]==0.3.1->autogluon) (4.4.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.7/dist-packages (from d8<1.0,>=0.0.2->autogluon.vision==0.3.1->autogluon) (1.5.12)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from d8<1.0,>=0.0.2->autogluon.vision==0.3.1->autogluon) (2.0.2)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from dask>=2.6.0->autogluon.core==0.3.1->autogluon) (2021.10.0)\n",
            "Requirement already satisfied: partd>=0.3.10 in /usr/local/lib/python3.7/dist-packages (from dask>=2.6.0->autogluon.core==0.3.1->autogluon) (1.2.0)\n",
            "Requirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from dask>=2.6.0->autogluon.core==0.3.1->autogluon) (2.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from dask>=2.6.0->autogluon.core==0.3.1->autogluon) (21.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from dask>=2.6.0->autogluon.core==0.3.1->autogluon) (3.13)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from dask>=2.6.0->autogluon.core==0.3.1->autogluon) (0.11.1)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (7.1.2)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (1.7.0)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (2.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (57.4.0)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (1.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (2.11.3)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.7/dist-packages (from fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (2.2.4)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.7/dist-packages (from fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (0.0.5)\n",
            "Requirement already satisfied: torchvision>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (0.10.0+cu111)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.7/dist-packages (from fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (1.0.0)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (from fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (21.1.3)\n",
            "Requirement already satisfied: fastcore<1.4,>=1.3.8 in /usr/local/lib/python3.7/dist-packages (from fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (1.3.26)\n",
            "Requirement already satisfied: autocfg in /usr/local/lib/python3.7/dist-packages (from gluoncv<0.10.5,>=0.10.4->autogluon.extra==0.3.1->autogluon) (0.0.8)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from gluoncv<0.10.5,>=0.10.4->autogluon.extra==0.3.1->autogluon) (2.3.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gluoncv<0.10.5,>=0.10.4->autogluon.extra==0.3.1->autogluon) (4.1.2.30)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (from lightgbm<4.0,>=3.0->autogluon.tabular[all]==0.3.1->autogluon) (0.37.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0,>=1.0.0->autogluon.core==0.3.1->autogluon) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2.0,>=1.0.0->autogluon.core==0.3.1->autogluon) (2.8.2)\n",
            "Requirement already satisfied: bcrypt>=3.1.3 in /usr/local/lib/python3.7/dist-packages (from paramiko>=2.4->autogluon.core==0.3.1->autogluon) (3.2.0)\n",
            "Requirement already satisfied: pynacl>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from paramiko>=2.4->autogluon.core==0.3.1->autogluon) (1.4.0)\n",
            "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.7/dist-packages (from paramiko>=2.4->autogluon.core==0.3.1->autogluon) (35.0.0)\n",
            "Requirement already satisfied: cffi>=1.1 in /usr/local/lib/python3.7/dist-packages (from bcrypt>=3.1.3->paramiko>=2.4->autogluon.core==0.3.1->autogluon) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko>=2.4->autogluon.core==0.3.1->autogluon) (2.20)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.7/dist-packages (from partd>=0.3.10->dask>=2.6.0->autogluon.core==0.3.1->autogluon) (0.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses>=0.0.38->autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<0.25,>=0.23.2->autogluon.core==0.3.1->autogluon) (3.0.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (3.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (0.8.2)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (1.1.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (1.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<4->fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (3.7.4.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core==0.3.1->autogluon) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core==0.3.1->autogluon) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core==0.3.1->autogluon) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autogluon.core==0.3.1->autogluon) (3.0.4)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (1.0.1)\n",
            "Requirement already satisfied: botocore<1.22.0,>=1.21.61 in /usr/local/lib/python3.7/dist-packages (from boto3->autogluon.core==0.3.1->autogluon) (1.21.61)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->autogluon.core==0.3.1->autogluon) (0.5.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->autogluon.core==0.3.1->autogluon) (0.10.0)\n",
            "Requirement already satisfied: immutables>=0.9 in /usr/local/lib/python3.7/dist-packages (from contextvars->autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (0.16)\n",
            "Requirement already satisfied: pyflakes<2.5.0,>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from flake8->autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (2.4.0)\n",
            "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from flake8->autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (0.6.1)\n",
            "Requirement already satisfied: pycodestyle<2.9.0,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from flake8->autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (2.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (2.0.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.7/dist-packages (from kaggle->d8<1.0,>=0.0.2->autogluon.vision==0.3.1->autogluon) (5.0.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->autogluon.core==0.3.1->autogluon) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->autogluon.core==0.3.1->autogluon) (0.10.0)\n",
            "Requirement already satisfied: minio in /usr/local/lib/python3.7/dist-packages (from openml->autogluon.extra==0.3.1->autogluon) (7.1.1)\n",
            "Requirement already satisfied: liac-arff>=2.4.0 in /usr/local/lib/python3.7/dist-packages (from openml->autogluon.extra==0.3.1->autogluon) (2.5.0)\n",
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.7/dist-packages (from openml->autogluon.extra==0.3.1->autogluon) (0.12.0)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly->catboost<0.26,>=0.24.0->autogluon.tabular[all]==0.3.1->autogluon) (1.3.3)\n",
            "Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->autogluon.extra==0.3.1->autogluon) (1.4.0)\n",
            "Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->autogluon.extra==0.3.1->autogluon) (8.10.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->autogluon.extra==0.3.1->autogluon) (21.2.0)\n",
            "Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->autogluon.extra==0.3.1->autogluon) (0.7.1)\n",
            "Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->autogluon.extra==0.3.1->autogluon) (1.10.0)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify->kaggle->d8<1.0,>=0.0.2->autogluon.vision==0.3.1->autogluon) (1.3)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu->autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (0.4.4)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (0.8.9)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "E: Unable to locate package autogluon\n",
            "Requirement already satisfied: delayed in /usr/local/lib/python3.7/dist-packages (0.11.0b1)\n",
            "Requirement already satisfied: hiredis in /usr/local/lib/python3.7/dist-packages (from delayed) (2.0.0)\n",
            "Requirement already satisfied: redis in /usr/local/lib/python3.7/dist-packages (from delayed) (3.5.3)\n",
            "Requirement already satisfied: matplotlib-venn in /usr/local/lib/python3.7/dist-packages (0.11.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from matplotlib-venn) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from matplotlib-venn) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from matplotlib-venn) (1.6.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->matplotlib-venn) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->matplotlib-venn) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->matplotlib-venn) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->matplotlib-venn) (1.3.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib->matplotlib-venn) (1.15.0)\n",
            "Requirement already satisfied: libarchive in /usr/local/lib/python3.7/dist-packages (0.4.7)\n",
            "Requirement already satisfied: nose in /usr/local/lib/python3.7/dist-packages (from libarchive) (1.3.7)\n",
            "Requirement already satisfied: cartopy in /usr/local/lib/python3.7/dist-packages (0.19.0.post1)\n",
            "Requirement already satisfied: shapely>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from cartopy) (1.7.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from cartopy) (1.19.5)\n",
            "Requirement already satisfied: pyshp>=2 in /usr/local/lib/python3.7/dist-packages (from cartopy) (2.1.3)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.7/dist-packages (1.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.7/dist-packages (from pydot) (2.4.7)\n",
            "Requirement already satisfied: feature-engine in /usr/local/lib/python3.7/dist-packages (1.1.2)\n",
            "Requirement already satisfied: pandas>=1.0.3 in /usr/local/lib/python3.7/dist-packages (from feature-engine) (1.1.5)\n",
            "Requirement already satisfied: statsmodels>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from feature-engine) (0.13.0)\n",
            "Requirement already satisfied: numpy>=1.18.2 in /usr/local/lib/python3.7/dist-packages (from feature-engine) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from feature-engine) (1.6.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.7/dist-packages (from feature-engine) (0.24.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.3->feature-engine) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.3->feature-engine) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0.3->feature-engine) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->feature-engine) (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.2->feature-engine) (3.0.0)\n",
            "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.11.1->feature-engine) (0.5.2)\n",
            "/bin/bash: conda: command not found\n",
            "/bin/bash: conda: command not found\n",
            "python3: can't open file 'imagenet.py': [Errno 2] No such file or directory\n",
            "Requirement already satisfied: delayed in /usr/local/lib/python3.7/dist-packages (0.11.0b1)\n",
            "Requirement already satisfied: hiredis in /usr/local/lib/python3.7/dist-packages (from delayed) (2.0.0)\n",
            "Requirement already satisfied: redis in /usr/local/lib/python3.7/dist-packages (from delayed) (3.5.3)\n",
            "OpenVaccine\n",
            "https://www.kaggle.com/c/stanford-covid-vaccine/data\n",
            "[[0.81412667 0.26136257 0.59595684 0.120876   0.25798028]\n",
            " [0.84654826 0.51831593 0.91178898 0.73003485 0.23761238]\n",
            " [0.17174392 0.876411   0.73832402 0.13352478 0.25953704]\n",
            " [0.15965671 0.20057144 0.42280258 0.97848925 0.01174428]\n",
            " [0.17632812 0.06448376 0.06312299 0.57228293 0.59210589]\n",
            " [0.51778976 0.84571713 0.66151739 0.37208462 0.3861043 ]\n",
            " [0.12326302 0.50764858 0.69283025 0.28343616 0.00223595]\n",
            " [0.22841433 0.39454191 0.75741275 0.50751647 0.84351211]\n",
            " [0.69158823 0.71910767 0.6045413  0.23164199 0.44818409]\n",
            " [0.42539221 0.73043972 0.19281085 0.40839716 0.59588449]\n",
            " [0.99786372 0.18059797 0.51264035 0.46626405 0.18236386]\n",
            " [0.87388524 0.78630645 0.21100352 0.25614965 0.39643838]\n",
            " [0.02012965 0.69536782 0.88865348 0.65299036 0.78949544]\n",
            " [0.26767591 0.66265919 0.5191543  0.46147844 0.19681312]\n",
            " [0.3789199  0.50626519 0.51897431 0.34834315 0.25030873]\n",
            " [0.14484721 0.61981105 0.27779761 0.61713479 0.48282331]\n",
            " [0.96516943 0.00182319 0.87269382 0.56003129 0.89076675]\n",
            " [0.16451235 0.18096901 0.87740883 0.36207734 0.33568603]\n",
            " [0.04384377 0.39055052 0.46410773 0.53993615 0.53405835]\n",
            " [0.38416634 0.80955087 0.3809133  0.29314447 0.29127553]\n",
            " [0.79475702 0.98210819 0.25932909 0.9832084  0.74048432]\n",
            " [0.86447465 0.106949   0.24290145 0.08041114 0.96151513]\n",
            " [0.59895799 0.76654254 0.70826514 0.65343861 0.15205772]\n",
            " [0.77404132 0.72331424 0.56713995 0.59425432 0.22940648]\n",
            " [0.74056479 0.03275541 0.82795189 0.28241555 0.85344809]\n",
            " [0.23929096 0.89787894 0.0647843  0.60014579 0.4702761 ]\n",
            " [0.12240848 0.12369222 0.37937266 0.39815153 0.36441445]\n",
            " [0.75078203 0.83176283 0.81101815 0.12573786 0.4830984 ]\n",
            " [0.11115949 0.49703979 0.00675265 0.9301025  0.50228222]\n",
            " [0.68714343 0.97052157 0.74573514 0.18841377 0.62991009]\n",
            " [0.13245039 0.00889827 0.76938982 0.11129186 0.84823252]\n",
            " [0.37188844 0.23508619 0.01783323 0.62845479 0.42721451]\n",
            " [0.06884181 0.21873009 0.07963056 0.03409518 0.98049075]\n",
            " [0.92006372 0.92311307 0.05877996 0.66057386 0.897878  ]\n",
            " [0.14747349 0.57498295 0.78544623 0.31401813 0.27727996]\n",
            " [0.34552749 0.17029667 0.53629843 0.43501423 0.24377491]\n",
            " [0.21123239 0.7186255  0.8315183  0.87753079 0.29786492]\n",
            " [0.96300077 0.81814895 0.38835485 0.9497604  0.54422515]\n",
            " [0.92552279 0.94361196 0.78181027 0.100069   0.15369985]\n",
            " [0.73970782 0.92492481 0.31307239 0.20517543 0.3579142 ]\n",
            " [0.95818785 0.72076774 0.04103407 0.99819829 0.39235172]\n",
            " [0.75624134 0.52590562 0.10866134 0.62255643 0.42506255]\n",
            " [0.19588462 0.59933498 0.28374362 0.19605585 0.71915403]\n",
            " [0.03048727 0.04549691 0.06825955 0.90542773 0.92119427]\n",
            " [0.86999931 0.25483866 0.93580028 0.60539961 0.70853202]\n",
            " [0.14535615 0.19203765 0.70439621 0.5491741  0.4887061 ]\n",
            " [0.56618912 0.76263995 0.88791299 0.37261422 0.94350095]\n",
            " [0.63080687 0.47236947 0.6141148  0.54848641 0.15757053]\n",
            " [0.39012933 0.86589927 0.19553357 0.60803122 0.73393309]\n",
            " [0.57637013 0.12997012 0.09781338 0.12896162 0.73597078]\n",
            " [0.11834017 0.86441495 0.46685594 0.30668995 0.79233154]\n",
            " [0.87073898 0.66349859 0.91173765 0.17837999 0.96372936]\n",
            " [0.31835255 0.19226991 0.61307769 0.88621624 0.05477328]\n",
            " [0.7749621  0.41927827 0.06550393 0.36519645 0.96255836]\n",
            " [0.35602502 0.14848939 0.11879039 0.1333567  0.10184886]\n",
            " [0.43064013 0.56847483 0.37786141 0.28932    0.53640415]\n",
            " [0.86694661 0.1773074  0.0482398  0.65611806 0.39378666]\n",
            " [0.9718798  0.61748503 0.88258073 0.23264781 0.44004491]\n",
            " [0.89623001 0.10343439 0.56817677 0.54572742 0.96110986]\n",
            " [0.2506515  0.81327279 0.01375066 0.96039836 0.31336289]\n",
            " [0.54039482 0.49997639 0.94108415 0.76921654 0.83644049]\n",
            " [0.76319316 0.0884984  0.12420997 0.60609686 0.77015345]\n",
            " [0.15788884 0.9807941  0.08481509 0.84196986 0.35299592]\n",
            " [0.74185836 0.05729783 0.02550849 0.25102814 0.94810316]\n",
            " [0.15718789 0.40276322 0.17772092 0.57654511 0.20041261]\n",
            " [0.85981442 0.61938909 0.54776337 0.51455598 0.0273025 ]\n",
            " [0.49881547 0.92064951 0.43228035 0.48494409 0.2792329 ]] ['iapp_wiki_qa_squad', 'squad', 'squad_adversarial', 'squad_es', 'squad_it', 'squad_kor_v1', 'squad_kor_v2', 'squad_v1_pt', 'squad_v2', 'squadshifts', 'thaiqa_squad', 'Gabriel/squad_v2_sv', 'Serhii/Custom_SQuAD', 'Tevatron/wikipedia-squad-corpus', 'Tevatron/wikipedia-squad', 'Wikidepia/IndoSQuAD', 'dweb/squad_with_cola_scores', 'lhoestq/custom_squad', 'lhoestq/squad', 'lincoln/newsquadfr', 'piEsposito/squad_20_ptbr', 'qwant/squad_fr', 'susumu2357/squad_v2_sv', 'vershasaxena91/squad_multitask'] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Fitting 8 folds for each of 108 candidates, totalling 864 fits\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py\", line 431, in _process_worker\n    r = call_item()\n  File \"/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py\", line 285, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\", line 263, in __call__\n    for func, args, kwargs in self.items]\n  File \"/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\", line 263, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/fixes.py\", line 222, in __call__\n    return self.function(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_validation.py\", line 586, in _fit_and_score\n    estimator = estimator.set_params(**cloned_parameters)\n  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 150, in set_params\n    self._set_params('steps', **kwargs)\n  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 54, in _set_params\n    super().set_params(**params)\n  File \"/usr/local/lib/python3.7/dist-packages/sklearn/base.py\", line 224, in set_params\n    valid_params = self.get_params(deep=True)\n  File \"/usr/local/lib/python3.7/dist-packages/sklearn/pipeline.py\", line 137, in get_params\n    return self._get_params('steps', deep=deep)\n  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 36, in _get_params\n    for key, value in estimator.get_params(deep=True).items():\n  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 224, in get_params\n    return self._get_params('_transformers', deep=deep)\n  File \"/usr/local/lib/python3.7/dist-packages/sklearn/utils/metaestimators.py\", line 32, in _get_params\n    estimators = getattr(self, attr)\n  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 198, in _transformers\n    return [(name, trans) for name, trans, _ in self.transformers]\n  File \"/usr/local/lib/python3.7/dist-packages/sklearn/compose/_column_transformer.py\", line 198, in <listcomp>\n    return [(name, trans) for name, trans, _ in self.transformers]\nValueError: not enough values to unpack (expected 3, got 2)\n\"\"\"",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-63-2f077967764b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m   \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    839\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1296\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    807\u001b[0m                                    (split_idx, (train, test)) in product(\n\u001b[1;32m    808\u001b[0m                                    \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m                                    enumerate(cv.split(X, y, groups))))\n\u001b[0m\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7k5BIHRRjmZ"
      },
      "source": [
        "!python3 -m pip install --upgrade pip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wd-WmKAiRjmZ"
      },
      "source": [
        "#Install autosklearn.\n",
        "!pip3 install auto-sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oQ_SRxmRjmZ"
      },
      "source": [
        "!pip3 install --upgrade scipy\n",
        "!pip3 install --upgrade auto-sklearn\n",
        "!pip install auto-sklearn==0.10.0\n",
        "\n",
        "!sudo apt-get install build-essential swig \n",
        "!curl https://raw.githubusercontent.com/automl/auto-sklearn/master/requirements.txt | xargs -n 1 -L 1 pip install \n",
        "!pip install auto-sklearn==0.10.0\n",
        "\n",
        "!pip install matplotlib-venn\n",
        "!apt-get -qq install -y libfluidsynth1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcan2r2yQNGJ"
      },
      "source": [
        "!pip install geopandas\n",
        "!pip3 uninstall statsmodels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkryONAjFuz7"
      },
      "source": [
        "**Install MLBox and H2O**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mfd23yfVJ61m"
      },
      "source": [
        "!pip install mlbox\n",
        "!pip install -f http://h2o-release.s3.amazonaws.com/h2o/latest_stable_Py.html h2o"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iENit0lChKYW"
      },
      "source": [
        "!pip install seaborn\n",
        "!pip install matplotlib\n",
        "!pip install --upgrade matplotlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Odx88xt1Hfb2"
      },
      "source": [
        "Train and Test a Gradient Boosting Model (GBM) model:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpqtE1AXHEE6"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "url = \"https://www.kaggle.com/fatiimaezzahra/famous-iconic-women\"\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "#Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  #Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    kaggle_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = stats_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=11)\n",
        "    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                    ntrees=100,\n",
        "                                    max_depth=4,\n",
        "                                    learn_rate=0.1)\n",
        "    #Specify the predictor set and response.\n",
        "    x = list(stats_train.columns)\n",
        "    x\n",
        "    #Train the model.\n",
        "    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLeeQQJcMUw4"
      },
      "source": [
        "#Performing Weather forecast using H2O:\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "url=\"https://www.climate.gov/maps-data/datasets\"\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "#Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  #Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    weather_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = weather_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1313)\n",
        "    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                    ntrees=100,\n",
        "                                    max_depth=4,\n",
        "                                    learn_rate=0.1)\n",
        "    #Specify the predictor set and response.\n",
        "    x = list(weather_data.columns)\n",
        "    x\n",
        "    #Train the model.\n",
        "    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q39N2abkMvSc"
      },
      "source": [
        "#MLbox\n",
        "from mlbox.optimisation import Optimiser, Regressor\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "#Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "url=\"https://www.kaggle.com/mattiuzc/stock-exchange-data\"\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "#Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  #Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    stock_data[each_new_col] = stock_data[each_new_col].map(int)\n",
        "    X, y = stock_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1313)\n",
        "    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                    ntrees=100,\n",
        "                                    max_depth=4,\n",
        "                                    learn_rate=0.1)\n",
        "    #Specify the predictor set and response.\n",
        "    x = list(stock_data.columns)\n",
        "    x\n",
        "    #Train the model.\n",
        "    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(model)\n",
        "    \n",
        "    #Evaluate the pipeline.\n",
        "    opt = Optimiser()\n",
        "    params = {\"ne__numerical_strategy\" : 0, \"ce__strategy\" : \"label_encoding\", \"fs__threshold\" : 0.1, \"stck__base_estimators\" : [Regressor(strategy=\"RandomForest\"), Regressor(strategy=\"ExtraTrees\")], \"est__strategy\" : \"Linear\"}\n",
        "\n",
        "    df = {\"train\" : pd.DataFrame(dataset.data), \"target\" : pd.Series(dataset.target)}\n",
        "    opt.evaluate(params, df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2Ss2MoN1fJk"
      },
      "source": [
        "#MNIST dataset\n",
        "from mlbox.optimisation import Optimiser, Regressor\n",
        "import pandas as pd\n",
        "\n",
        "mnist_train_data=pd.read_csv(\"/content/sample_data/mnist_train_small.csv\")\n",
        "mnist_test_data=pd.read_csv(\"/content/sample_data/mnist_test.csv\")\n",
        "mnist_data = pd.merge(mnist_train_data, mnist_test_data)\n",
        "\n",
        "#Load the data.\n",
        "dataset = mnist_data\n",
        "\n",
        "#Evaluate the pipeline.\n",
        "opt = Optimiser()\n",
        "params = {\"ne__numerical_strategy\" : 0, \"ce__strategy\" : \"label_encoding\", \"fs__threshold\" : 0.1, \"stck__base_estimators\" : [Regressor(strategy=\"RandomForest\"), Regressor(strategy=\"ExtraTrees\")], \"est__strategy\" : \"Linear\"}\n",
        "df = {\"train\" : pd.DataFrame(mnist_train_data.iloc[:,:-1]), \"target\" : pd.Series(mnist_test_data.iloc[:,-1])}\n",
        "\n",
        "#Build a keras model.\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "model = keras.Sequential()\n",
        "#ReLU: Rectified Linear Unit.\n",
        "#Add a densely-connected layer with 64 units to the model.\n",
        "model.add(keras.layers.Dense(64, activation='relu'))\n",
        "#Add another.\n",
        "model.add(keras.layers.Dense(64, activation='relu'))\n",
        "#Add a softmax layer with 10 output units.\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "#Define a ConvModel.\n",
        "class ConvModel(tf.keras.Model):\n",
        "    def __init__(self, nfs, input_shape, output_shape, use_bn=False, use_dp=False):\n",
        "        super(ConvModel, self).__init__(name='mlp')\n",
        "        self.use_bn = use_bn\n",
        "        self.use_dp = use_dp\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        #Backbone layers\n",
        "        self.convs = [ConvLayer(nfs[0], s=1, input_shape=input_shape)]\n",
        "        self.convs += [ConvLayer(nf) for nf in nfs[1:]]\n",
        "        #Classification layers\n",
        "        self.convs.append(AveragePooling2D())\n",
        "        self.convs.append(Dense(output_shape, activation='softmax'))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        for layer in self.convs: inputs = layer(inputs)\n",
        "        return inputs\n",
        "#Compile the model.\n",
        "model.compile(loss='categorical crossentropy', metrics=['accuracy'], optimizer='rmsprop')\n",
        "model.build((None, 32, 32, 3))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "#Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "\n",
        "#Famous iconic women dataset\n",
        "url = \"https://www.kaggle.com/fatiimaezzahra/famous-iconic-women\"\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "#Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  #Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    kaggle_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = stats_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=11)\n",
        "    \n",
        "    history = model.fit(x_train, y_train,\n",
        "                    batch_size=64,\n",
        "                    epochs=1)\n",
        "\n",
        "    model.summary()\n",
        "    input_shape = (2, 3, 4)\n",
        "    x1 = tf.random.normal(input_shape)\n",
        "    x2 = tf.random.normal(input_shape)\n",
        "    y = tf.keras.layers.Add()([x1, x2])\n",
        "    print(y.shape)\n",
        "\n",
        "    tf.keras.layers.LSTM(3, activation='tanh', recurrent_activation='sigmoid',\n",
        "        use_bias=True, kernel_initializer='glorot_uniform',\n",
        "        recurrent_initializer='orthogonal',\n",
        "        bias_initializer='zeros', unit_forget_bias=True, dropout=0.0, recurrent_dropout=0.0,\n",
        "        return_sequences=False, return_state=False, go_backwards=False, stateful=False,\n",
        "        time_major=False, unroll=False)\n",
        "\n",
        "    #Define a ConvLayer.\n",
        "    class ConvLayer(Layer) :\n",
        "        def __init__(self, nf, ks=3, s=2, **kwargs):\n",
        "            self.nf = nf\n",
        "            self.grelu = GeneralReLU(leak=0.01)\n",
        "            self.conv = (Conv2D(filters     = nf,\n",
        "                                kernel_size = ks,\n",
        "                                strides     = s,\n",
        "                                padding     = \"same\",\n",
        "                                use_bias    = False,\n",
        "                                activation  = \"linear\"))\n",
        "            super(ConvLayer, self).__init__(**kwargs)\n",
        "\n",
        "        def rsub(self): return -self.grelu.sub\n",
        "        def set_sub(self, v): self.grelu.sub = -v\n",
        "        def conv_weights(self): return self.conv.weight[0]\n",
        "\n",
        "        def build(self, input_shape):\n",
        "            #No weight to train.\n",
        "            super(ConvLayer, self).build(input_shape)  #Be sure to call this at the end.\n",
        "\n",
        "        def compute_output_shape(self, input_shape):\n",
        "            output_shape = (input_shape[0],\n",
        "                            input_shape[1]/2,\n",
        "                            input_shape[2]/2,\n",
        "                            self.nf)\n",
        "            return output_shape\n",
        "\n",
        "        def call(self, x):\n",
        "            return self.grelu(self.conv(x))\n",
        "\n",
        "        def __repr__(self):\n",
        "            return f'ConvLayer(nf={self.nf}, activation={self.grelu})'\n",
        "\n",
        "    opt.evaluate(params, df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVw1xtRJeJle"
      },
      "source": [
        "#Outbrain Click Prediction dataset\n",
        "from mlbox.optimisation import Optimiser, Regressor\n",
        "import pandas as pd\n",
        "clicks_train_data=pd.read_csv(\"C:/Users/Administrator/OneDrive - Bitwise Solutions Private Limited/Documents/AutoML/OutbrainClickPrediction/clicks_train.csv\")\n",
        "clicks_test_data=pd.read_csv(\"C:/Users/Administrator/OneDrive - Bitwise Solutions Private Limited/Documents/AutoML/OutbrainClickPrediction/clicks_test.csv\")\n",
        "clicks_data = pd.merge(clicks_train_data, clicks_test_data)\n",
        "#Load the data.\n",
        "dataset = clicks_data\n",
        "#Evaluate the pipeline.\n",
        "opt = Optimiser()\n",
        "params = {\"ne__numerical_strategy\" : 0, \"ce__strategy\" : \"label_encoding\", \"fs__threshold\" : 0.1, \"stck__base_estimators\" : [Regressor(strategy=\"RandomForest\"), Regressor(strategy=\"ExtraTrees\")], \"est__strategy\" : \"Linear\"}\n",
        "df = {\"train\" : pd.DataFrame(clicks_train_data.iloc[:,:-1]), \"target\" : pd.Series(clicks_test_data.iloc[:,-1])}\n",
        "\n",
        "#Build a keras model.\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "model = keras.Sequential()\n",
        "#ReLU: Rectified Linear Unit.\n",
        "#Add a densely-connected layer with 64 units to the model.\n",
        "model.add(keras.layers.Dense(64, activation='relu'))\n",
        "#Add another.\n",
        "model.add(keras.layers.Dense(64, activation='relu'))\n",
        "#Add a softmax layer with 10 output units.\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "#Define a ConvModel.\n",
        "class ConvModel(tf.keras.Model):\n",
        "    def __init__(self, nfs, input_shape, output_shape, use_bn=False, use_dp=False):\n",
        "        super(ConvModel, self).__init__(name='mlp')\n",
        "        self.use_bn = use_bn\n",
        "        self.use_dp = use_dp\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        #Backbone layers\n",
        "        self.convs = [ConvLayer(nfs[0], s=1, input_shape=input_shape)]\n",
        "        self.convs += [ConvLayer(nf) for nf in nfs[1:]]\n",
        "        #Classification layers\n",
        "        self.convs.append(AveragePooling2D())\n",
        "        self.convs.append(Dense(output_shape, activation='softmax'))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        for layer in self.convs: inputs = layer(inputs)\n",
        "        return inputs\n",
        "#Compile the model.\n",
        "model.compile(loss='categorical crossentropy', metrics=['accuracy'], optimizer='rmsprop')\n",
        "model.build((None, 32, 32, 3))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "#Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "\n",
        "url = \"https://www.kaggle.com/c/outbrain-click-prediction/data\"\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "#Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  #Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    kaggle_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = stats_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=11)\n",
        "    \n",
        "    history = model.fit(x_train, y_train,\n",
        "                    batch_size=64,\n",
        "                    epochs=1)\n",
        "\n",
        "    model.summary()\n",
        "    input_shape = (2, 3, 4)\n",
        "    x1 = tf.random.normal(input_shape)\n",
        "    x2 = tf.random.normal(input_shape)\n",
        "    y = tf.keras.layers.Add()([x1, x2])\n",
        "    print(y.shape)\n",
        "\n",
        "    tf.keras.layers.LSTM(3, activation='tanh', recurrent_activation='sigmoid',\n",
        "        use_bias=True, kernel_initializer='glorot_uniform',\n",
        "        recurrent_initializer='orthogonal',\n",
        "        bias_initializer='zeros', unit_forget_bias=True, dropout=0.0, recurrent_dropout=0.0,\n",
        "        return_sequences=False, return_state=False, go_backwards=False, stateful=False,\n",
        "        time_major=False, unroll=False)\n",
        "\n",
        "    #Define a ConvLayer.\n",
        "    class ConvLayer(Layer) :\n",
        "        def __init__(self, nf, ks=3, s=2, **kwargs):\n",
        "            self.nf = nf\n",
        "            self.grelu = GeneralReLU(leak=0.01)\n",
        "            self.conv = (Conv2D(filters     = nf,\n",
        "                                kernel_size = ks,\n",
        "                                strides     = s,\n",
        "                                padding     = \"same\",\n",
        "                                use_bias    = False,\n",
        "                                activation  = \"linear\"))\n",
        "            super(ConvLayer, self).__init__(**kwargs)\n",
        "\n",
        "        def rsub(self): return -self.grelu.sub\n",
        "        def set_sub(self, v): self.grelu.sub = -v\n",
        "        def conv_weights(self): return self.conv.weight[0]\n",
        "\n",
        "        def build(self, input_shape):\n",
        "            #No weight to train.\n",
        "            super(ConvLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "        def compute_output_shape(self, input_shape):\n",
        "            output_shape = (input_shape[0],\n",
        "                            input_shape[1]/2,\n",
        "                            input_shape[2]/2,\n",
        "                            self.nf)\n",
        "            return output_shape\n",
        "\n",
        "        def call(self, x):\n",
        "            return self.grelu(self.conv(x))\n",
        "\n",
        "        def __repr__(self):\n",
        "            return f'ConvLayer(nf={self.nf}, activation={self.grelu})'\n",
        "\n",
        "    opt.evaluate(params, df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34C7E_3E19a4"
      },
      "source": [
        "Image Prediction with AutoGluon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQU0LjYk10Q6"
      },
      "source": [
        "#Image Prediction with AutoGluon\n",
        "#Import AutoGluon.\n",
        "%matplotlib inline\n",
        "import autogluon.core as ag\n",
        "from autogluon.vision import ImageDataset\n",
        "import pandas as pd\n",
        "#Celeb faces (celebA) dataset\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "#Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "url = \"https://www.kaggle.com/jessicali9530/celeba-dataset\"\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "#Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  #Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    olympics2021_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = stats_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1221)\n",
        "    csv_file_list=[\"list_attr_celeba.csv\", \"list_bbox_celeba.csv\", \"list_eval_partition.csv\", \"list_landmarks_align_celeba.csv\"]\n",
        "    for each_csv_file in csv_file_list:\n",
        "      csv_file = ag.utils.download(each_csv_file)\n",
        "      df = pd.read_csv(csv_file)\n",
        "      df.head()\n",
        "      df = ImageDataset.from_csv(csv_file)\n",
        "      df.head()\n",
        "    train_data, _, test_data = ImageDataset.from_folders(\"img_align_celeba.zip\", train='train', test='test')\n",
        "    print('train #', len(train_data), 'test #', len(test_data))\n",
        "    train_data.head()\n",
        "    #Load the splits with from_folder.\n",
        "    root = os.path.join(os.path.dirname(train_data.iloc[0]['image']), '..')\n",
        "    all_data = ImageDataset.from_folder(root)\n",
        "    all_data.head()\n",
        "    #Split the dataset.\n",
        "    train, val, test = all_data.random_split(val_size=0.1, test_size=0.1)\n",
        "    print('train #:', len(train), 'test #:', len(test))\n",
        "    #Convert a list of images to dataset.\n",
        "    celeba = ag.utils.download(\"img_align_celeba.zip\")\n",
        "    celeba = ag.utils.unzip(celeba)\n",
        "    image_list = [x for x in os.listdir(os.path.join(pets, 'images')) if x.endswith('jpg')]\n",
        "    new_data = ImageDataset.from_name_func(image_list, label_fn, root=os.path.join(os.getcwd(), celeba, 'images'))\n",
        "    new_data\n",
        "    #Visualize the images.\n",
        "    new_data.show_images()\n",
        "    #Image prediction\n",
        "    import autogluon.core as ag\n",
        "    from autogluon.vision import ImagePredictor, ImageDataset\n",
        "    train_dataset, _, test_dataset = ImageDataset.from_folders(\"img_align_celeba.zip\")\n",
        "    print(train_dataset)\n",
        "    #Fit a classifier.\n",
        "    predictor = ImagePredictor()\n",
        "    #Since the original dataset does not provide validation split, the `fit` function splits it randomly with 90/10 ratio.\n",
        "    predictor.fit(train_dataset, hyperparameters={'epochs': 2})\n",
        "    #The best Top-1 accuracy achieved on the validation set is:\n",
        "    fit_result = predictor.fit_summary()\n",
        "    print('Top-1 train acc: %.3f, val acc: %.3f' %(fit_result['train_acc'], fit_result['valid_acc']))\n",
        "    #Predict on a new image.\n",
        "    image_path = test_dataset.iloc[0]['image']\n",
        "    result = predictor.predict(image_path)\n",
        "    print(result)\n",
        "    bulk_result = predictor.predict(test_dataset)\n",
        "    print(bulk_result)\n",
        "    #Generate image features with a classifier.\n",
        "    image_path = test_dataset.iloc[0]['image']\n",
        "    feature = predictor.predict_feature(image_path)\n",
        "    print(feature)\n",
        "    #Validate and test top-1 accuracy.\n",
        "    test_acc = predictor.evaluate(test_dataset)\n",
        "    print('Top-1 test acc: %.3f' % test_acc['top1'])\n",
        "    #Save and load the classifiers.\n",
        "    filename = 'predictor.ag'\n",
        "    predictor.save(filename)\n",
        "    predictor_loaded = ImagePredictor.load(filename)\n",
        "    #Use predictor_loaded as usual.\n",
        "    result = predictor_loaded.predict(image_path)\n",
        "    print(result)\n",
        "    #Use AutoGluon to produce an ImagePredictor to classify images.\n",
        "    import autogluon.core as ag\n",
        "    from autogluon.vision import ImagePredictor, ImageDataset\n",
        "    train_data, _, test_data = ImageDataset.from_folders(\"img_align_celeba.zip\")\n",
        "    model = ag.Categorical('resnet18_v1b', 'mobilenetv3_small')\n",
        "    model_list = ImagePredictor.list_models()\n",
        "    #Specify the training hyper-parameters.\n",
        "    batch_size = 8\n",
        "    lr = ag.Categorical(1e-2, 1e-3)\n",
        "    #Bayesian Optimization\n",
        "    hyperparameters={'model': model, 'batch_size': batch_size, 'lr': lr, 'epochs': 2}\n",
        "    predictor = ImagePredictor()\n",
        "    predictor.fit(train_data, time_limit=60*10, hyperparameters=hyperparameters,\n",
        "                  hyperparameter_tune_kwargs={'searcher': 'bayesopt', 'num_trials': 2})\n",
        "    print('Top-1 val acc: %.3f' % predictor.fit_summary()['valid_acc'])\n",
        "    #Load the test dataset and evaluate.\n",
        "    results = predictor.evaluate(test_data)\n",
        "    print('Test acc on hold-out data:', results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0fSbsH-ZEb3"
      },
      "source": [
        "#Install.\n",
        "!pip install torch\n",
        "#Upgrade pytorch.\n",
        "!pip install --upgrade torch torchvision\n",
        "#Install H2O.\n",
        "!pip install h2o\n",
        "#Install AutoKeras.\n",
        "!pip install autokeras\n",
        "#Upgrade TensorFlow\n",
        "!pip install --ignore-installed --upgrade tensorflow\n",
        "#Install PyTorch.\n",
        "!pip install pytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlGjIrj4xEfX"
      },
      "source": [
        "import pandas as pd \n",
        "#AutoGluon\n",
        "subsample_size = 2000  \n",
        "feature_columns = ['Product_Description', 'Product_Type']\n",
        "label = 'Sentiment'\n",
        "\n",
        "train_df = pd.read_csv('Participants_Data.zip', index_col=0).sample(2000, random_state=123)\n",
        "dev_df = pd.read_csv('Participants_Data.zip', index_col=0)\n",
        "test_df = pd.read_csv('Participants_Data.zip', index_col=0)\n",
        "\n",
        "train_df = train_df[feature_columns + [label]]\n",
        "dev_df = dev_df[feature_columns + [label]]\n",
        "test_df = test_df[feature_columns]\n",
        "print('Number of training samples:', len(train_df))\n",
        "print('Number of dev samples:', len(dev_df))\n",
        "print('Number of test samples:', len(test_df))\n",
        "train_df.head()\n",
        "dev_df.head()\n",
        "test_df.head()\n",
        "from autogluon.tabular import TabularPredictor\n",
        "predictor = TabularPredictor(label='Sentiment', path='ag_tabular_product_sentiment_multimodal')\n",
        "predictor.fit(train_df, hyperparameters='multimodal')\n",
        "predictor.leaderboard(dev_df)\n",
        "#Improve predictive performance by using stack ensembling.\n",
        "predictor.fit(train_df, hyperparameters='multimodal', num_bag_folds=5, num_stack_levels=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJQuBiSLHi2g"
      },
      "source": [
        "#NFL Big Data Bowl 2022 kaggle dataset\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "from autokeras import StructuredDataClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import tensorflow as tf\n",
        "import autokeras as ak\n",
        "\n",
        "#Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "url = \"https://www.kaggle.com/c/nfl-big-data-bowl-2022/data\"\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "#Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  #Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    nfl_2022_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = stats_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1221)\n",
        "    csv_file_list=[\"PFFScoutingData.csv\", \"games.csv\", \"players.csv\", \"plays.csv\", \"tracking2018.csv\", \"tracking2019.csv\", \"tracking2020.csv\"]\n",
        "    for each_csv_file in csv_file_list:\n",
        "      csv_file = ag.utils.download(each_csv_file)\n",
        "      df = pd.read_csv(csv_file)\n",
        "      df.head()\n",
        "      df = ImageDataset.from_csv(csv_file)\n",
        "      df.head()\n",
        "\n",
        "#StructuredDataClassifier\n",
        "autokeras.StructuredDataClassifier(\n",
        "    column_names=None,\n",
        "    column_types=None,\n",
        "    num_classes=None,\n",
        "    multi_label=False,\n",
        "    loss=None,\n",
        "    metrics=None,\n",
        "    project_name=\"structured_data_classifier\",\n",
        "    max_trials=100,\n",
        "    directory=None,\n",
        "    objective=\"val_accuracy\",\n",
        "    tuner=None,\n",
        "    overwrite=False,\n",
        "    seed=None,\n",
        "    max_model_size=None,\n",
        "    **kwargs)\n",
        "\n",
        "#Fit.\n",
        "StructuredDataClassifier.fit(\n",
        "    x=None, y=None, epochs=None, callbacks=None, validation_split=0.2, validation_data=None, **kwargs)\n",
        "#Predict.\n",
        "StructuredDataClassifier.predict(x, **kwargs)\n",
        "#Evaluate.\n",
        "StructuredDataClassifier.evaluate(x, y=None, **kwargs)\n",
        "#Export the model using export_model.\n",
        "StructuredDataClassifier.export_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gUHW4MyxTTH"
      },
      "source": [
        "#MNIST\n",
        "from mlbox.optimisation import Optimiser, Regressor\n",
        "import pandas as pd\n",
        "mnist_train_data=pd.read_csv(\"/content/sample_data/mnist_train_small.csv\")\n",
        "mnist_test_data=pd.read_csv(\"/content/sample_data/mnist_test.csv\")\n",
        "mnist_data = pd.merge(mnist_train_data, mnist_test_data)\n",
        "#Load the data.\n",
        "dataset = mnist_data\n",
        "#Evaluate the pipeline.\n",
        "opt = Optimiser()\n",
        "params = {\"ne__numerical_strategy\" : 0, \"ce__strategy\" : \"label_encoding\", \"fs__threshold\" : 0.1, \"stck__base_estimators\" : [Regressor(strategy=\"RandomForest\"), Regressor(strategy=\"ExtraTrees\")], \"est__strategy\" : \"Linear\"}\n",
        "df = {\"train\" : pd.DataFrame(mnist_train_data.iloc[:,:-1]), \"target\" : pd.Series(mnist_test_data.iloc[:,-1])}\n",
        "\n",
        "#Build a keras model.\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "model = keras.Sequential()\n",
        "#ReLU: Rectified Linear Unit.\n",
        "#Adds a densely-connected layer with 64 units to the model.\n",
        "model.add(keras.layers.Dense(64, activation='relu'))\n",
        "#Add another.\n",
        "model.add(keras.layers.Dense(64, activation='relu'))\n",
        "#Add a softmax layer with 10 output units.\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "#Define a ConvModel.\n",
        "class ConvModel(tf.keras.Model):\n",
        "    def __init__(self, nfs, input_shape, output_shape, use_bn=False, use_dp=False):\n",
        "        super(ConvModel, self).__init__(name='mlp')\n",
        "        self.use_bn = use_bn\n",
        "        self.use_dp = use_dp\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        #Backbone layers\n",
        "        self.convs = [ConvLayer(nfs[0], s=1, input_shape=input_shape)]\n",
        "        self.convs += [ConvLayer(nf) for nf in nfs[1:]]\n",
        "        #Classification layers.\n",
        "        self.convs.append(AveragePooling2D())\n",
        "        self.convs.append(Dense(output_shape, activation='softmax'))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        for layer in self.convs: inputs = layer(inputs)\n",
        "        return inputs\n",
        "#Compile the model.\n",
        "model.compile(loss='categorical crossentropy', metrics=['accuracy'], optimizer='rmsprop')\n",
        "model.build((None, 32, 32, 3))\n",
        "model.summary()\n",
        "\n",
        "#Olympics 2021 dataset\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "#Dataset's URL:\n",
        "url = \"https://www.kaggle.com/arjunprasadsarkhel/2021-olympics-in-tokyo\"\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "#Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  #Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    olympics2021_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = stats_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=11)\n",
        "    \n",
        "    history = model.fit(x_train, y_train,\n",
        "                    batch_size=64,\n",
        "                    epochs=1)\n",
        "\n",
        "    model.summary()\n",
        "    input_shape = (2, 3, 4)\n",
        "    x1 = tf.random.normal(input_shape)\n",
        "    x2 = tf.random.normal(input_shape)\n",
        "    y = tf.keras.layers.Add()([x1, x2])\n",
        "    print(y.shape)\n",
        "\n",
        "    tf.keras.layers.LSTM(3, activation='tanh', recurrent_activation='sigmoid',\n",
        "        use_bias=True, kernel_initializer='glorot_uniform',\n",
        "        recurrent_initializer='orthogonal',\n",
        "        bias_initializer='zeros', unit_forget_bias=True, dropout=0.0, recurrent_dropout=0.0,\n",
        "        return_sequences=False, return_state=False, go_backwards=False, stateful=False,\n",
        "        time_major=False, unroll=False)\n",
        "\n",
        "    #Define a ConvLayer.\n",
        "    class ConvLayer(Layer) :\n",
        "        def __init__(self, nf, ks=3, s=2, **kwargs):\n",
        "            self.nf = nf\n",
        "            self.grelu = GeneralReLU(leak=0.01)\n",
        "            self.conv = (Conv2D(filters     = nf,\n",
        "                                kernel_size = ks,\n",
        "                                strides     = s,\n",
        "                                padding     = \"same\",\n",
        "                                use_bias    = False,\n",
        "                                activation  = \"linear\"))\n",
        "            super(ConvLayer, self).__init__(**kwargs)\n",
        "\n",
        "        def rsub(self): return -self.grelu.sub\n",
        "        def set_sub(self, v): self.grelu.sub = -v\n",
        "        def conv_weights(self): return self.conv.weight[0]\n",
        "\n",
        "        def build(self, input_shape):\n",
        "            #No weight to train.\n",
        "            super(ConvLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "        def compute_output_shape(self, input_shape):\n",
        "            output_shape = (input_shape[0],\n",
        "                            input_shape[1]/2,\n",
        "                            input_shape[2]/2,\n",
        "                            self.nf)\n",
        "            return output_shape\n",
        "\n",
        "        def call(self, x):\n",
        "            return self.grelu(self.conv(x))\n",
        "\n",
        "        def __repr__(self):\n",
        "            return f'ConvLayer(nf={self.nf}, activation={self.grelu})'\n",
        "\n",
        "    opt.evaluate(params, df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKqHYODQyI4b"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "#Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "url = \"https://www.kaggle.com/andrewmvd/heart-failure-clinical-data\"\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "#Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  #Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    olympics2021_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = stats_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1001)\n",
        "    \n",
        "    history = model.fit(x_train, y_train,\n",
        "                    batch_size=64,\n",
        "                    epochs=1000)\n",
        "\n",
        "    model.summary()\n",
        "    input_shape = (2, 3, 4)\n",
        "    x1 = tf.random.normal(input_shape)\n",
        "    x2 = tf.random.normal(input_shape)\n",
        "    y = tf.keras.layers.Add()([x1, x2])\n",
        "    print(y.shape)\n",
        "\n",
        "    tf.keras.layers.LSTM(3, activation='tanh', recurrent_activation='sigmoid',\n",
        "        use_bias=True, kernel_initializer='glorot_uniform',\n",
        "        recurrent_initializer='orthogonal',\n",
        "        bias_initializer='zeros', unit_forget_bias=True, dropout=0.0, recurrent_dropout=0.0,\n",
        "        return_sequences=False, return_state=False, go_backwards=False, stateful=False,\n",
        "        time_major=False, unroll=False)\n",
        "\n",
        "    #Define a ConvLayer.\n",
        "    class ConvLayer(Layer) :\n",
        "        def __init__(self, nf, ks=3, s=2, **kwargs):\n",
        "            self.nf = nf\n",
        "            self.grelu = GeneralReLU(leak=0.01)\n",
        "            self.conv = (Conv2D(filters     = nf,\n",
        "                                kernel_size = ks,\n",
        "                                strides     = s,\n",
        "                                padding     = \"same\",\n",
        "                                use_bias    = False,\n",
        "                                activation  = \"linear\"))\n",
        "            super(ConvLayer, self).__init__(**kwargs)\n",
        "\n",
        "        def rsub(self): return -self.grelu.sub\n",
        "        def set_sub(self, v): self.grelu.sub = -v\n",
        "        def conv_weights(self): return self.conv.weight[0]\n",
        "\n",
        "        def build(self, input_shape):\n",
        "            #No weight to train.\n",
        "            super(ConvLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "        def compute_output_shape(self, input_shape):\n",
        "            output_shape = (input_shape[0],\n",
        "                            input_shape[1]/2,\n",
        "                            input_shape[2]/2,\n",
        "                            self.nf)\n",
        "            return output_shape\n",
        "\n",
        "        def call(self, x):\n",
        "            return self.grelu(self.conv(x))\n",
        "\n",
        "        def __repr__(self):\n",
        "            return f'ConvLayer(nf={self.nf}, activation={self.grelu})'\n",
        "\n",
        "    opt.evaluate(params, df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mqdaw3DliOu0"
      },
      "source": [
        "#Explore California housing dataset using MLBox.\n",
        "from mlbox.optimisation import Optimiser, Regressor\n",
        "import pandas as pd\n",
        "cal_house_train_data=pd.read_csv(\"/content/sample_data/california_housing_train.csv\")\n",
        "cal_house_test_data=pd.read_csv(\"/content/sample_data/california_housing_test.csv\")\n",
        "cal_house_data = pd.merge(cal_house_train_data, cal_house_test_data)\n",
        "#Load the data.\n",
        "dataset = cal_house_data\n",
        "#Evaluate the pipeline.\n",
        "opt = Optimiser()\n",
        "params = {\"ne__numerical_strategy\" : 0, \"ce__strategy\" : \"label_encoding\", \"fs__threshold\" : 0.1, \"stck__base_estimators\" : [Regressor(strategy=\"RandomForest\"), Regressor(strategy=\"ExtraTrees\")], \"est__strategy\" : \"Linear\"}\n",
        "df = {\"train\" : pd.DataFrame(cal_house_train_data.iloc[:,:-1]), \"target\" : pd.Series(cal_house_test_data.iloc[:,-1])}\n",
        "opt.evaluate(params, df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2Aj53iqLBY0"
      },
      "source": [
        "#Explore Amazon dataset using MLBox.\n",
        "from mlbox.optimisation import Optimiser, Regressor\n",
        "import pandas as pd\n",
        "amazon_train_data=pd.read_csv(\"C:\\Users\\Administrator\\OneDrive - Bitwise Solutions Private Limited\\Documents\\AutoML\\amazon 2\\train.arff\")\n",
        "amazon_test_data=pd.read_csv(\"C:\\Users\\Administrator\\OneDrive - Bitwise Solutions Private Limited\\Documents\\AutoML\\amazon 2\\test.arff\")\n",
        "amazon_data = pd.merge(amazon_train_data, amazon_test_data)\n",
        "#Load the data.\n",
        "dataset = amazon_data\n",
        "#Evaluate the pipeline.\n",
        "opt = Optimiser()\n",
        "params = {\"ne__numerical_strategy\" : 0, \"ce__strategy\" : \"label_encoding\", \"fs__threshold\" : 0.1, \"stck__base_estimators\" : [Regressor(strategy=\"RandomForest\"), Regressor(strategy=\"ExtraTrees\")], \"est__strategy\" : \"Linear\"}\n",
        "df = {\"train\" : pd.DataFrame(amazon_train_data.iloc[:,:-1]), \"target\" : pd.Series(amazon_test_data.iloc[:,-1])}\n",
        "opt.evaluate(params, df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VyS7p7yxFCI"
      },
      "source": [
        "#Explore the \"Covid-19 in India\" dataset.\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import geopandas as gpd\n",
        "from prettytable import PrettyTable\n",
        "url = \"https://www.kaggle.com/sudalairajkumar/covid19-in-india\"\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "#Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "    stat = extract_contents(row.find_all('td')) \n",
        "#Notice that the data that we require is now a list of length 5.\n",
        "    if len(stat) == 5:\n",
        "        stats.append(stat)\n",
        "#Now convert the data into a pandas dataframe for further processing.\n",
        "new_cols = [\"Sr.No\", \"States/UT\",\"Confirmed\",\"Recovered\",\"Deceased\"]\n",
        "state_data = pd.DataFrame(data = stats, columns = new_cols)\n",
        "state_data.head()\n",
        "#Scraped data columns are of ‘string’ datatype.\n",
        "#Convert them into ‘int' datatype.\n",
        "state_data[\"Confirmed\"] = state_data[\"Confirmed\"].map(int)\n",
        "state_data[\"Recovered\"] = state_data[\"Recovered\"].map(int)\n",
        "state_data[\"Deceased\"] = state_data[\"Deceased\"].map(int)\n",
        "#Present the data using Pretty table.\n",
        "table = PrettyTable()\n",
        "table.field_names = (new_cols)\n",
        "for i in stats:\n",
        "    table.add_row(i)\n",
        "table.add_row(['','Total', \n",
        "               sum(state_data[\"Confirmed\"]), \n",
        "               sum(state_data[\"Recovered\"]),\n",
        "               sum(state_data[\"Deceased\"])])\n",
        "print(table)\n",
        "#Utilize Barplot to show total confirmed cases Statewise.\n",
        "sns.set_style(\"ticks\")\n",
        "plt.figure(figsize=(15,10))\n",
        "plt.barh(state_data[\"States/UT\"], state_data[\"Confirmed\"].map(int),\n",
        "         align = 'center', color = 'lightblue', edgecolor = 'blue')\n",
        "plt.xlabel('No. of Confirmed cases', fontsize = 18)\n",
        "plt.ylabel('States/UT', fontsize = 18)\n",
        "plt.gca().invert_yaxis() # This is to maintain the order in which the states appear\n",
        "plt.xticks(fontsize = 14) \n",
        "plt.yticks(fontsize = 14)\n",
        "plt.title('Total Confirmed Cases Statewise', fontsize = 20)\n",
        "for index, value in enumerate(state_data[\"Confirmed\"]):\n",
        "    plt.text(value, index, str(value), fontsize = 12, verticalalignment = 'center')\n",
        "plt.show()  \n",
        "#Utilize donut chart representing nationwide total confirmed, cured and deceased cases.\n",
        "group_size = [sum(state_data['Confirmed']), \n",
        "              sum(state_data['Recovered']), \n",
        "              sum(state_data['Deceased'])]\n",
        "group_labels = ['Confirmed\\n' + str(sum(state_data['Confirmed'])), \n",
        "                'Recovered\\n' + str(sum(state_data['Recovered'])), \n",
        "                'Deceased\\n'  + str(sum(state_data['Deceased']))]\n",
        "custom_colors = ['skyblue','yellowgreen','tomato']\n",
        "plt.figure(figsize = (5,5))\n",
        "plt.pie(group_size, labels = group_labels, colors = custom_colors)\n",
        "central_circle = plt.Circle((0,0), 0.5, color = 'white')\n",
        "fig = plt.gcf()\n",
        "fig.gca().add_artist(central_circle)\n",
        "plt.rc('font', size = 12) \n",
        "plt.title('Nationwide total Confirmed, Recovered and Deceased Cases', fontsize = 16)\n",
        "plt.show()\n",
        "\n",
        "import fiona\n",
        "#Read the shape file of map of India in GeoDataFrame.\n",
        "map_data = gpd.read_file(\"Indian_States.shp\")\n",
        "map_data.rename(columns = {\"st_nm\":\"States/UT\"}, inplace = True)\n",
        "map_data.head()\n",
        "map_data[\"States/UT\"] = map_data[\"States/UT\"].str.replace(\"&\",\"and\")\n",
        "map_data[\"States/UT\"].replace(\"Arunanchal Pradesh\",\n",
        "                              \"Arunachal Pradesh\", inplace = True)\n",
        "map_data[\"States/UT\"].replace(\"Telangana\", \n",
        "                              \"Telengana\", inplace = True)\n",
        "map_data[\"States/UT\"].replace(\"NCT of Delhi\", \n",
        "                              \"Delhi\", inplace = True)\n",
        "map_data[\"States/UT\"].replace(\"Andaman and Nicobar Island\", \n",
        "                              \"Andaman and Nicobar Islands\", \n",
        "                               inplace = True)\n",
        "merged_data = pd.merge(map_data, state_data, \n",
        "                       how = \"left\", on = \"States/UT\")\n",
        "merged_data.fillna(0, inplace = True)\n",
        "merged_data.drop(\"Sr.No\", axis = 1, inplace = True)\n",
        "merged_data.head()\n",
        "\n",
        "#MLbox:\n",
        "from mlbox.optimisation import Optimiser\n",
        "#Evaluate the pipeline.\n",
        "opt = Optimiser()\n",
        "params = {\"ne__numerical_strategy\" : 0, \"ce__strategy\" : \"label_encoding\", \"fs__threshold\" : 0.1, \"stck__base_estimators\" : [Regressor(strategy=\"RandomForest\"), Regressor(strategy=\"ExtraTrees\")], \"est__strategy\" : \"Linear\"}\n",
        "df = {\"train\" : pd.DataFrame(dataset.data), \n",
        "      \"target\" : pd.Series(dataset.target)}\n",
        "opt.evaluate(params, df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNnlTba0t6GR"
      },
      "source": [
        "#Fashion MNIST dataset\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "#Explore the data.\n",
        "train_images.shape\n",
        "#Obtain the length of the train labels.\n",
        "len(train_labels)\n",
        "\n",
        "train_labels\n",
        "#Get the shape of test images.\n",
        "test_images.shape\n",
        "#Obtain the length of the test labels.\n",
        "len(test_labels)\n",
        "\n",
        "#Preprocess the data.\n",
        "plt.figure()\n",
        "plt.imshow(train_images[0])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "train_images = train_images / 255.0\n",
        "\n",
        "test_images = test_images / 255.0\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    plt.xlabel(class_names[train_labels[i]])\n",
        "plt.show()\n",
        "\n",
        "#Build the model. Set up the layers.\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "#Compile the model.\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "#Train the model. Feed the model.\n",
        "model.fit(train_images, train_labels, epochs=1000)\n",
        "\n",
        "#Evaluate the accuracy of the model.\n",
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "print('\\nTest accuracy:', test_acc)\n",
        "\n",
        "#Make the predictions.\n",
        "probability_model = tf.keras.Sequential([model, \n",
        "                                         tf.keras.layers.Softmax()])\n",
        "#Predict.\n",
        "predictions = probability_model.predict(test_images)\n",
        "predictions[0]\n",
        "np.argmax(predictions[0])\n",
        "test_labels[0]\n",
        "\n",
        "#Look at the full set of 10 class predictions.\n",
        "def plot_image(i, predictions_array, true_label, img):\n",
        "  true_label, img = true_label[i], img[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])\n",
        "\n",
        "  plt.imshow(img, cmap=plt.cm.binary)\n",
        "\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "  if predicted_label == true_label:\n",
        "    color = 'blue'\n",
        "  else:\n",
        "    color = 'red'\n",
        "\n",
        "  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n",
        "                                100*np.max(predictions_array),\n",
        "                                class_names[true_label]),\n",
        "                                color=color)\n",
        "\n",
        "def plot_value_array(i, predictions_array, true_label):\n",
        "  true_label = true_label[i]\n",
        "  plt.grid(False)\n",
        "  plt.xticks(range(10))\n",
        "  plt.yticks([])\n",
        "  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n",
        "  plt.ylim([0, 1])\n",
        "  predicted_label = np.argmax(predictions_array)\n",
        "\n",
        "  thisplot[predicted_label].set_color('red')\n",
        "  thisplot[true_label].set_color('blue')\n",
        "\n",
        "#Verify the predictions.\n",
        "i = 0\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(i, predictions[i], test_labels, test_images)\n",
        "plt.subplot(1,2,2)\n",
        "plot_value_array(i, predictions[i],  test_labels)\n",
        "plt.show()\n",
        "\n",
        "i = 12\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.subplot(1,2,1)\n",
        "plot_image(i, predictions[i], test_labels, test_images)\n",
        "plt.subplot(1,2,2)\n",
        "plot_value_array(i, predictions[i],  test_labels)\n",
        "plt.show()\n",
        "\n",
        "#Plot the first X test images, their predicted labels, and the true labels.\n",
        "#Color correct predictions in blue and incorrect predictions in red.\n",
        "num_rows = 5\n",
        "num_cols = 3\n",
        "num_images = num_rows*num_cols\n",
        "plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n",
        "for i in range(num_images):\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n",
        "  plot_image(i, predictions[i], test_labels, test_images)\n",
        "  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n",
        "  plot_value_array(i, predictions[i], test_labels)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#Use the trained model.\n",
        "#Grab an image from the test dataset.\n",
        "img = test_images[1]\n",
        "print(img.shape)\n",
        "#Add the image to a batch where it's the only member.\n",
        "img = (np.expand_dims(img,0))\n",
        "print(img.shape)\n",
        "#Now predict the correct label for this image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "la10NR9BPrDA"
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import geopandas as gpd\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "#Specify URL for Web Scraping.\n",
        "url = \"https://www.mygov.in/corona-data/covid19-statewise-status/\"\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "#Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "    stat = extract_contents(row.find_all('td')) \n",
        "#Notice that the data that we require is now a list of length 5.\n",
        "    if len(stat) == 5:\n",
        "        stats.append(stat)\n",
        "#Now convert the data into a pandas dataframe for further processing.\n",
        "new_cols = [\"Sr.No\", \"States/UT\",\"Confirmed\",\"Recovered\",\"Deceased\"]\n",
        "state_data = pd.DataFrame(data = stats, columns = new_cols)\n",
        "state_data.head()\n",
        "#Scraped data columns are of ‘string’ datatype.\n",
        "#Convert them into ‘int' datatype.\n",
        "state_data[\"Confirmed\"] = state_data[\"Confirmed\"].map(int)\n",
        "state_data[\"Recovered\"] = state_data[\"Recovered\"].map(int)\n",
        "state_data[\"Deceased\"] = state_data[\"Deceased\"].map(int)\n",
        "\n",
        "#Present the data using Pretty table.\n",
        "table = PrettyTable()\n",
        "table.field_names = (new_cols)\n",
        "for i in stats:\n",
        "    table.add_row(i)\n",
        "table.add_row(['','Total', \n",
        "               sum(state_data[\"Confirmed\"]), \n",
        "               sum(state_data[\"Recovered\"]),\n",
        "               sum(state_data[\"Deceased\"])])\n",
        "print(table)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1GERsCJXPOq"
      },
      "source": [
        "from mlbox.preprocessing import Reader\n",
        "from mlbox.preprocessing import Drift_thresholder\n",
        "from mlbox.optimisation import Optimiser\n",
        "from mlbox.prediction import Predictor\n",
        "\n",
        "#Paths to the train set and the test set.\n",
        "url = \"https://www.kaggle.com/shivamb/netflix-shows\"\n",
        "#Name of the feature to predict.\n",
        "#This columns should only be present in the train set.\n",
        "target_name = \"rating\"\n",
        "\n",
        "#Reading and cleaning all files\n",
        "#Declare a reader for csv files\n",
        "rd = Reader(sep=',')\n",
        "#Return a dictionary containing three entries\n",
        "# dict[\"train\"] contains training samples withtout target columns\n",
        "# dict[\"test\"] contains testing elements withtout target columns\n",
        "# dict[\"target\"] contains target columns for training samples.\n",
        "data = rd.train_test_split(\"https://www.kaggle.com/shivamb/netflix-shows\", target_name)\n",
        "\n",
        "dft = Drift_thresholder()\n",
        "data = dft.fit_transform(data)\n",
        "\n",
        "#Tuning\n",
        "# Declare an optimiser. Scoring possibilities for classification lie in :\n",
        "# {\"accuracy\", \"roc_auc\", \"f1\", \"neg_log_loss\", \"precision\", \"recall\"}\n",
        "opt = Optimiser(scoring='accuracy', n_folds=3)\n",
        "opt.evaluate(None, data)\n",
        "\n",
        "# Space of hyperparameters\n",
        "# The keys must respect the following syntax : \"enc__param\".\n",
        "#   \"enc\" = \"ne\" for na encoder\n",
        "#   \"enc\" = \"ce\" for categorical encoder\n",
        "#   \"enc\" = \"fs\" for feature selector [OPTIONAL]\n",
        "#   \"enc\" = \"stck\"+str(i) to add layer n°i of meta-features [OPTIONAL]\n",
        "#   \"enc\" = \"est\" for the final estimator\n",
        "#   \"param\" : a correct associated parameter for each step.\n",
        "#   Ex: \"max_depth\" for \"enc\"=\"est\", ...\n",
        "# The values must respect the syntax: {\"search\":strategy,\"space\":list}\n",
        "#   \"strategy\" = \"choice\" or \"uniform\". Default = \"choice\"\n",
        "#   list : a list of values to be tested if strategy=\"choice\".\n",
        "#   Else, list = [value_min, value_max].\n",
        "# Available strategies for ne_numerical_strategy are either an integer, a float\n",
        "#   or in {'mean', 'median', \"most_frequent\"}\n",
        "# Available strategies for ce_strategy are:\n",
        "#   {\"label_encoding\", \"dummification\", \"random_projection\", entity_embedding\"}\n",
        "space = {'ne__numerical_strategy': {\"search\": \"choice\", \"space\": [0]},\n",
        "         'ce__strategy': {\"search\": \"choice\",\n",
        "                          \"space\": [\"label_encoding\",\n",
        "                                    \"random_projection\",\n",
        "                                    \"entity_embedding\"]},\n",
        "         'fs__threshold': {\"search\": \"uniform\",\n",
        "                           \"space\": [0.01, 0.3]},\n",
        "         'est__max_depth': {\"search\": \"choice\",\n",
        "                            \"space\": [3, 4, 5, 6, 7]}\n",
        "\n",
        "         }\n",
        "\n",
        "# Optimises hyper-parameters of the whole Pipeline with a given scoring\n",
        "# function. Algorithm used to optimize : Tree Parzen Estimator.\n",
        "#\n",
        "# IMPORTANT : Try to avoid dependent parameters and to set one feature\n",
        "# selection strategy and one estimator strategy at a time.\n",
        "best = opt.optimise(space, data, 15)\n",
        "\n",
        "# Make prediction and save the results in save folder.\n",
        "prd = Predictor()\n",
        "prd.fit_predict(best, data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7rsYiwzMX5J"
      },
      "source": [
        "kaggle competitions download -c petfinder-adoption-predictionkaggle competitions download -c petfinder-adoption-url = \n",
        "u$ brew install jenv$!val scoreFn = new OpWorkflowRunnerLocal(workflow).scoreFunction(opParams)\n",
        "val scoreFn = new OpWorkflowRunnerLocal(workflow).scoreFunction(opParams)\n",
        "valInstall GraphViz & PyDot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNrnVvzHMVW0"
      },
      "source": [
        "#Install Graphviz and pydot.\n",
        "!apt-get -qq install -y graphviz && pip install pydot\n",
        "import pydot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZQD0xcaMwkx"
      },
      "source": [
        "#Install Cartopy.\n",
        "!pip install cartopy\n",
        "import cartopy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MExhf7pqfDEN"
      },
      "source": [
        "**TPOT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3p4bUw3ey9p"
      },
      "source": [
        "Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science\n",
        "\n",
        "[link text](https://)Randal S. Olson, Nathan Bartley, Ryan J. Urbanowicz, and Jason H. Moore (2016). Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science. Proceedings of GECCO 2016, pages 485-492.\n",
        "\n",
        "> Developed by Randal S. Olson and others at the University of Pennsylvania.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8P9iEpZkB9"
      },
      "source": [
        "**Install TPOT:**\n",
        "\n",
        "> \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IaLiNKmjJDa"
      },
      "source": [
        "!pip install tpot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLSTuPEEZsxE"
      },
      "source": [
        "**Classification using TPOT:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xv2HV0QzgQH"
      },
      "source": [
        "Wine dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bm9rV-104ZFa"
      },
      "source": [
        "#TPOT\n",
        "from tpot import TPOTClassifier\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "wine=load_wine()\n",
        "#Perform a train test split.\n",
        "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, train_size=0.75, test_size=0.25)\n",
        "#TPOT classifier\n",
        "tpot=TPOTClassifier(generations=99, population_size=99, mutation_rate=0.7, crossover_rate=0.3, random_state=11, cv=5, subsample=0.98, verbosity=2, n_jobs=-2, max_eval_time_mins=0.00000001, config_dict='TPOT light', memory='áuto', log_file='tpot_digits_logs')\n",
        "tpot.fit(X_train, y_train)\n",
        "print(tpot.score(X_test, y_test))\n",
        "tpot.export('tpot_wine_pipeline.py')\n",
        "\n",
        "#TPOT to only use a PyTorch-based logistic regression classifier as its main estimator is:\n",
        "tpot_config = {\n",
        "    'tpot.nn.PytorchLRClassifier': {\n",
        "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.]\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrAXNfKAzm2t"
      },
      "source": [
        "Digits dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkuN5Fcj8QZa"
      },
      "source": [
        "#TPOT\n",
        "from tpot import TPOTClassifier\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "wine = load_digits()\n",
        "#Perform a train test split.\n",
        "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, train_size=0.75, test_size=0.25)\n",
        "#TPOT classifier\n",
        "tpot=TPOTClassifier(generations=99, population_size=99, mutation_rate=0.7, crossover_rate=0.3, random_state=1110, cv=5, subsample=0.98, verbosity=2, n_jobs=-2, max_eval_time_mins=0.00000001, config_dict='TPOT light', memory='áuto', log_file='tpot_digits_logs')\n",
        "tpot.fit(X_train, y_train)\n",
        "print(tpot.score(X_test, y_test))\n",
        "tpot.export(\"tpot_digits_pipeline.py\")\n",
        "\n",
        "#TPOT to only use a PyTorch-based logistic regression classifier as its main estimator is:\n",
        "tpot_config = {\n",
        "    'tpot.nn.PytorchLRClassifier': {\n",
        "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.]\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNYMwpKlz7tn"
      },
      "source": [
        "Diabetes dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWnDD-cvQF0R"
      },
      "source": [
        "#TPOT\n",
        "from tpot import TPOTClassifier\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "diabetes = load_diabetes()\n",
        "#Perform a train test split.\n",
        "X_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, train_size=0.75, test_size=0.25)\n",
        "#TPOT classifier\n",
        "tpot=TPOTClassifier(generations=99, population_size=99, mutation_rate=0.7, crossover_rate=0.3, random_state=131, cv=5, subsample=0.999999999, verbosity=2, n_jobs=-2, max_eval_time_mins=0.00000001, config_dict='TPOT light', memory='áuto', log_file='tpot_diabetes_logs')\n",
        "tpot.fit(X_train, y_train)\n",
        "print(tpot.score(X_test, y_test))\n",
        "tpot.export('tpot_diabetes_pipeline.py')\n",
        "\n",
        "#TPOT to only use a PyTorch-based logistic regression classifier as its main estimator is:\n",
        "tpot_config = {\n",
        "    'tpot.nn.PytorchLRClassifier': {\n",
        "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.]\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUf7VPYB0jBM"
      },
      "source": [
        "Iris dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlBQKlDZ0MLk"
      },
      "source": [
        "#TPOT\n",
        "from tpot import TPOTClassifier\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "diabetes = load_iris()\n",
        "#Perform a train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(diabetes.data, diabetes.target, train_size=0.75, test_size=0.25)\n",
        "#TPOT classifier\n",
        "tpot=TPOTClassifier(generations=99, population_size=99, mutation_rate=0.7, crossover_rate=0.3, random_state=131, cv=5, subsample=0.98, verbosity=2, n_jobs=-2, max_eval_time_mins=0.000001, config_dict='TPOT light', memory='áuto', log_file='tpot_iris_logs')\n",
        "tpot.fit(X_train, y_train)\n",
        "print(tpot.score(X_test, y_test))\n",
        "tpot.export('tpot_iris_pipeline.py')\n",
        "\n",
        "#TPOT to only use a PyTorch-based logistic regression classifier as its main estimator is:\n",
        "tpot_config = {\n",
        "    'tpot.nn.PytorchLRClassifier': {\n",
        "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.]\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLeZZP-ufcHO"
      },
      "source": [
        "\n",
        "\n",
        "Latest India Covid-19 status dataset\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSxQxs7RegMX"
      },
      "source": [
        "#TPOT\n",
        "from tpot import TPOTClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Import\n",
        "import sklearn.model_selection\n",
        "import sklearn.metrics\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import geopandas as gpd\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "#Specify the URL for web-scraping.\n",
        "url = \"https://www.kaggle.com/sudalairajkumar/covid19-in-india\"\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "#Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "    stat = extract_contents(row.find_all('td')) \n",
        "#Notice that the data that we require is now a list of length 5.\n",
        "    if len(stat) == 5:\n",
        "        stats.append(stat)\n",
        "#Now convert the data into a pandas dataframe for further processing.\n",
        "new_cols = [\"Sr.No\", \"States/UT\",\"Confirmed\",\"Recovered\",\"Deceased\"]\n",
        "state_data = pd.DataFrame(data = stats, columns = new_cols)\n",
        "state_data.head()\n",
        "#Scraped data columns are of ‘string’ datatype.\n",
        "#Convert them into ‘int' datatype.\n",
        "state_data[\"Confirmed\"] = state_data[\"Confirmed\"].map(int)\n",
        "state_data[\"Recovered\"] = state_data[\"Recovered\"].map(int)\n",
        "state_data[\"Deceased\"] = state_data[\"Deceased\"].map(int)\n",
        "India_covid_status = state_data\n",
        "#Perform a train test split.\n",
        "X_train, X_test, y_train, y_test = train_test_split(India_covid_status.data, India_covid_status.target, train_size=0.75, test_size=0.25)\n",
        "#TPOT classifier\n",
        "tpot=TPOTClassifier(generations=99, population_size=99, mutation_rate=0.7, crossover_rate=0.3, random_state=131, cv=5, subsample=0.98, verbosity=2, n_jobs=-2, max_eval_time_mins=0.00000001, config_dict='TPOT light', memory='áuto', log_file='tpot_India_covid_status_logs')\n",
        "tpot.fit(X_train, y_train)\n",
        "print(tpot.score(X_test, y_test))\n",
        "tpot.export('tpot_India_covid_status_pipeline.py')\n",
        "\n",
        "#TPOT to only use a PyTorch-based logistic regression classifier as its main estimator is:\n",
        "tpot_config = {\n",
        "    'tpot.nn.PytorchLRClassifier': {\n",
        "        'learning_rate': [1e-3, 1e-2, 1e-1, 0.5, 1.]\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Set962s_xR9D"
      },
      "source": [
        "California housing dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU_4lc8jxDfa"
      },
      "source": [
        "train_data = /content/sample_data/california_housing_train.csv;\n",
        "test_data = /content/sample_data/california_housing_test.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XW55pybyY9F"
      },
      "source": [
        "import pandas as pd\n",
        "cal_house_train_data=pd.read_csv(\"/content/sample_data/california_housing_train.csv\")\n",
        "cal_house_test_data=pd.read_csv(\"/content/sample_data/california_housing_test.csv\")\n",
        "\n",
        "cal_house_data = pd.merge(cal_house_train_data, cal_house_test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVedZEfDxzM0"
      },
      "source": [
        "#Regression\n",
        "from tpot import TPOTRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Perform a train test split.\n",
        "X_train, X_test, y_train, y_test = train_test_split(cal_house_data.data, cal_house_data.target, train_size=0.75, test_size=0.25)\n",
        "tpot_reg=TPOTRegressor(generations=99, population_size=99, mutation_rate=0.75, crossover_rate=0.25, cv=5, subsample=0.95, verbosity=2, n_jobs=-2, scoring='r2', random_state=21, max_eval_time_mins=0.5, config_dict='TPOT light', memory='áuto', log_file='tpot_cal_house_data_logs')\n",
        "tpot_reg.fit(X_train, y_train)\n",
        "print(tpot_reg.score(X_test, y_test))\n",
        "tpot_reg.export('tpot_california_house_prices_pipeline.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgI9fgEGEGol"
      },
      "source": [
        "Latest India Covid-19 statewise status dataset:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHvHbSLbgJ8l"
      },
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import requests \n",
        "from bs4 import BeautifulSoup \n",
        "import geopandas as gpd\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "# Specify the URL for the offical ministry of health website.\n",
        "url = \"https://www.mygov.in/corona-data/covid19-statewise-status/\" \n",
        "\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace(\"\\n\", \"\") for x in row] \n",
        "\n",
        "stats = [] # Initialize stats.\n",
        "all_rows = soup.find_all(\"tr\") # Find all the table rows.\n",
        "\n",
        "for row in all_rows: \n",
        "    stat = extract_contents(row.find_all(\"td\")) # Find all data cells.\n",
        "    # Notice that the data that we require is now a list of length 5.\n",
        "    if len(stat) == 5: \n",
        "        stats.append(stat)\n",
        "\n",
        "#Now convert the data into a pandas dataframe for further processing.\n",
        "new_cols = [\"Sr.No\", \"States/UT\",\"Confirmed\",\"Recovered\",\"Deceased\"]\n",
        "state_data = pd.DataFrame(data = stats, columns = new_cols)\n",
        "\n",
        "#Converting the 'string' data to 'int'.\n",
        "state_data[\"Confirmed\"] = state_data[\"Confirmed\"].map(int)\n",
        "state_data[\"Recovered\"] = state_data[\"Recovered\"].map(int)\n",
        "state_data[\"Deceased\"]  = state_data[\"Deceased\"].map(int)\n",
        "\n",
        "# Pretty table representation\n",
        "table = PrettyTable()\n",
        "table.field_names = (new_cols)\n",
        "for i in stats:\n",
        "    table.add_row(i)\n",
        "table.add_row([\"\",\"Total\", \n",
        "               sum(state_data[\"Confirmed\"]), \n",
        "               sum(state_data[\"Recovered\"]), \n",
        "               sum(state_data[\"Deceased\"])])\n",
        "print(table)\n",
        "\n",
        "#Use barplot to show total confirmed cases Statewise. \n",
        "sns.set_style(\"ticks\")\n",
        "plt.figure(figsize = (15,10))\n",
        "plt.barh(state_data[\"States/UT\"], state_data[\"Confirmed\"].map(int),\n",
        "         align = \"center\", color = \"lightblue\", edgecolor = \"blue\")\n",
        "plt.xlabel(\"No. of Confirmed cases\", fontsize = 18)\n",
        "plt.ylabel(\"States/UT\", fontsize = 18)\n",
        "plt.gca().invert_yaxis() # This is to maintain the order in which the states appear.\n",
        "plt.xticks(fontsize = 14) \n",
        "plt.yticks(fontsize = 14)\n",
        "plt.title(\"Total Confirmed Cases Statewise\", fontsize = 20)\n",
        "\n",
        "for index, value in enumerate(state_data[\"Confirmed\"]):\n",
        "    plt.text(value, index, str(value), fontsize = 12, verticalalignment = \"center\")\n",
        "plt.show()  \n",
        "\n",
        "#Use donut chart representing nationwide total confirmed, cured and deceased cases.\n",
        "group_size = [sum(state_data[\"Confirmed\"]), \n",
        "              sum(state_data[\"Recovered\"]), \n",
        "              sum(state_data[\"Deceased\"])]\n",
        "\n",
        "group_labels = [\"Confirmed\\n\" + str(sum(state_data[\"Confirmed\"])), \n",
        "                \"Recovered\\n\" + str(sum(state_data[\"Recovered\"])), \n",
        "                \"Deceased\\n\"  + str(sum(state_data[\"Deceased\"]))]\n",
        "custom_colors = [\"skyblue\", \"yellowgreen\", \"tomato\"]\n",
        "\n",
        "plt.figure(figsize = (5,5))\n",
        "plt.pie(group_size, labels = group_labels, colors = custom_colors)\n",
        "central_circle = plt.Circle((0,0), 0.5, color = \"white\")\n",
        "fig = plt.gcf()\n",
        "fig.gca().add_artist(central_circle)\n",
        "plt.rc(\"font\", size = 12) \n",
        "plt.title(\"Nationwide total Confirmed, Recovered and Deceased Cases\", fontsize = 16)\n",
        "plt.show()\n",
        "\n",
        "# Read the state wise shapefile of India in a GeoDataFrame and preview it.\n",
        "map_data = gpd.read_file(\"Indian_States.shp\")\n",
        "map_data.rename(columns = {\"st_nm\":\"States/UT\"}, inplace = True)\n",
        "map_data.head()\n",
        "\n",
        "# Correct the name of states in the map dataframe. \n",
        "map_data[\"States/UT\"] = map_data[\"States/UT\"].str.replace('&', 'and')\n",
        "map_data[\"States/UT\"].replace(\"Arunanchal Pradesh\", \"Arunachal Pradesh\", inplace = True)\n",
        "map_data[\"States/UT\"].replace(\"Telangana\", \"Telengana\", inplace = True)\n",
        "map_data[\"States/UT\"].replace(\"NCT of Delhi\", \"Delhi\", inplace = True)\n",
        "\n",
        "# Merge both the dataframes - state_data and map_data.\n",
        "merged_data = pd.merge(map_data, state_data, how = \"left\", on = \"States/UT\")\n",
        "merged_data.fillna(0, inplace = True)\n",
        "merged_data.drop(\"Sr.No\", axis = 1, inplace = True)\n",
        "merged_data.head()\n",
        "\n",
        "# Create figure and axes for Matplotlib and set the title.\n",
        "fig, ax = plt.subplots(1, figsize=(20, 12))\n",
        "ax.axis('off')\n",
        "ax.set_title('Covid-19 Statewise Data - Confirmed Cases', fontdict = {'fontsize': '25', 'fontweight' : '3'})\n",
        "# Plot the figure.\n",
        "merged_data.plot(column = 'Confirmed', cmap='YlOrRd', linewidth=0.8, ax=ax, edgecolor='0.8', legend = True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAdt8pamLOBk"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from tpot.export_utils import set_param_recursive\n",
        "#TPOT\n",
        "from tpot import TPOTClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Perform the data and metric imports\n",
        "import sklearn.model_selection\n",
        "import sklearn.metrics\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import geopandas as gpd\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "#Specify the URL for Web Scraping.\n",
        "url = \"https://www.mygov.in/corona-data/covid19-statewise-status/\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "    stat = extract_contents(row.find_all('td')) \n",
        "# Notice that the data that we require is now a list of length 5.\n",
        "    if len(stat) == 5:\n",
        "        stats.append(stat)\n",
        "#Now convert the data into a pandas dataframe for further processing.\n",
        "new_cols = [\"Sr.No\", \"States/UT\",\"Confirmed\",\"Recovered\",\"Deceased\"]\n",
        "state_data = pd.DataFrame(data = stats, columns = new_cols)\n",
        "state_data.head()\n",
        "#Scraped data columns are of ‘string’ datatype.\n",
        "#Convert them into ‘int' datatype.\n",
        "state_data[\"Confirmed\"] = state_data[\"Confirmed\"].map(int)\n",
        "state_data[\"Recovered\"] = state_data[\"Recovered\"].map(int)\n",
        "state_data[\"Deceased\"] = state_data[\"Deceased\"].map(int)\n",
        "\n",
        "import csv\n",
        "data = state_data\n",
        "data = csv.reader(data)  \n",
        "print(data)\n",
        "# NOTE: Make sure that the outcome column is labeled 'target' in the data file.\n",
        "tpot_data = pd.read_csv(data, sep=',', dtype=np.float64)\n",
        "features = tpot_data.drop('target', axis=1)\n",
        "training_features, testing_features, training_target, testing_target = \\\n",
        "            train_test_split(features, tpot_data['target'], random_state=42)\n",
        "\n",
        "# Average CV score on the training set was: 0.9826086956521738\n",
        "exported_pipeline = make_pipeline(\n",
        "    Normalizer(norm=\"l2\"),\n",
        "    KNeighborsClassifier(n_neighbors=5, p=2, weights=\"distance\")\n",
        ")\n",
        "# Fix random state for all the steps in exported pipeline.\n",
        "set_param_recursive(exported_pipeline.steps, 'random_state', 42)\n",
        "\n",
        "exported_pipeline.fit(training_features, training_target)\n",
        "results = exported_pipeline.predict(testing_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9oS84ayLZES"
      },
      "source": [
        "**TPOT Regressor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fo8Zbc_oE20r"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from tpot import TPOTRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from tpot.export_utils import set_param_recursive\n",
        "#TPOT\n",
        "from tpot import TPOTRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "# data and metric imports\n",
        "import sklearn.model_selection\n",
        "import sklearn.metrics\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import geopandas as gpd\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "#Specify the URL for Web Scraping.\n",
        "url = \"https://www.mygov.in/corona-data/covid19-statewise-status/\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "    stat = extract_contents(row.find_all('td')) \n",
        "# Notice that the data that we require is now a list of length 5.\n",
        "    if len(stat) == 5:\n",
        "        stats.append(stat)\n",
        "#Now convert the data into a pandas dataframe for further processing.\n",
        "new_cols = [\"Sr.No\", \"States/UT\",\"Confirmed\",\"Recovered\",\"Deceased\"]\n",
        "state_data = pd.DataFrame(data = stats, columns = new_cols)\n",
        "state_data.head()\n",
        "#Scraped data columns are of ‘string’ datatype.\n",
        "#Convert them into ‘int' datatype.\n",
        "state_data[\"Confirmed\"] = state_data[\"Confirmed\"].map(int)\n",
        "state_data[\"Recovered\"] = state_data[\"Recovered\"].map(int)\n",
        "state_data[\"Deceased\"] = state_data[\"Deceased\"].map(int)\n",
        "#Perform a train test split.\n",
        "X_train, X_test, y_train, y_test = train_test_split(state_data.data, state_data.target, train_size=0.75, test_size=0.25)\n",
        "tpot_reg=TPOTRegressor(generations=99, population_size=99, mutation_rate=0.75, crossover_rate=0.25, cv=7, subsample=0.95, verbosity=2, n_jobs=-2, scoring='r2', random_state=21, max_eval_time_mins=0.5, config_dict='TPOT light', memory='áuto', log_file='tpot_state_data_logs')\n",
        "tpot_reg.fit(X_train, y_train)\n",
        "print(tpot_reg.score(X_test, y_test))\n",
        "tpot_reg.export(\"tpot_state_data_pipeline.py\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dro038ri-Cri"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from tpot import TPOTRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "house_data = load_boston()\n",
        "#Perform a train test split.\n",
        "X_train, X_test, y_train, y_test = train_test_split(house_data.data, house_data.target, train_size=0.75, test_size=0.25)\n",
        "tpot_reg=TPOTRegressor(generations=99, population_size=99, mutation_rate=0.75, crossover_rate=0.25, cv=5, subsample=0.95, verbosity=2, n_jobs=-2, scoring='r2', random_state=21, max_eval_time_mins=0.5, config_dict='TPOT light', memory='áuto', log_file='tpot_boston_data_logs')\n",
        "tpot_reg.fit(X_train, y_train)\n",
        "print(tpot_reg.score(X_test, y_test))\n",
        "tpot_reg.export('tpot_reg_house_prices_pipeline.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C6h5Ce86kLo"
      },
      "source": [
        "**Neural network classifier using TPOT-NN:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-BrkvyMO3TP"
      },
      "source": [
        "#Perform weather forecast using H2O.\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "url = \"https://www.climate.gov/maps-data/datasets\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  # Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    weather_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = weather_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1313)\n",
        "    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                    ntrees=100,\n",
        "                                    max_depth=4,\n",
        "                                    learn_rate=0.1)\n",
        "    #Specify the predictor set and response.\n",
        "    x = list(weather_data.columns)\n",
        "    x\n",
        "    #Train the model.\n",
        "    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(model)\n",
        "    #TPOT-NN\n",
        "    from tpot import TPOTClassifier\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)\n",
        "\n",
        "    clf = TPOTClassifier(config_dict='TPOT NN', template='Selector-Transformer-PytorchLRClassifier',\n",
        "                        verbosity=2, population_size=10, generations=10)\n",
        "    clf.fit(X_train, y_train)\n",
        "    print(clf.score(X_test, y_test))\n",
        "    clf.export('tpot_nn_weather_data_pipeline.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp6ve4m3BP0r"
      },
      "source": [
        "#RSNA dataset\n",
        "#Perform weather forecast using H2O.\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "#Specify the URL.\n",
        "url = \"https://www.kaggle.com/c/rsna-miccai-brain-tumor-radiogenomic-classification/data\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  # Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    rsna_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = rsna_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=01230)\n",
        "    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                    ntrees=100,\n",
        "                                    max_depth=4,\n",
        "                                    learn_rate=0.1)\n",
        "    \n",
        "    #Specify the predictor set and response.\n",
        "    x = list(weather_data.columns)\n",
        "    x\n",
        "\n",
        "    #Train the model.\n",
        "    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(model)\n",
        "\n",
        "    #TPOT-NN\n",
        "    from tpot import TPOTClassifier\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3)\n",
        "\n",
        "    clf = TPOTClassifier(config_dict='TPOT NN', template='Selector-Transformer-PytorchLRClassifier',\n",
        "                        verbosity=2, population_size=100, generations=100)\n",
        "    clf.fit(X_train, y_train)\n",
        "    print(clf.score(X_test, y_test))\n",
        "    clf.export('tpot_nn_rsna_data_pipeline.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fnft2FLoQjg2"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "url = \"https://www.kaggle.com/c/chaii-hindi-and-tamil-question-answering/data\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  # Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    chaii_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = chaii_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=13131)\n",
        "    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                    ntrees=1000,\n",
        "                                    max_depth=4,\n",
        "                                    learn_rate=0.1)\n",
        "    #Specify the predictor set and response.\n",
        "    x = list(chaii_data.columns)\n",
        "    x\n",
        "    #Train the model.\n",
        "    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(model)\n",
        "    #TPOT-NN\n",
        "    from tpot import TPOTClassifier\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)\n",
        "\n",
        "    clf = TPOTClassifier(config_dict='TPOT NN', template='Selector-Transformer-PytorchLRClassifier',\n",
        "                        verbosity=2, population_size=10, generations=10)\n",
        "    clf.fit(X_train, y_train)\n",
        "    print(clf.score(X_test, y_test))\n",
        "    clf.export('tpot_nn_weather_data_pipeline.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYpyPTRtmn3r"
      },
      "source": [
        "Music dataset using TPOT-NN:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP8WrP1bmnO_"
      },
      "source": [
        "#Musicnet dataset\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "url = \"https://www.kaggle.com/imsparsh/musicnet-dataset?select=musicnet_metadata.csv\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  # Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    music_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = music_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1313)\n",
        "    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                    ntrees=100,\n",
        "                                    max_depth=4,\n",
        "                                    learn_rate=0.1)\n",
        "    #Specify the predictor set and response.\n",
        "    x = list(music_data.columns)\n",
        "    x\n",
        "    #Train the model.\n",
        "    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(model)\n",
        "    #TPOT-NN\n",
        "    from tpot import TPOTClassifier\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)\n",
        "\n",
        "    clf = TPOTClassifier(config_dict='TPOT NN', template='Selector-Transformer-PytorchLRClassifier',\n",
        "                        verbosity=2, population_size=10, generations=10)\n",
        "    clf.fit(X_train, y_train)\n",
        "    print(clf.score(X_test, y_test))\n",
        "    clf.export('tpot_nn_music_data_pipeline.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMw7cC5kDXTx"
      },
      "source": [
        "Autoweka datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CcaiEGUfKzR"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "url = \"http://www.cs.ubc.ca/labs/beta/Projects/autoweka/datasets/\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  # Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    autoweka_datasets[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = autoweka_datasets\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1313)\n",
        "    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                    ntrees=100,\n",
        "                                    max_depth=4,\n",
        "                                    learn_rate=0.1)\n",
        "    \n",
        "    #Specify the predictor set and response.\n",
        "    x = list(autoweka_datasets.columns)\n",
        "    x\n",
        "\n",
        "    #Train the model.\n",
        "    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(model)\n",
        "\n",
        "    #TPOT-NN\n",
        "    from tpot import TPOTClassifier\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)\n",
        "\n",
        "    clf = TPOTClassifier(config_dict='TPOT NN', template='Selector-Transformer-PytorchLRClassifier',\n",
        "                        verbosity=2, population_size=10, generations=10)\n",
        "    clf.fit(X_train, y_train)\n",
        "    print(clf.score(X_test, y_test))\n",
        "    clf.export('tpot_nn_autoweka_datasets_pipeline.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVBwnJaMF4X7"
      },
      "source": [
        "FIFA 22 - Ultimate Team Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCCEReAyNO6a"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "url = \"https://www.kaggle.com/baranb/fut-22-ultimate-team-dataset\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  # Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    kaggle_data[each_new_col] = kaggle_data[each_new_col].map(int)\n",
        "    X, y = kaggle_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1313)\n",
        "    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                    ntrees=100,\n",
        "                                    max_depth=4,\n",
        "                                    learn_rate=0.01)\n",
        "    #Specify the predictor set and response.\n",
        "    x = list(kaggle_data.columns)\n",
        "    x\n",
        "    #Train the model.\n",
        "    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(model)\n",
        "    perf = model.model_performance(test)\n",
        "    print(perf.__class__)\n",
        "    #Area Under the ROC Curve (AUC)\n",
        "    perf.auc()\n",
        "    perf.mse()\n",
        "    #Cross-validated Performance\n",
        "    cvmodel = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                       ntrees=1000,\n",
        "                                       max_depth=4,\n",
        "                                       learn_rate=0.001,\n",
        "                                       nfolds=5)\n",
        "    cvmodel.train(x=x, y=y, training_frame=data)\n",
        "    print(cvmodel.auc(train=True))\n",
        "    print(cvmodel.auc(xval=True))\n",
        "    #Grid Search\n",
        "    #ntrees: Number of trees\n",
        "    #max_depth: Maximum depth of a tree\n",
        "    #learn_rate: Learning rate in the GBM\n",
        "    ntrees_opt = [5,50,100]\n",
        "    max_depth_opt = [2,3,5]\n",
        "    learn_rate_opt = [0.1,0.2]\n",
        "    hyper_params = {'ntrees': ntrees_opt, \n",
        "                    'max_depth': max_depth_opt,\n",
        "                    'learn_rate': learn_rate_opt}\n",
        "\n",
        "    #Define an \"H2OGridSearch\" object by specifying the algorithm (GBM) and the hyper parameters.\n",
        "    from h2o.grid.grid_search import H2OGridSearch\n",
        "    gs = H2OGridSearch(H2OGradientBoostingEstimator, hyper_params = hyper_params)\n",
        "    gs.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(gs)\n",
        "\n",
        "    # Print out the AUC for all of the models.\n",
        "    auc_table = gs.sort_by('auc(valid=True)',increasing=False)\n",
        "    print(auc_table)\n",
        "    #Get the best model in terms of AUC.\n",
        "    best_model = h2o.get_model(auc_table['Model Id'][0])\n",
        "    best_model.auc() \n",
        "    #Generate predictions on the test set using the \"best\" model, and evaluate the test set AUC.\n",
        "    best_perf = best_model.model_performance(test)\n",
        "    best_perf.auc()  \n",
        "\n",
        "    #TPOT-NN\n",
        "    from tpot import TPOTClassifier\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3)\n",
        "\n",
        "    clf = TPOTClassifier(config_dict='TPOT NN', template='Selector-Transformer-PytorchLRClassifier',\n",
        "                        verbosity=2, population_size=100, generations=100)\n",
        "    clf.fit(X_train, y_train)\n",
        "    print(clf.score(X_test, y_test))\n",
        "    clf.export('tpot_nn_data_pipeline.py')\n",
        "    fig, ax = plt.subplots(1, figsize=(20, 12))\n",
        "    ax.axis(‘off’)\n",
        "    ax.set_title(‘FIFA 22 Data’, \n",
        "                fontdict =  {‘fontsize’: ‘25’, ‘fontweight’ : ‘3’})\n",
        "    merged_data.plot(column = ‘Confirmed’, cmap=’YlOrRd’, \n",
        "                    linewidth=0.8, ax=ax, edgecolor=’0.8', \n",
        "                    legend = True)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44C36cw4H2fM"
      },
      "source": [
        "Turkey Covid 19 Vaccination Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htufg0LUGg19"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "url = \"https://www.kaggle.com/omercolakoglu/turkey-covid-19-vaccination-data\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  # Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    kaggle_data[each_new_col] = kaggle_data[each_new_col].map(int)\n",
        "    X, y = kaggle_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1313)\n",
        "    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                    ntrees=100,\n",
        "                                    max_depth=3,\n",
        "                                    learn_rate=0.001)\n",
        "    #Specify the predictor set and response.\n",
        "    x = list(kaggle_data.columns)\n",
        "    x\n",
        "    #Train the model.\n",
        "    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(model)\n",
        "    perf = model.model_performance(test)\n",
        "    print(perf.__class__)\n",
        "    #Area Under the ROC Curve (AUC)\n",
        "    perf.auc()\n",
        "    perf.mse()\n",
        "    #Cross-validated Performance\n",
        "    cvmodel = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                       ntrees=1000,\n",
        "                                       max_depth=5,\n",
        "                                       learn_rate=0.001,\n",
        "                                       nfolds=5)\n",
        "    cvmodel.train(x=x, y=y, training_frame=data)\n",
        "    print(cvmodel.auc(train=True))\n",
        "    print(cvmodel.auc(xval=True))\n",
        "    #Grid Search\n",
        "    #ntrees: Number of trees\n",
        "    #max_depth: Maximum depth of a tree\n",
        "    #learn_rate: Learning rate in the GBM\n",
        "    ntrees_opt = [5,50,100]\n",
        "    max_depth_opt = [2,3,5]\n",
        "    learn_rate_opt = [0.1,0.2]\n",
        "    hyper_params = {'ntrees': ntrees_opt, \n",
        "                    'max_depth': max_depth_opt,\n",
        "                    'learn_rate': learn_rate_opt}\n",
        "\n",
        "    #Define an \"H2OGridSearch\" object by specifying the algorithm (GBM) and the hyper parameters.\n",
        "    from h2o.grid.grid_search import H2OGridSearch\n",
        "    gs = H2OGridSearch(H2OGradientBoostingEstimator, hyper_params = hyper_params)\n",
        "    gs.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(gs)\n",
        "\n",
        "    # Print out the AUC for all of the models.\n",
        "    auc_table = gs.sort_by('auc(valid=True)',increasing=False)\n",
        "    print(auc_table)\n",
        "    #Get the best model in terms of AUC.\n",
        "    best_model = h2o.get_model(auc_table['Model Id'][0])\n",
        "    best_model.auc() \n",
        "    #Generate predictions on the test set using the \"best\" model, and evaluate the test set AUC.\n",
        "    best_perf = best_model.model_performance(test)\n",
        "    best_perf.auc()  \n",
        "\n",
        "    #TPOT-NN\n",
        "    from tpot import TPOTClassifier\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3)\n",
        "\n",
        "    clf = TPOTClassifier(config_dict='TPOT NN', template='Selector-Transformer-PytorchLRClassifier',\n",
        "                        verbosity=2, population_size=100, generations=100)\n",
        "    clf.fit(X_train, y_train)\n",
        "    print(clf.score(X_test, y_test))\n",
        "    clf.export('tpot_nn_data_pipeline.py')\n",
        "    fig, ax = plt.subplots(1, figsize=(20, 12))\n",
        "    ax.axis(‘off’)\n",
        "    ax.set_title(‘Turkey Covid 19 Vaccination Data’, \n",
        "                fontdict =  {‘fontsize’: ‘25’, ‘fontweight’ : ‘3’})\n",
        "    merged_data.plot(column = ‘Confirmed’, cmap=’YlOrRd’, \n",
        "                    linewidth=0.8, ax=ax, edgecolor=’0.8', \n",
        "                    legend = True)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYwjNEhD4dTC"
      },
      "source": [
        "#Kaggle dataset\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "url = \"https://www.kaggle.com/kiva/data-science-for-good-kiva-crowdfunding\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  # Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    kaggle_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = kaggle_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=131311)\n",
        "    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                    ntrees=100,\n",
        "                                    max_depth=4,\n",
        "                                    learn_rate=0.001)\n",
        "    \n",
        "    #Specify the predictor set and response.\n",
        "    x = list(kaggle_data.columns)\n",
        "    x\n",
        "\n",
        "    #Train the model.\n",
        "    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(model)\n",
        "    \n",
        "    #TPOT-NN\n",
        "    from tpot import TPOTClassifier\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)\n",
        "\n",
        "    clf = TPOTClassifier(config_dict='TPOT NN', template='Selector-Transformer-PytorchLRClassifier',\n",
        "                        verbosity=2, population_size=100, generations=100)\n",
        "    clf.fit(X_train, y_train)\n",
        "    print(clf.score(X_test, y_test))\n",
        "    clf.export('tpot_nn_kaggle_dataset_pipeline.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke-mcGrZ6q6F"
      },
      "source": [
        "from tpot import TPOTClassifier\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = make_blobs(n_samples=100, centers=2, n_features=3, random_state=424)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)\n",
        "\n",
        "clf = TPOTClassifier(config_dict='TPOT NN', template='Selector-Transformer-PytorchLRClassifier',\n",
        "                     verbosity=2, population_size=10, generations=10)\n",
        "clf.fit(X_train, y_train)\n",
        "print(clf.score(X_test, y_test))\n",
        "clf.export('tpot_nn_data_pipeline.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnlw3IMO5H2-"
      },
      "source": [
        "#Import scikit-learn.\n",
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_YoMW88Lrz0"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Define a dataset dictionary.\n",
        "dataset_dict={\"ames\": \"https://www.kaggle.com/search?q=ames+in%3Adatasets\", \n",
        "              \"A Large Scale Fish Dataset\": \"https://www.kaggle.com/crowww/a-large-scale-fish-dataset\",\n",
        "              \"House Prices-Advanced Regression Techniques\": \"https://www.kaggle.com/c/house-prices-advanced-regression-techniques\", \n",
        "              \"NFL Big Data Bowl\": \"https://www.kaggle.com/c/nfl-big-data-bowl-2022/data\", \n",
        "              \"Tabular Playground Series\": \"colab.research.google.com/#create=true\", \n",
        "              \"New York City Airport Activity\": \"https://www.kaggle.com/sveneschlbeck/new-york-city-airport-activity?select=nyc-flights.csv\"\n",
        "              \"1M Sudoku games\":\"https://www.kaggle.com/bryanpark/sudoku\",\n",
        "              \"Huggingface BERT\":\"https://www.kaggle.com/xhlulu/huggingface-bert\"\n",
        "              }\n",
        "\n",
        "trained_model = automl(**dataset_dict)\n",
        "print(trained_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY2Oll9CVZdM"
      },
      "source": [
        "**NOTE:** Turns out TPOT cannot solve multi label regression problems at this time as below;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-L7uiRHGZPU"
      },
      "source": [
        "#Latest India Covid-19 statewise status\n",
        "from tpot import TPOTClassifier\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from tpot.export_utils import set_param_recursive\n",
        "#TPOT\n",
        "from tpot import TPOTClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Perform the data and metric imports.\n",
        "import sklearn.model_selection\n",
        "import sklearn.metrics\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import geopandas as gpd\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "#Specify the URL for the Web Scraping.\n",
        "url = \"https://www.mygov.in/corona-data/covid19-statewise-status/\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "#Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "    stat = extract_contents(row.find_all('td')) \n",
        "# Notice that the data that we require is now a list of length 5.\n",
        "    if len(stat) == 5:\n",
        "        stats.append(stat)\n",
        "\n",
        "#now convert the data into a pandas dataframe for further processing\n",
        "new_cols = [\"Sr.No\", \"States/UT\",\"Confirmed\",\"Recovered\",\"Deceased\"]\n",
        "state_data = pd.DataFrame(data = stats, columns = new_cols)\n",
        "state_data.head()\n",
        "\n",
        "#Scraped data columns are of ‘string’ datatype.\n",
        "#Convert them into ‘int' datatype.\n",
        "state_data[\"Confirmed\"] = state_data[\"Confirmed\"].map(int)\n",
        "state_data[\"Recovered\"] = state_data[\"Recovered\"].map(int)\n",
        "state_data[\"Deceased\"] = state_data[\"Deceased\"].map(int)\n",
        "\n",
        "def getNumbers():\n",
        "    return 'one', 'two'\n",
        "one, two = getNumbers()\n",
        "\n",
        "X, y = getNumbers()\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)\n",
        "#TPOT-NN\n",
        "clf = TPOTClassifier(config_dict='TPOT NN', template='Selector-Transformer-PytorchLRClassifier',\n",
        "                     verbosity=2, population_size=10, generations=10)\n",
        "\n",
        "#NOTE: Turns out TPOT cannot solve multi label regression problems at this time\n",
        "'''clf.fit(X_train, y_train)\n",
        "print(clf.score(X_test, y_test))\n",
        "clf.export('tpot_nn_state_data_pipeline.py')\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an_j8xecqKV9"
      },
      "source": [
        "**Auto-Sklearn**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3ICgx56qGuW"
      },
      "source": [
        "arXiv:2007.04074v2 [cs.LG]\n",
        "\n",
        "arXiv:2007.04074 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH0EPLazrNJ1"
      },
      "source": [
        "@article{ASKL2,\n",
        "   title = {Auto-Sklearn 2.0},\n",
        "   author = {Feurer, Matthias and Eggensperger, Katharina and\n",
        "             Falkner, Stefan and Lindauer, Marius and Hutter, Frank},\n",
        "   booktitle = {Advances in Neural Information Processing Systems 28},\n",
        "   year = {2020},\n",
        "   journal = {arXiv:2007.04074 [cs.LG]},\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEs0CID5raUs"
      },
      "source": [
        "Make the necessary installations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ublFe3RO8Czz"
      },
      "source": [
        "!python3 -m pip install --upgrade pip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soMMfoYqqEyt"
      },
      "source": [
        "!pip3 install --upgrade pandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNDti9ukq86I"
      },
      "source": [
        "!pip3 install auto-sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UqEQ4iH6Uv-"
      },
      "source": [
        "!pip3 install --upgrade scipy\n",
        "!pip3 install --upgrade auto-sklearn\n",
        "!pip install auto-sklearn==0.10.0\n",
        "!pip install --upgrade pip\n",
        "!sudo apt-get install build-essential swig \n",
        "!curl https://raw.githubusercontent.com/automl/auto-sklearn/master/requirements.txt | xargs -n 1 -L 1 pip install \n",
        "!pip install auto-sklearn==0.10.0\n",
        "!pip install matplotlib-venn\n",
        "!apt-get -qq install -y libfluidsynth1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XQy4lG_EFru"
      },
      "source": [
        "Install 7zip reader libarchive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R9pjAB_EFEy"
      },
      "source": [
        "!apt-get -qq install -y libarchive-dev && pip install -U libarchive\n",
        "import libarchive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZ1v1osNrwXM"
      },
      "source": [
        "import autosklearn.classification\n",
        "cls = autosklearn.classification.AutoSklearnClassifier()\n",
        "import sklearn.model_selection\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "X, y = sklearn.datasets.load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "        sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
        "\n",
        "cls.fit(X_train, y_train)\n",
        "predictions = cls.predict(X_test)\n",
        "\n",
        "import sklearn.model_selection\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "if __name__ == \"__main__\":\n",
        "    X, y = sklearn.datasets.load_iris(return_X_y=True)\n",
        "    X_train, X_test, y_train, y_test = \\\n",
        "            sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
        "    automl = autosklearn.classification.AutoSklearnClassifier()\n",
        "    #Fit.\n",
        "    automl.fit(X_train, y_train)\n",
        "    #Predict.\n",
        "    y_hat = automl.predict(X_test)\n",
        "    #Print the accuracy score.\n",
        "    print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_hat))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5YgOYMyJVSc"
      },
      "source": [
        "import autosklearn.classification\n",
        "cls = autosklearn.classification.AutoSklearnClassifier()\n",
        "import sklearn.model_selection\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "X, y = sklearn.datasets.load_digits(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "        sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
        "\n",
        "cls.fit(X_train, y_train)\n",
        "predictions = cls.predict(X_test)\n",
        "\n",
        "import sklearn.model_selection\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "if __name__ == \"__main__\":\n",
        "    X, y = sklearn.datasets.load_digits(return_X_y=True)\n",
        "    X_train, X_test, y_train, y_test = \\\n",
        "            sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
        "    automl = autosklearn.classification.AutoSklearnClassifier()\n",
        "    #Fit.\n",
        "    automl.fit(X_train, y_train)\n",
        "    #Predict.\n",
        "    y_hat = automl.predict(X_test)\n",
        "    #Print the accuracy score.\n",
        "    print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_hat))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjD7mEOpOui3"
      },
      "source": [
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(\"https://www.kaggle.com/kiva/data-science-for-good-kiva-crowdfunding\").content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "    stat = extract_contents(row.find_all('td')) \n",
        "\n",
        "    # Notice that the data that we require is now a list of length 5.\n",
        "    if len(stat) == 5:\n",
        "        stats.append(stat)\n",
        "    #Now convert the data into a pandas dataframe for further processing.\n",
        "    new_cols=[]\n",
        "    ozone_data = pd.DataFrame(data = stats, columns = new_cols)\n",
        "    ozone_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype.\n",
        "    #Convert them into ‘int' datatype.\n",
        "    ozone_data[new_cols] = ozone_data[new_cols].map(int)\n",
        "    X, y = ozone_data\n",
        "    X_train, X_test, y_train, y_test = \\\n",
        "        sklearn.model_selection.train_test_split(X, y, random_state=110011)\n",
        "\n",
        "    #Auto-PyTorch\n",
        "    autoPyTorch = AutoNetClassification(\"tiny_cs\",  # config preset\n",
        "                                    log_level='info',\n",
        "                                    max_runtime=999999999**10000000,\n",
        "                                    min_budget=30,\n",
        "                                    max_budget=999999999*100000)\n",
        "    \n",
        "    autoPyTorch.fit(X_train, y_train, validation_split=0.3)\n",
        "    y_pred = autoPyTorch.predict(X_test)\n",
        "    print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred))\n",
        "    #Fit.\n",
        "    clf = KNeighborsClassifier(n_neighbors=1).fit(X_train, y_train)\n",
        "    #Predict.\n",
        "    y_pred = clf.predict(X_test)\n",
        "    #Print the confusion matrix and classification report.\n",
        "    print(metrics.confusion_matrix(y_test, y_pred))\n",
        "    print(metrics.classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Za7WXoJC3GGv"
      },
      "source": [
        "**ConfigSpace**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTdAFe3oqrxX"
      },
      "source": [
        "@article{\n",
        "    title   = {BOAH: A Tool Suite for Multi-Fidelity Bayesian Optimization & Analysis of Hyperparameters},\n",
        "    author  = {M. Lindauer and K. Eggensperger and M. Feurer and A. Biedenkapp and J. Marben and P. Müller and F. Hutter},\n",
        "    journal = {arXiv:1908.06756 {[cs.LG]}},\n",
        "    date    = {2019},\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIjx3N083ZKY"
      },
      "source": [
        "**Install ConfigSpace:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwwP_p9Z3XFw"
      },
      "source": [
        "!pip install ConfigSpace"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMiBntXc32IQ"
      },
      "source": [
        "import ConfigSpace as CS\n",
        "cs = CS.ConfigurationSpace(seed=1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23dfBRHc4LOT"
      },
      "source": [
        "#Choose a hyperparameter alpha.\n",
        "import ConfigSpace.hyperparameters as CSH\n",
        "alpha = CSH.UniformFloatHyperparameter(name='alpha', lower=0, upper=1)\n",
        "#Create a ConfigurationSpace object.\n",
        "import ConfigSpace as CS\n",
        "import ConfigSpace.hyperparameters as CSH\n",
        "cs = CS.ConfigurationSpace(seed=1234)\n",
        "\n",
        "a = CSH.UniformIntegerHyperparameter('a', lower=10, upper=100, log=False)\n",
        "b = CSH.CategoricalHyperparameter('b', choices=['red', 'green', 'blue'])\n",
        "cs.add_hyperparameters([a, b])\n",
        "cs.sample_configuration()\n",
        "#Add ordinal hyper-parameter.\n",
        "ord_hp = CSH.OrdinalHyperparameter('ordinal_hp', sequence=['10', '20', '30'])\n",
        "cs.add_hyperparameter(ord_hp)\n",
        "#Sample a configuration from the ConfigurationSpace object.\n",
        "cs.sample_configuration()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5sVPgiNJuXdX"
      },
      "source": [
        "**Install Auto-PyTorch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgf_wv36-BIT"
      },
      "source": [
        "!pip install autopytorch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo-Zy8_ncntw"
      },
      "source": [
        "from autoPyTorch import AutoNetClassification\n",
        "#Perform the data and metric imports.\n",
        "import sklearn.model_selection\n",
        "import sklearn.metrics\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import geopandas as gpd\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "#Specify the URL for Web Scraping.\n",
        "url = \"https://www.kaggle.com/sudalairajkumar/covid19-in-india\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "    stat = extract_contents(row.find_all('td'))\n",
        "\n",
        "# Notice that the data that we require is now a list of length 5.\n",
        "    if len(stat) == 5:\n",
        "        stats.append(stat)\n",
        "#Now convert the data into a pandas dataframe for further processing.\n",
        "new_cols = [\"Sr.No\", \"States/UT\",\"Confirmed\",\"Recovered\",\"Deceased\"]\n",
        "state_data = pd.DataFrame(data = stats, columns = new_cols)\n",
        "state_data.head()\n",
        "\n",
        "#Scraped data columns are of ‘string’ datatype.\n",
        "#Convert them into ‘int' datatype.\n",
        "state_data[\"Confirmed\"] = state_data[\"Confirmed\"].map(int)\n",
        "state_data[\"Recovered\"] = state_data[\"Recovered\"].map(int)\n",
        "state_data[\"Deceased\"] = state_data[\"Deceased\"].map(int)\n",
        "\n",
        "X, y = state_data\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "        sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
        "\n",
        "#Auto-PyTorch\n",
        "autoPyTorch = AutoNetClassification(\"tiny_cs\",  # config preset\n",
        "                                    log_level='info',\n",
        "                                    max_runtime=999999999**10000000,\n",
        "                                    min_budget=30,\n",
        "                                    max_budget=999999999*100000)\n",
        "#Fit.\n",
        "autoPyTorch.fit(X_train, y_train, validation_split=0.3)\n",
        "#Predict.\n",
        "y_pred = autoPyTorch.predict(X_test)\n",
        "#Print the accuracy score.\n",
        "print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7_BlrG-e-Qi"
      },
      "source": [
        "from autoPyTorch import AutoNetClassification\n",
        "\n",
        "# Perform the data and metric imports\n",
        "import sklearn.model_selection\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "X, y = sklearn.datasets.load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "        sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
        "\n",
        "#Auto-PyTorch\n",
        "autoPyTorch = AutoNetClassification(\"tiny_cs\",  # config preset\n",
        "                                    log_level='info',\n",
        "                                    max_runtime=999999999**10000000,\n",
        "                                    min_budget=30,\n",
        "                                    max_budget=999999999*100000)\n",
        "#Fit.\n",
        "autoPyTorch.fit(X_train, y_train, validation_split=0.3)\n",
        "#Predict.\n",
        "y_pred = autoPyTorch.predict(X_test)\n",
        "#Print the accuracy score.\n",
        "print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LazAKGwKtblC"
      },
      "source": [
        "from autoPyTorch import AutoNetClassification\n",
        "\n",
        "# Perform the data and metric imports\n",
        "import sklearn.model_selection\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "X, y = sklearn.datasets.load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "        sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
        "\n",
        "#Auto-PyTorch\n",
        "autoPyTorch = AutoNetClassification(\"tiny_cs\",  # Config preset\n",
        "                                    log_level='info',\n",
        "                                    max_runtime=999999999**10000000,\n",
        "                                    min_budget=30,\n",
        "                                    max_budget=999999999*100000)\n",
        "#Fit.\n",
        "autoPyTorch.fit(X_train, y_train, validation_split=0.3)\n",
        "#Predict.\n",
        "y_pred = autoPyTorch.predict(X_test)\n",
        "#Print the accuracy score.\n",
        "print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLECuGvLta1a"
      },
      "source": [
        "from autoPyTorch import AutoNetClassification\n",
        "# Perform the data and metric imports\n",
        "import sklearn.model_selection\n",
        "import sklearn.datasets\n",
        "import sklearn.metrics\n",
        "X, y = sklearn.datasets.load_linnerud(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "        sklearn.model_selection.train_test_split(X, y, random_state=1)\n",
        "\n",
        "#Auto-PyTorch\n",
        "autoPyTorch = AutoNetClassification(\"tiny_cs\",  # config preset\n",
        "                                    log_level='info',\n",
        "                                    max_runtime=999999999**10000000,\n",
        "                                    min_budget=30,\n",
        "                                    max_budget=999999999*100000)\n",
        "#Fit.\n",
        "autoPyTorch.fit(X_train, y_train, validation_split=0.3)\n",
        "#Predict.\n",
        "y_pred = autoPyTorch.predict(X_test)\n",
        "#Print the accuracy score.\n",
        "print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ot4uPRLillj4"
      },
      "source": [
        "# Perform the data and metric imports.\n",
        "import sklearn.model_selection\n",
        "import sklearn.metrics\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import geopandas as gpd\n",
        "from prettytable import PrettyTable\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#Specify the URL.\n",
        "url = \"https://www.kaggle.com/brsdincer/ozone-tendency-new-data-20182021-nasa\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "\n",
        "for row in all_rows:\n",
        "    stat = extract_contents(row.find_all('td')) \n",
        "    # Notice that the data that we require is now a list of length 5.\n",
        "    if len(stat) == 5:\n",
        "        stats.append(stat)\n",
        "    #Now convert the data into a pandas dataframe for further processing.\n",
        "    new_cols=[]\n",
        "    ozone_data = pd.DataFrame(data = stats, columns = new_cols)\n",
        "    ozone_data.head()\n",
        "\n",
        "    #Scraped data columns are of ‘string’ datatype.\n",
        "    #Convert them into ‘int' datatype.\n",
        "    ozone_data[new_cols] = ozone_data[new_cols].map(int)\n",
        "    X, y = ozone_data\n",
        "    X_train, X_test, y_train, y_test = \\\n",
        "        sklearn.model_selection.train_test_split(X, y, random_state=110011)\n",
        "\n",
        "    #Auto-PyTorch\n",
        "    autoPyTorch = AutoNetClassification(\"tiny_cs\",  # config preset\n",
        "                                    log_level='info',\n",
        "                                    max_runtime=999999999**10000000,\n",
        "                                    min_budget=30,\n",
        "                                    max_budget=999999999*100000)\n",
        "    \n",
        "    #Fit. Predict. Get the accuracy score.\n",
        "    autoPyTorch.fit(X_train, y_train, validation_split=0.3)\n",
        "    y_pred = autoPyTorch.predict(X_test)\n",
        "    print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnDjGDpv8MvI"
      },
      "source": [
        "# Perform the data and metric imports.\n",
        "import sklearn.model_selection\n",
        "import sklearn.metrics\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import geopandas as gpd\n",
        "from prettytable import PrettyTable\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#Specify the URL.\n",
        "url = \"https://www.kaggle.com/kiva/data-science-for-good-kiva-crowdfunding\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "\n",
        "for row in all_rows:\n",
        "    stat = extract_contents(row.find_all('td')) \n",
        "    # Notice that the data that we require is now a list of length 5.\n",
        "    if len(stat) == 5:\n",
        "        stats.append(stat)\n",
        "\n",
        "    #Now convert the data into a pandas dataframe for further processing.\n",
        "    new_cols=[]\n",
        "    coll_data = pd.DataFrame(data = stats, columns = new_cols)\n",
        "    coll_data.head()\n",
        "\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    coll_data[new_cols] = coll_data[new_cols].map(int)\n",
        "    X, y = coll_data\n",
        "    X_train, X_test, y_train, y_test = \\\n",
        "        sklearn.model_selection.train_test_split(X, y, random_state=11001100)\n",
        "\n",
        "    #Auto-PyTorch\n",
        "    autoPyTorch = AutoNetClassification(\"tiny_cs\",  # config preset\n",
        "                                    log_level='info',\n",
        "                                    max_runtime=999999999**10000000,\n",
        "                                    min_budget=333,\n",
        "                                    max_budget=999999999*100000)\n",
        "    \n",
        "    #Fit. Predict. Get the accuracy score.\n",
        "    autoPyTorch.fit(X_train, y_train, validation_split=0.3)\n",
        "    y_pred = autoPyTorch.predict(X_test)\n",
        "    print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Km3lWBn9uDuL"
      },
      "source": [
        "# Perform the data and metric imports.\n",
        "import sklearn.model_selection\n",
        "import sklearn.metrics\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import geopandas as gpd\n",
        "from prettytable import PrettyTable\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#Specify the URL.\n",
        "url = \"https://www.kaggle.com/nipunarora8/age-gender-and-ethnicity-face-data-csv\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "    stat = extract_contents(row.find_all('td')) \n",
        "    # Notice that the data that we require is now a list of length 5.\n",
        "    if len(stat) == 5:\n",
        "        stats.append(stat)\n",
        "    #Now convert the data into a pandas dataframe for further processing.\n",
        "    new_cols=[]\n",
        "    face_data = pd.DataFrame(data = stats, columns = new_cols)\n",
        "    face_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype.\n",
        "    #Convert them into ‘int' datatype.\n",
        "    face_data[new_cols] = face_data[new_cols].map(int)\n",
        "    X, y = face_data\n",
        "    X_train, X_test, y_train, y_test = \\\n",
        "        sklearn.model_selection.train_test_split(X, y, random_state=11001100)\n",
        "\n",
        "    #Auto-PyTorch\n",
        "    autoPyTorch = AutoNetClassification(\"tiny_cs\",  # config preset\n",
        "                                    log_level='info',\n",
        "                                    max_runtime=999999999**10000000,\n",
        "                                    min_budget=333,\n",
        "                                    max_budget=999999999*100000)\n",
        "    \n",
        "    #Fit. Predict. Get the accuracy score. \n",
        "    autoPyTorch.fit(X_train, y_train, validation_split=0.3)\n",
        "    y_pred = autoPyTorch.predict(X_test)\n",
        "    print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwatvKUNHQQH"
      },
      "source": [
        "# Perform the data and metric imports.\n",
        "import sklearn.model_selection\n",
        "import sklearn.metrics\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import geopandas as gpd\n",
        "from prettytable import PrettyTable\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#Specify the URL.\n",
        "url = \"https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "    stat = extract_contents(row.find_all('td')) \n",
        "    # Notice that the data that we require is now a list of length 5.\n",
        "    if len(stat) == 5:\n",
        "        stats.append(stat)\n",
        "    #Now convert the data into a pandas dataframe for further processing.\n",
        "    new_cols=[]\n",
        "    ames_data = pd.DataFrame(data = stats, columns = new_cols)\n",
        "    ames_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype.\n",
        "    #Convert them into ‘int' datatype.\n",
        "    ames_data[new_cols] = ames_data[new_cols].map(int)\n",
        "    X, y = ames_data\n",
        "    X_train, X_test, y_train, y_test = \\\n",
        "        sklearn.model_selection.train_test_split(X, y, random_state=110011)\n",
        "\n",
        "    #Auto-PyTorch\n",
        "    autoPyTorch = AutoNetClassification(\"tiny_cs\",  # config preset\n",
        "                                    log_level='info',\n",
        "                                    max_runtime=999999999**10000000,\n",
        "                                    min_budget=333,\n",
        "                                    max_budget=999999999*100000)\n",
        "    \n",
        "    #Fit. Predict. Get the accuracy score. \n",
        "    autoPyTorch.fit(X_train, y_train, validation_split=0.3)\n",
        "    y_pred = autoPyTorch.predict(X_test)\n",
        "    print(\"Accuracy score\", sklearn.metrics.accuracy_score(y_test, y_pred))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZ5krsSAxAMT"
      },
      "source": [
        "Install AutoGluon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wysi7YekxN5_"
      },
      "source": [
        "#Install AutoGluon.\n",
        "python3 -m pip install -U pip\n",
        "python3 -m pip install -U setuptools wheel\n",
        "python3 -m pip install -U \"mxnet<2.0.0\"\n",
        "python3 -m pip install autogluon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woP4-jC9MWrh"
      },
      "source": [
        "#Image Prediction with AutoGluon\n",
        "#Import AutoGluon.\n",
        "%matplotlib inline\n",
        "import autogluon.core as ag\n",
        "from autogluon.vision import ImageDataset\n",
        "import pandas as pd\n",
        "#Celeb faces (celebA) dataset\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "#Specify the URL.\n",
        "url = \"https://www.kaggle.com/c/petfinder-adoption-prediction/data\"\n",
        "\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  # Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  \n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    olympics2021_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = stats_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1221)\n",
        "    csv_file_list=[\"BreedLabels.csv\", \"ColorLabels.csv\", \"PetFinder-BreedLabels.csv\", \n",
        "                   \"PetFinder-ColorLabels.csv\", \"PetFinder-StateLabels.csv\", \"PetFinder-StateLabels.csv (285 B)\", \n",
        "                   \"StateLabels.csv\", \"breed_labels.csv\", \"color_labels.csv\", \"state_labels.csv\"]\n",
        "    \n",
        "    for each_csv_file in csv_file_list:\n",
        "      csv_file = ag.utils.download(each_csv_file)\n",
        "      df = pd.read_csv(csv_file)\n",
        "      df.head()\n",
        "      df = ImageDataset.from_csv(csv_file)\n",
        "      df.head()\n",
        "      train_data, _, test_data = ImageDataset.from_folders(each_csv_file, train='train', test='test')\n",
        "      print('train #', len(train_data), 'test #', len(test_data))\n",
        "      train_data.head()\n",
        "\n",
        "      #Load the splits with from_folder.\n",
        "      root = os.path.join(os.path.dirname(train_data.iloc[0]['image']), '..')\n",
        "      all_data = ImageDataset.from_folder(root)\n",
        "      all_data.head()\n",
        "\n",
        "      #Split the dataset.\n",
        "      train, val, test = all_data.random_split(val_size=0.1, test_size=0.1)\n",
        "      print('train #:', len(train), 'test #:', len(test))\n",
        "\n",
        "      #Convert a list of images to dataset.\n",
        "      pets = ag.utils.download(each_csv_file)\n",
        "      pets = ag.utils.unzip(pets)\n",
        "      image_list = [x for x in os.listdir(os.path.join(pets, 'images')) if x.endswith('jpg')]\n",
        "      new_data = ImageDataset.from_name_func(image_list, label_fn, root=os.path.join(os.getcwd(), pets, 'images'))\n",
        "      new_data\n",
        "\n",
        "    #Visualize the images.\n",
        "    new_data.show_images()\n",
        "\n",
        "    #Image prediction\n",
        "    import autogluon.core as ag\n",
        "    from autogluon.vision import ImagePredictor, ImageDataset\n",
        "    train_dataset, _, test_dataset = ImageDataset.from_folders(\"img_align_pets.zip\")\n",
        "    print(train_dataset)\n",
        "\n",
        "    #Fit a classifier.\n",
        "    predictor = ImagePredictor()\n",
        "\n",
        "    # Since the original dataset does not provide validation split, the `fit` function splits it randomly with 90/10 ratio.\n",
        "    predictor.fit(train_dataset, hyperparameters={'epochs': 1000})\n",
        "\n",
        "    #The best Top-1 accuracy achieved on the validation set is:\n",
        "    fit_result = predictor.fit_summary()\n",
        "    print('Top-1 train acc: %.3f, val acc: %.3f' %(fit_result['train_acc'], fit_result['valid_acc']))\n",
        "    \n",
        "    #Predict on a new image.\n",
        "    image_path = test_dataset.iloc[0]['image']\n",
        "    result = predictor.predict(image_path)\n",
        "    print(result)\n",
        "    bulk_result = predictor.predict(test_dataset)\n",
        "    print(bulk_result)\n",
        "\n",
        "    #Generate image features with a classifier.\n",
        "    image_path = test_dataset.iloc[0]['image']\n",
        "    feature = predictor.predict_feature(image_path)\n",
        "    print(feature)\n",
        "\n",
        "    #Validation and test top-1 accuracy is:\n",
        "    test_acc = predictor.evaluate(test_dataset)\n",
        "    print('Top-1 test acc: %.3f' % test_acc['top1'])\n",
        "\n",
        "    #Save and load the classifiers.\n",
        "    filename = 'predictor.ag'\n",
        "    predictor.save(filename)\n",
        "    predictor_loaded = ImagePredictor.load(filename)\n",
        "\n",
        "    # Use predictor_loaded as usual.\n",
        "    result = predictor_loaded.predict(image_path)\n",
        "    print(result)\n",
        "\n",
        "    #Use AutoGluon to produce an ImagePredictor to classify images.\n",
        "    import autogluon.core as ag\n",
        "    from autogluon.vision import ImagePredictor, ImageDataset\n",
        "    train_data, _, test_data = ImageDataset.from_folders(\"img_align_celeba.zip\")\n",
        "    model = ag.Categorical('resnet18_v1b', 'mobilenetv3_small')\n",
        "    model_list = ImagePredictor.list_models()\n",
        "\n",
        "    #Specify the training hyper-parameters.\n",
        "    batch_size = 8\n",
        "    lr = ag.Categorical(1e-2, 1e-3)\n",
        "\n",
        "    #Bayesian Optimization\n",
        "    hyperparameters={'model': model, 'batch_size': batch_size, 'lr': lr, 'epochs': 2}\n",
        "    predictor = ImagePredictor()\n",
        "    predictor.fit(train_data, time_limit=60*10, hyperparameters=hyperparameters,\n",
        "                  hyperparameter_tune_kwargs={'searcher': 'bayesopt', 'num_trials': 2})\n",
        "    print('Top-1 val acc: %.3f' % predictor.fit_summary()['valid_acc'])\n",
        "\n",
        "    #Load the test dataset and evaluate.\n",
        "    results = predictor.evaluate(test_data)\n",
        "    print('Test acc on hold-out data:', results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mwAsCQMSg6k"
      },
      "source": [
        "#Global Superstore Orders 2016 dataset\n",
        "#Tabular prediction with AutoGluon:\n",
        "\n",
        "#Predict columns in a table.\n",
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "train_data = TabularDataset('Global Superstore Orders 2016.csv')\n",
        "subsample_size = 999000000000  # Subsample subset of data for faster demo, try setting this to much larger values.\n",
        "train_data = train_data.sample(n=subsample_size, random_state=0)\n",
        "train_data.head()\n",
        "label = 'class'\n",
        "print(\"Summary of class variable: \\n\", train_data[label].describe())\n",
        "\n",
        "#Use AutoGluon to train multiple models.\n",
        "save_path = 'agModels-predictClass'  # Specifies folder to store trained models.\n",
        "predictor = TabularPredictor(label=label, path=save_path).fit(train_data)\n",
        "test_data = TabularDataset('Global Superstore Orders 2016.csv')\n",
        "y_test = test_data[label]  # Values to predict.\n",
        "test_data_nolab = test_data.drop(columns=[label])  # Delete label column to prove we're not cheating.\n",
        "test_data_nolab.head()\n",
        "\n",
        "#predictor = TabularPredictor.load(save_path)\n",
        "y_pred = predictor.predict(test_data_nolab)\n",
        "print(\"Predictions:  \\n\", y_pred)\n",
        "perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)\n",
        "predictor.leaderboard(test_data, silent=True)\n",
        "\n",
        "from autogluon.tabular import TabularPredictor\n",
        "predictor = TabularPredictor(label=label).fit(train_data='Global Superstore Orders 2016.csv')\n",
        "\n",
        "#.fit() returns a predictor object.\n",
        "pred_probs = predictor.predict_proba(test_data_nolab)\n",
        "pred_probs.head(5)\n",
        "\n",
        "#Summarize what happened during fit.\n",
        "results = predictor.fit_summary(show_plot=True)\n",
        "print(\"AutoGluon infers problem type is: \", predictor.problem_type)\n",
        "print(\"AutoGluon identified the following types of features:\")\n",
        "print(predictor.feature_metadata)\n",
        "predictor.leaderboard(test_data, silent=True)\n",
        "predictor.predict(test_data, model='LightGBM')\n",
        "\n",
        "#Maximize the predictive performance.\n",
        "time_limit = 11  \n",
        "metric = 'roc_auc'  # Specify the evaluation metric here.\n",
        "\n",
        "predictor = TabularPredictor(label, eval_metric=metric).fit(train_data, time_limit=time_limit, presets='best_quality')\n",
        "predictor.leaderboard(test_data, silent=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXtk2gPQxbrM"
      },
      "source": [
        "#Tabular prediction with AutoGluon:\n",
        "\n",
        "#Predict columns in a table.\n",
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "train_data = TabularDataset('pueblosMagicos.csv')\n",
        "subsample_size = 999000000000  # Subsample subset of data for faster demo, try setting this to much larger values.\n",
        "train_data = train_data.sample(n=subsample_size, random_state=0)\n",
        "train_data.head()\n",
        "label = 'class'\n",
        "print(\"Summary of class variable: \\n\", train_data[label].describe())\n",
        "\n",
        "#Use AutoGluon to train multiple models.\n",
        "save_path = 'agModels-predictClass'  # specifies folder to store trained models\n",
        "predictor = TabularPredictor(label=label, path=save_path).fit(train_data)\n",
        "test_data = TabularDataset('pueblosMagicos.csv')\n",
        "y_test = test_data[label]  # values to predict\n",
        "test_data_nolab = test_data.drop(columns=[label])  # delete label column to prove we're not cheating\n",
        "test_data_nolab.head()\n",
        "\n",
        "#predictor = TabularPredictor.load(save_path)\n",
        "y_pred = predictor.predict(test_data_nolab)\n",
        "print(\"Predictions:  \\n\", y_pred)\n",
        "perf = predictor.evaluate_predictions(y_true=y_test, y_pred=y_pred, auxiliary_metrics=True)\n",
        "predictor.leaderboard(test_data, silent=True)\n",
        "\n",
        "from autogluon.tabular import TabularPredictor\n",
        "predictor = TabularPredictor(label=label).fit(train_data='pueblosMagicos.csv')\n",
        "\n",
        "#.fit() returns a predictor object.\n",
        "pred_probs = predictor.predict_proba(test_data_nolab)\n",
        "pred_probs.head(5)\n",
        "\n",
        "#Summarize what happened during fit.\n",
        "results = predictor.fit_summary(show_plot=True)\n",
        "print(\"AutoGluon infers problem type is: \", predictor.problem_type)\n",
        "print(\"AutoGluon identified the following types of features:\")\n",
        "print(predictor.feature_metadata)\n",
        "predictor.leaderboard(test_data, silent=True)\n",
        "predictor.predict(test_data, model='LightGBM')\n",
        "\n",
        "#Maximizing predictive performance.\n",
        "time_limit = 11  \n",
        "metric = 'roc_auc'  # Specify the evaluation metric here.\n",
        "predictor = TabularPredictor(label, eval_metric=metric).fit(train_data, time_limit=time_limit, presets='best_quality')\n",
        "predictor.leaderboard(test_data, silent=True)\n",
        "\n",
        "#Regression (predict numeric table columns)\n",
        "pueblo_column = 'PUEBLO'\n",
        "print(\"Summary of PUEBLO variable: \\n\", train_data[pueblo_column].describe())\n",
        "\n",
        "predictor_pueblo = TabularPredictor(label=pueblo_column, path=\"agModels-predictAge\").fit(train_data, time_limit=60)\n",
        "performance = predictor_pueblo.evaluate(test_data)\n",
        "\n",
        "#See the per-model performance.\n",
        "predictor_pueblo.leaderboard(test_data, silent=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL-x-IK0_8EW"
      },
      "source": [
        "#All NeurIPS (NIPS) Papers dataset\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "# Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "#Specify the URL.\n",
        "url = \"https://www.kaggle.com/rowhitswami/nips-papers-1987-2019-updated\"\n",
        "# Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "# Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "# Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  # Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    weather_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = weather_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=1313)\n",
        "    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                    ntrees=100,\n",
        "                                    max_depth=4,\n",
        "                                    learn_rate=0.1)\n",
        "    \n",
        "    #Specify the predictor set and response.\n",
        "    x = list(weather_data.columns)\n",
        "    x\n",
        "    #Train the model.\n",
        "    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(model)\n",
        "    \n",
        "    #TPOT-NN\n",
        "    from tpot import TPOTClassifier\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.75, test_size=0.25)\n",
        "\n",
        "    clf = TPOTClassifier(config_dict='TPOT NN', template='Selector-Transformer-PytorchLRClassifier',\n",
        "                        verbosity=2, population_size=10, generations=10)\n",
        "    clf.fit(X_train, y_train)\n",
        "    print(clf.score(X_test, y_test))\n",
        "    clf.export('tpot_nn_weather_data_pipeline.py')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FW9xd-v-Uv1n"
      },
      "source": [
        "AutoKeras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahTgRczzUo2V"
      },
      "source": [
        "#Install AutoKeras\n",
        "!pip install autokeras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5XVgUkuU0DX"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import autokeras as ak\n",
        "input_node = ak.ImageInput()\n",
        "output_node = ak.Normalization()(input_node)\n",
        "output_node1 = ak.ConvBlock()(output_node)\n",
        "output_node2 = ak.ResNetBlock(version=\"v2\")(output_node)\n",
        "output_node = ak.Merge()([output_node1, output_node2])\n",
        "output_node = ak.ClassificationHead()(output_node)\n",
        "\n",
        "auto_model = ak.AutoModel(\n",
        "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=100\n",
        ")\n",
        "\n",
        "#Prepare data to run the model.\n",
        "x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print(y_train[:3])\n",
        "\n",
        "# Feed the AutoModel with training data.\n",
        "auto_model.fit(x_train[:100], y_train[:100], epochs=1000)\n",
        "# Predict with the best model.\n",
        "predicted_y = auto_model.predict(x_test)\n",
        "# Evaluate the best model with testing data.\n",
        "print(auto_model.evaluate(x_test, y_test))\n",
        "\n",
        "#Implement new block.\n",
        "class SingleDenseLayerBlock(ak.Block):\n",
        "    def build(self, hp, inputs=None):\n",
        "        # Get the input_node from inputs.\n",
        "        input_node = tf.nest.flatten(inputs)[0]\n",
        "        layer = tf.keras.layers.Dense(\n",
        "            hp.Int(\"num_units\", min_value=32, max_value=512, step=32)\n",
        "        )\n",
        "        output_node = layer(input_node)\n",
        "        return output_node\n",
        "\n",
        "# Build the AutoModel.\n",
        "input_node = ak.Input()\n",
        "output_node = SingleDenseLayerBlock()(input_node)\n",
        "output_node = ak.RegressionHead()(output_node)\n",
        "auto_model = ak.AutoModel(input_node, output_node, overwrite=True, max_trials=100)\n",
        "\n",
        "# Prepare the data.\n",
        "num_instances = 100\n",
        "x_train = np.random.rand(num_instances, 20).astype(np.float32)\n",
        "y_train = np.random.rand(num_instances, 1).astype(np.float32)\n",
        "x_test = np.random.rand(num_instances, 20).astype(np.float32)\n",
        "y_test = np.random.rand(num_instances, 1).astype(np.float32)\n",
        "\n",
        "# Train the model.\n",
        "auto_model.fit(x_train, y_train, epochs=1000)\n",
        "print(auto_model.evaluate(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaWpQVvBnVgf"
      },
      "source": [
        "#Google VPP dataset\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "from autokeras import StructuredDataClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import tensorflow as tf\n",
        "import autokeras as ak\n",
        "\n",
        "input_node = ak.ImageInput()\n",
        "output_node = ak.Normalization()(input_node)\n",
        "output_node1 = ak.ConvBlock()(output_node)\n",
        "output_node2 = ak.ResNetBlock(version=\"v2\")(output_node)\n",
        "output_node = ak.Merge()([output_node1, output_node2])\n",
        "output_node = ak.ClassificationHead()(output_node)\n",
        "\n",
        "#Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "#Specify the URL.\n",
        "url = \"https://www.kaggle.com/kalaikumarr/google-vpp-comparing-with-28-models/data\"\n",
        "\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  #Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    kaggle_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = stats_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=11)\n",
        "    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                    ntrees=1000,\n",
        "                                    max_depth=4,\n",
        "                                    learn_rate=0.1)\n",
        "    #Specify the predictor set and response.\n",
        "    x = list(stats_train.columns)\n",
        "    x\n",
        "    #Train the model.\n",
        "    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(model)\n",
        "\n",
        "    auto_model = ak.AutoModel(\n",
        "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=100)\n",
        "    \n",
        "    #Prepare data to run the model.\n",
        "    (x_train, y_train), (x_test, y_test) = kaggle_data\n",
        "    print(x_train.shape)\n",
        "    print(y_train.shape)\n",
        "    print(y_train[:3])\n",
        "\n",
        "    #Feed the AutoModel with training data.\n",
        "    auto_model.fit(x_train[:100], y_train[:100], epochs=100000)\n",
        "    # Predict with the best model.\n",
        "    predicted_y = auto_model.predict(x_test)\n",
        "    # Evaluate the best model with testing data.\n",
        "    print(auto_model.evaluate(x_test, y_test))\n",
        "\n",
        "    #Implement new block.\n",
        "    class SingleDenseLayerBlock(ak.Block):\n",
        "        def build(self, hp, inputs=None):\n",
        "            # Get the input_node from inputs.\n",
        "            input_node = tf.nest.flatten(inputs)[0]\n",
        "            layer = tf.keras.layers.Dense(\n",
        "                hp.Int(\"num_units\", min_value=32, max_value=512, step=32))\n",
        "            output_node = layer(input_node)\n",
        "            return output_node\n",
        "\n",
        "    # Build the AutoModel.\n",
        "    input_node = ak.Input()\n",
        "    output_node = SingleDenseLayerBlock()(input_node)\n",
        "    output_node = ak.RegressionHead()(output_node)\n",
        "    auto_model = ak.AutoModel(input_node, output_node, overwrite=True, max_trials=100)\n",
        "   \n",
        "    # Prepare the data.\n",
        "    num_instances = 100\n",
        "    x_train = np.random.rand(num_instances, 20).astype(np.float32)\n",
        "    y_train = np.random.rand(num_instances, 1).astype(np.float32)\n",
        "    x_test = np.random.rand(num_instances, 20).astype(np.float32)\n",
        "    y_test = np.random.rand(num_instances, 1).astype(np.float32)\n",
        "    \n",
        "    # Train the model.\n",
        "    auto_model.fit(x_train, y_train, epochs=100000)\n",
        "    print(auto_model.evaluate(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfa83T9PcPK2"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "from autokeras import StructuredDataClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import tensorflow as tf\n",
        "import autokeras as ak\n",
        "\n",
        "input_node = ak.ImageInput()\n",
        "output_node = ak.Normalization()(input_node)\n",
        "output_node1 = ak.ConvBlock()(output_node)\n",
        "output_node2 = ak.ResNetBlock(version=\"v2\")(output_node)\n",
        "output_node = ak.Merge()([output_node1, output_node2])\n",
        "output_node = ak.ClassificationHead()(output_node)\n",
        "\n",
        "#Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "#Specify the URL.\n",
        "url = \"https://www.kaggle.com/kannan1314/apple-stock-price-all-time?select=Apple.csv\"\n",
        "\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "# Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "  stat = extract_contents(row.find_all('td')) \n",
        "  #Notice that the data that we require is now a list of length 5.\n",
        "  if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "  #Now convert the data into a pandas dataframe for further processing.\n",
        "  new_cols = []\n",
        "  for each_new_col in row:\n",
        "    stats_data = pd.DataFrame(data = stats, columns = each_new_col)\n",
        "    stats_data.head()\n",
        "    #Scraped data columns are of ‘string’ datatype so convert them into ‘int' datatype.\n",
        "    kaggle_data[each_new_col] = stats_data[each_new_col].map(int)\n",
        "    X, y = stats_data\n",
        "    X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=11)\n",
        "    model = H2OGradientBoostingEstimator(distribution='bernoulli',\n",
        "                                    ntrees=100,\n",
        "                                    max_depth=4,\n",
        "                                    learn_rate=0.1)\n",
        "    #Specify the predictor set and response.\n",
        "    x = list(stats_train.columns)\n",
        "    x\n",
        "    #Train the model.\n",
        "    model.train(x=x, y=y, training_frame=train, validation_frame=valid)\n",
        "    print(model)\n",
        "\n",
        "    auto_model = ak.AutoModel(\n",
        "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=100)\n",
        "    \n",
        "    #Prepare data to run the model.\n",
        "    (x_train, y_train), (x_test, y_test) = kaggle_data\n",
        "    print(x_train.shape)\n",
        "    print(y_train.shape)\n",
        "    print(y_train[:3])\n",
        "\n",
        "    #Feed the AutoModel with training data.\n",
        "    auto_model.fit(x_train[:100], y_train[:100], epochs=1000)\n",
        "    # Predict with the best model.\n",
        "    predicted_y = auto_model.predict(x_test)\n",
        "    # Evaluate the best model with testing data.\n",
        "    print(auto_model.evaluate(x_test, y_test))\n",
        "\n",
        "    #Implement new block.\n",
        "    class SingleDenseLayerBlock(ak.Block):\n",
        "        def build(self, hp, inputs=None):\n",
        "            # Get the input_node from inputs.\n",
        "            input_node = tf.nest.flatten(inputs)[0]\n",
        "            layer = tf.keras.layers.Dense(\n",
        "                hp.Int(\"num_units\", min_value=32, max_value=512, step=32)\n",
        "            )\n",
        "            output_node = layer(input_node)\n",
        "            return output_node\n",
        "\n",
        "    # Build the AutoModel.\n",
        "    input_node = ak.Input()\n",
        "    output_node = SingleDenseLayerBlock()(input_node)\n",
        "    output_node = ak.RegressionHead()(output_node)\n",
        "    auto_model = ak.AutoModel(input_node, output_node, overwrite=True, max_trials=100)\n",
        "   \n",
        "    # Prepare the data.\n",
        "    num_instances = 100\n",
        "    x_train = np.random.rand(num_instances, 20).astype(np.float32)\n",
        "    y_train = np.random.rand(num_instances, 1).astype(np.float32)\n",
        "    x_test = np.random.rand(num_instances, 20).astype(np.float32)\n",
        "    y_test = np.random.rand(num_instances, 1).astype(np.float32)\n",
        "    \n",
        "    # Train the model.\n",
        "    auto_model.fit(x_train, y_train, epochs=1000)\n",
        "    print(auto_model.evaluate(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAc3QuV6CN9z"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "\n",
        "import autokeras as ak\n",
        "import tensorflow_cloud as tfc\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(description=\"Model save path arguments.\")\n",
        "parser.add_argument(\"--path\", required=True, type=str, help=\"Keras model save path\")\n",
        "args = parser.parse_args()\n",
        "\n",
        "tfc.run(\n",
        "    chief_config=tfc.COMMON_MACHINE_CONFIGS[\"V100_1X\"],\n",
        "    docker_base_image=\"haifengjin/autokeras:1.0.3\",\n",
        ")\n",
        "\n",
        "# Prepare the dataset.\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "print(x_train.shape)  \n",
        "print(y_train.shape) \n",
        "print(y_train[:3])\n",
        "\n",
        "# Initialize the ImageClassifier.\n",
        "clf = ak.ImageClassifier(max_trials=2)\n",
        "# Search for the best model.\n",
        "clf.fit(x_train, y_train, epochs=10)\n",
        "# Evaluate on the testing data.\n",
        "print(\"Accuracy: {accuracy}\".format(accuracy=clf.evaluate(x_test, y_test)[1]))\n",
        "\n",
        "clf.export_model().save(os.path.join(args.path, \"model.h5\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HG7WbDsKbdgK"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "from autokeras import StructuredDataClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import tensorflow as tf\n",
        "import autokeras as ak\n",
        "\n",
        "input_node = ak.ImageInput()\n",
        "output_node = ak.Normalization()(input_node)\n",
        "output_node1 = ak.ConvBlock()(output_node)\n",
        "output_node2 = ak.ResNetBlock(version=\"v2\")(output_node)\n",
        "output_node = ak.Merge()([output_node1, output_node2])\n",
        "output_node = ak.ClassificationHead()(output_node)\n",
        "\n",
        "#Import H2O GBM.\n",
        "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
        "#Specify the URL.\n",
        "url = \"https://www.kaggle.com/c/petfinder-adoption-prediction/data\"\n",
        "#Make a GET request to fetch the raw HTML content.\n",
        "web_content = requests.get(url).content\n",
        "#Parse the html content.\n",
        "soup = BeautifulSoup(web_content, \"html.parser\")\n",
        "#Remove any newlines and extra spaces from left and right.\n",
        "extract_contents = lambda row: [x.text.replace('\\n', '') for x in row]\n",
        "#Find all table rows and data cells within.\n",
        "stats = [] \n",
        "all_rows = soup.find_all('tr')\n",
        "for row in all_rows:\n",
        "stat = extract_contents(row.find_all('td')) \n",
        "#Notice that the data that we require is now a list of length 5.\n",
        "if len(stat) == 5:\n",
        "    stats.append(stat)\n",
        "\n",
        "#Now convert the data into a pandas dataframe for further processing.\n",
        "new_cols = []\n",
        "for each_new_col in row:\n",
        "    petfinder_data = pd.DataFrame(data = petfinder_data, columns = each_new_col)\n",
        "    petfinder_data.head()\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = petfinder_data\n",
        "x_train = x_train[:100]\n",
        "y_train = y_train[:100]\n",
        "print(x_train.shape)  \n",
        "print(y_train.shape)  \n",
        "print(y_train[:3]) \n",
        "\n",
        "# Initialize the image regressor.\n",
        "reg = ak.ImageRegressor(overwrite=True, max_trials=1)\n",
        "\n",
        "# Feed the image regressor with training data.\n",
        "reg.fit(x_train, y_train, epochs=2)\n",
        "\n",
        "# Predict with the best model.\n",
        "predicted_y = reg.predict(x_test)\n",
        "print(predicted_y)\n",
        "\n",
        "# Evaluate the best model with testing data.\n",
        "print(reg.evaluate(x_test, y_test))\n",
        "\n",
        "#Validation data\n",
        "reg.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    # Split the training data and use the last 15% as validation data.\n",
        "    validation_split=0.15,\n",
        "    epochs=2000,\n",
        ")\n",
        "\n",
        "#Customized search space\n",
        "input_node = ak.ImageInput()\n",
        "output_node = ak.ImageBlock(\n",
        "    # Only search ResNet architectures.\n",
        "    block_type=\"resnet\",\n",
        "    # Normalize the dataset.\n",
        "    normalize=False,\n",
        "    # Do not do data augmentation.\n",
        "    augment=False,\n",
        ")(input_node)\n",
        "output_node = ak.RegressionHead()(output_node)\n",
        "reg = ak.AutoModel(\n",
        "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=100\n",
        ")\n",
        "\n",
        "reg.fit(x_train, y_train, epochs=2000)\n",
        "\n",
        "input_node = ak.ImageInput()\n",
        "output_node = ak.Normalization()(input_node)\n",
        "output_node = ak.ImageAugmentation(horizontal_flip=False)(output_node)\n",
        "output_node = ak.ResNetBlock(version=\"v2\")(output_node)\n",
        "output_node = ak.RegressionHead()(output_node)\n",
        "\n",
        "reg = ak.AutoModel(\n",
        "    inputs=input_node, outputs=output_node, overwrite=True, max_trials=100\n",
        ")\n",
        "\n",
        "reg.fit(x_train, y_train, epochs=2000)\n",
        "\n",
        "#Data format\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Reshape the images to have the channel dimension.\n",
        "x_train = x_train.reshape(x_train.shape + (1,))\n",
        "x_test = x_test.reshape(x_test.shape + (1,))\n",
        "y_train = y_train.reshape(y_train.shape + (1,))\n",
        "y_test = y_test.reshape(y_test.shape + (1,))\n",
        "\n",
        "print(x_train.shape) \n",
        "print(y_train.shape)  \n",
        "\n",
        "train_set = tf.data.Dataset.from_tensor_slices(((x_train,), (y_train,)))\n",
        "test_set = tf.data.Dataset.from_tensor_slices(((x_test,), (y_test,)))\n",
        "\n",
        "reg = ak.ImageRegressor(overwrite=True, max_trials=100)\n",
        "\n",
        "# Feed the tensorflow Dataset to the regressor.\n",
        "reg.fit(train_set, epochs=2000)\n",
        "\n",
        "# Predict with the best model.\n",
        "predicted_y = reg.predict(test_set)\n",
        "\n",
        "# Evaluate the best model with testing data.\n",
        "print(reg.evaluate(test_set))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZ9iD7yNhDa7"
      },
      "source": [
        "**Deliverables:**\n",
        "\n",
        "*   Python package with an Automated ML function to be called using any data frame (dataset) to give a good trained model.\n",
        "*   Details on the steps automated and the scenario in which they execute.\n",
        "*   Any scenario which is not automated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyIRiBWrkk2g"
      },
      "source": [
        "TPOT cannot solve multi-label regression problems at this point of time."
      ]
    }
  ]
}